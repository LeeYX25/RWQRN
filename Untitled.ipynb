{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71dd05ab-1785-494c-8707-d3e258b337d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\86188\\Anaconda3\\envs\\py37\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\86188\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\Users\\86188\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\86188\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\externals\\six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
      "c:\\Users\\86188\\Anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.tree.tree module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.tree. Anything that cannot be imported from sklearn.tree is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "from funcs import Sanity, Scenario1, Scenario2, Scenario3, Scenario4, Scenario5,\\\n",
    "                  MultivariateScenario1, MultivariateScenario2, Scenario1_Combined, Scenario4_Combined,Model1,Model2\n",
    "#from neural_sqerr import SqErrNetwork\n",
    "from RF_neural_model import QuantileNetwork\n",
    "#from neural_model import QuantileNetwork\n",
    "from spline_model import QuantileSpline\n",
    "from forest_model import QuantileForest\n",
    "#from visualize import heatmap_from_points\n",
    "from skgarden import MondrianForestRegressor\n",
    "from skgarden import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import model_selection\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9da0cb6-f730-4042-9291-37b162910672",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_benchmarks(demo=True):\n",
    "    N_trials = 1\n",
    "    N_test = 100\n",
    "    sample_sizes = [500]\n",
    "    quantiles = np.array([0.05,0.25,0.5,0.75,0.95])\n",
    "    functions = [Scenario2()]\n",
    "    #bands=np.array([0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000])\n",
    "    #bands=np.linspace(0.1, 10, 20)\n",
    "    band=0.0001\n",
    "    tau=0.01\n",
    "    models = [#lambda: SqErrNetwork(),\n",
    "               #lambda: QuantileNetwork(quantiles=quantiles),]\n",
    "               #lambda: QuantileSpline(quantiles=quantiles),]\n",
    "              lambda: QuantileForest(quantiles=quantiles)]\n",
    "\n",
    "\n",
    "\n",
    "    # Track the performance results\n",
    "    mse_results = np.full((N_trials, len(functions), len(models), len(sample_sizes), len(quantiles)), np.nan)\n",
    "    mae_results = np.full((N_trials, len(functions), len(models), len(sample_sizes), len(quantiles)), np.nan)\n",
    "    mce_results = np.full((N_trials, len(functions), len(models), len(sample_sizes), len(quantiles)), np.nan)\n",
    "    #picp_results = np.full((N_trials, len(functions), len(models), len(sample_sizes)), np.nan)\n",
    "    cross_results = np.full((N_trials, len(functions), len(models), len(sample_sizes), len(quantiles)), np.nan)\n",
    "    for trial in range(N_trials):\n",
    "        print(f'Trial {trial+1}')\n",
    "        for scenario, func in enumerate(functions):\n",
    "            print(f'\\t Scenario {scenario+1}')\n",
    "\n",
    "            # Sample test set covariates and response\n",
    "            X_test = np.random.random(size=(N_test,func.n_in))\n",
    "            y_test = func.sample(X_test)\n",
    "\n",
    "            # Get the ground truth quantiles\n",
    "            y_quantiles = np.array([func.quantile(X_test, q) for q in quantiles]).T\n",
    "            y_sample=  np.array([func.sample(X_test)]).T\n",
    "\n",
    "\n",
    "            # Demo plotting\n",
    "            if demo:\n",
    "                for qidx, q in enumerate((quantiles*100).astype(int)):\n",
    "                    heatmap_from_points(f'plots/scenario{scenario+1}-quantile{q}-truth.pdf', X_test[:,:2], y_quantiles[:,qidx], vmin=y_quantiles.min(), vmax=y_quantiles.max())\n",
    "\n",
    "            for nidx, N_train in enumerate(sample_sizes):\n",
    "                print(f'\\t\\tN={N_train}')\n",
    "                # Sample training covariates and response\n",
    "                X_train = np.random.random(size=(N_train,func.n_in))\n",
    "                y_train = func.sample(X_train)\n",
    "                # Evaluate each of the quantile models\n",
    "                # Note: we generate a new model each time so as to not\n",
    "                # accidentally cheat by warm-starting from the last point\n",
    "                for midx, model in enumerate([m() for m in models]):\n",
    "                    print(f'\\t\\t\\t{model.label}')\n",
    "\n",
    "                    if X_train.shape[1] > 3 and model.filename == 'spline':\n",
    "                        print('Too many covariates. Skipping...')\n",
    "                        continue\n",
    "\n",
    "                    #model.fit(X_train, y_train,tau)\n",
    "                    #model.fit(X_train, y_train,band)\n",
    "                    model.fit(X_train, y_train)\n",
    "                    preds = model.predict(X_test)\n",
    "                    print(preds)\n",
    "                    #print(y_quantiles)\n",
    "\n",
    "\n",
    "                    # Evaluate the model on the ground truth quantiles\n",
    "                    mse_results[trial, scenario, midx, nidx] = ((y_quantiles - preds)**2).mean()\n",
    "                    #mce_results[trial, scenario, midx, nidx] = mce(y_sample,preds,quantiles)\n",
    "                    #mae_results[trial, scenario, midx, nidx] = np.abs(y_quantiles - preds).mean(axis=0)\n",
    "                    cross_results[trial, scenario, midx, nidx] = calculate_crossing_rate(preds)\n",
    "                    \n",
    "                print('\\t', mse_results[trial, scenario])\n",
    "                print('\\t', cross_results[trial, scenario])\n",
    "                #print('\\t', mce_results[trial, scenario])\n",
    "                #print('\\t', mae_results[trial, scenario])\n",
    "\n",
    "            \n",
    "                    # Demo plotting\n",
    "                    # if demo:\n",
    "                    #     # Plot the results for the first 2 coordinates\n",
    "                    #     for qidx, q in enumerate((quantiles*100).astype(int)):\n",
    "                    #         heatmap_from_points(f'plots/scenario{scenario+1}-quantile{q}-n{N_train}-{model.filename}.pdf', X_test[:,:2],\n",
    "                    #                                 preds[:,qidx] if preds.shape[1] > qidx else preds[:,-1],\n",
    "                    #                                 vmin=y_quantiles.min(), vmax=y_quantiles.max(),\n",
    "                    #                                 colorbar=midx == len(models)-1)\n",
    "\n",
    "#                 for midx, lowmodel in enumerate([m() for m in models_lower_picp]):\n",
    "\n",
    "\n",
    "#                     if X_train.shape[1] > 3 and lowmodel.filename == 'spline':\n",
    "#                         print('Too many covariates. Skipping...')\n",
    "#                         continue\n",
    "\n",
    "#                     lowmodel.fit(X_train, y_train,tau)\n",
    "#                     #lowmodel.fit(X_train, y_train)\n",
    "#                     preds_low = lowmodel.predict(X_test)\n",
    "#                 for midx, upmodel in enumerate([m() for m in models_upper_picp]):\n",
    "\n",
    "\n",
    "#                     if X_train.shape[1] > 3 and upmodel.filename == 'spline':\n",
    "#                         print('Too many covariates. Skipping...')\n",
    "#                         continue\n",
    "\n",
    "#                     upmodel.fit(X_train, y_train,tau)\n",
    "#                     #upmodel.fit(X_train, y_train)\n",
    "                    #preds_up = upmodel.predict(X_test)\n",
    "\n",
    "                    # Evaluate the model on the ground truth quantiles\n",
    "\n",
    "                #picp_results[trial, scenario, midx, nidx] = picp(y_sample,preds_low,preds_up)\n",
    "\n",
    "\n",
    "#                 print('\\t', mse_results[trial, scenario])\n",
    "#                 print('\\t', mce_results[trial, scenario])\n",
    "#                 print('\\t', mae_results[trial, scenario])\n",
    "#                 #print('\\t', picp_results[trial, scenario])\n",
    "\n",
    "            if not demo:\n",
    "                pd.DataFrame(mse_results[:,0,0,0,0]).to_csv('NQRN.csv', index=False)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    mean_mse_results = np.nanmean(mse_results, axis=0)\n",
    "    #mean_mce_results = np.nanmean(mce_results, axis=0)\n",
    "    #mean_mae_results = np.nanmean(mae_results, axis=0)\n",
    "    #mean_picp_results = np.nanmean(picp_results, axis=0)\n",
    "    mean_cross_results = np.nanmean(cross_results, axis=0)\n",
    "\n",
    "    print(f'Mean mse {mean_mse_results}')\n",
    "    print(f'Mean cross rate {cross_results}')\n",
    "    #print(f'Mean mce {mean_mce_results}')\n",
    "    #print(f'Mean mae {mean_mae_results}')\n",
    "    #print(f'Mean picp {mean_picp_results}')\n",
    "\n",
    "def run_multivariate_benchmarks(demo=True):\n",
    "    N_trials = 1\n",
    "    N_test = 100\n",
    "    sample_sizes = [500]\n",
    "    quantiles = np.array([0.05,0.25,0.5,0.75,0.95])\n",
    "    tau=0.5\n",
    "    functions = [MultivariateScenario1()]\n",
    "    models = [#lambda: SqErrNetwork(),\n",
    "            lambda: QuantileNetwork(quantiles=quantiles,loss=\"geometric\"),]\n",
    "               #lambda: QuantileSpline(quantiles=quantiles),]\n",
    "              #lambda: QuantileForest(quantiles=quantiles)]\n",
    "#     models_lower_picp = [#lambda: SqErrNetwork(),\n",
    "#                #lambda: QuantileNetwork(quantiles=np.array([0.05]),loss=\"marginal\"),]\n",
    "#                #lambda: QuantileSpline(quantiles=np.array([0.05])),]\n",
    "#               lambda: QuantileForest(quantiles=np.array([0.05]))]\n",
    "\n",
    "#     models_upper_picp = [#lambda: SqErrNetwork(),\n",
    "#                #lambda: QuantileNetwork(quantiles=np.array([0.95]),loss=\"marginal\"),]\n",
    "#                #lambda: QuantileSpline(quantiles=np.array([0.95])),]\n",
    "#               lambda: QuantileForest(quantiles=np.array([0.95]))]\n",
    "    # Track the performance results\n",
    "    mse_results = np.full((N_trials, len(functions), len(models), len(sample_sizes), len(quantiles)), np.nan)\n",
    "    mae_results = np.full((N_trials, len(functions), len(models), len(sample_sizes), len(quantiles)), np.nan)\n",
    "    mce_results = np.full((N_trials, len(functions), len(models), len(sample_sizes), len(quantiles)), np.nan)\n",
    "    #picp_results = np.full((N_trials, len(functions), len(models), len(sample_sizes), len(quantiles)), np.nan)\n",
    "\n",
    "\n",
    "    for trial in range(N_trials):\n",
    "        print(f'Trial {trial+1}')\n",
    "        for scenario, func in enumerate(functions):\n",
    "            print(f'\\tScenario {scenario+1}')\n",
    "\n",
    "            # Sample test set covariates and response\n",
    "            X_test = np.random.random(size=(N_test,func.n_in))\n",
    "\n",
    "            # Get the ground truth quantiles\n",
    "            y_quantiles = np.transpose(np.array([func.quantile(X_test, q) for q in quantiles]), [1, 2, 0])\n",
    "            y_sample = np.transpose(np.array([func.sample(X_test)]),[1, 2, 0])\n",
    "            for nidx, N_train in enumerate(sample_sizes):\n",
    "                print(f'\\t\\tN={N_train}')\n",
    "                # Sample training covariates and response\n",
    "                X_train = np.random.random(size=(N_train,func.n_in))\n",
    "                y_train = func.sample(X_train)\n",
    "\n",
    "                # Evaluate each of the quantile models\n",
    "                # Note: we generate a new model each time so as to not\n",
    "                # accidentally cheat by warm-starting from the last point\n",
    "                for midx, model in enumerate([m() for m in models]):\n",
    "                    print(f'\\t\\t\\t{model.label}')\n",
    "\n",
    "                    if X_train.shape[1] > 3 and model.filename == 'spline':\n",
    "                        print('Too many covariates. Skipping...')\n",
    "                        continue\n",
    "\n",
    "                    #model.fit(X_train, y_train,)\n",
    "                    model.fit(X_train, y_train, tau)\n",
    "                    preds = model.predict(X_test)\n",
    "                    print(preds.shape)\n",
    "                    # Evaluate the model on the ground truth quantiles\n",
    "                    mse_results[trial, scenario, midx, nidx] = ((y_quantiles - preds) ** 2).mean()\n",
    "                    # mce_results[trial, scenario, midx, nidx] = mce(y_sample, preds, quantiles)\n",
    "                    # mae_results[trial, scenario, midx, nidx] = np.abs(y_quantiles - preds).mean(axis=(0,1))\n",
    "                    print('\\t', mse_results[trial, scenario])\n",
    "                    # print('\\t', mce_results[trial, scenario])\n",
    "                    # print('\\t', mae_results[trial, scenario])\n",
    "\n",
    "#                 for midx, lowmodel in enumerate([m() for m in models_lower_picp]):\n",
    "\n",
    "#                     if X_train.shape[1] > 3 and lowmodel.filename == 'spline':\n",
    "#                         print('Too many covariates. Skipping...')\n",
    "#                         continue\n",
    "\n",
    "#                     lowmodel.fit(X_train, y_train,tau)\n",
    "#                     #lowmodel.fit(X_train, y_train)\n",
    "#                     preds_low = lowmodel.predict(X_test)\n",
    "#                 for midx, upmodel in enumerate([m() for m in models_upper_picp]):\n",
    "\n",
    "#                     if X_train.shape[1] > 3 and upmodel.filename == 'spline':\n",
    "#                         print('Too many covariates. Skipping...')\n",
    "#                         continue\n",
    "\n",
    "#                     upmodel.fit(X_train, y_train,tau)\n",
    "#                     #upmodel.fit(X_train, y_train)\n",
    "#                     preds_up = upmodel.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#                 picp_results[trial, scenario, midx, nidx] = picp(y_sample, preds_low, preds_up)\n",
    "\n",
    "\n",
    "            #print('\\t', picp_results[trial, scenario])\n",
    "\n",
    "            if not demo:\n",
    "                pd.DataFrame(mse_results[:,0,0,0,0]).to_csv('NQRN.csv', index=False)\n",
    "            else:\n",
    "                pass\n",
    "#     mean_mse_results = np.nanmean(mse_results,axis=0)\n",
    "#     mean_mce_results = np.nanmean(mce_results,axis=0)\n",
    "#     mean_mae_results = np.nanmean(mae_results,axis=0)\n",
    "#     mean_picp_results = np.nanmean(picp_results,axis=0)\n",
    "\n",
    "#     print(f'Mean mse {mean_mse_results}')\n",
    "#     print(f'Mean mce {mean_mce_results}')\n",
    "#     print(f'Mean mae {mean_mae_results}')\n",
    "#     print(f'Mean picp {mean_picp_results}')\n",
    "\n",
    "\n",
    "def mce(y, yp, quan):\n",
    "\n",
    "\n",
    "    diff = y - yp  # 形状与 y 和 yp 相同\n",
    "\n",
    "    quan_expanded = np.expand_dims(quan, axis=tuple(range(diff.ndim - 1)))\n",
    "\n",
    "    term1 = diff * quan_expanded\n",
    "    term2 = diff * (quan_expanded - 1)\n",
    "    ndim= quan_expanded .ndim\n",
    "    max_terms = np.maximum(term1, term2)\n",
    "    result = np.mean(max_terms, axis=tuple(range(ndim - 1)))\n",
    "    return result\n",
    "\n",
    "def picp(y, yp1, yp2):\n",
    "    # 确保y, yp1, yp2是n×1的数组\n",
    "\n",
    "    # 判断y的每个元素是否在由yp1和yp2定义的区间内\n",
    "    within_interval = (yp1 <= y) & (y <= yp2)\n",
    "\n",
    "    coverage_rate = np.mean(within_interval)\n",
    "\n",
    "    return coverage_rate\n",
    "\n",
    "def calculate_crossing_rate(y_pred):\n",
    "    \"\"\"\n",
    "    计算分位数交叉率 (Crossing Rate)\n",
    "    \n",
    "    Args:\n",
    "        y_pred: (N_samples, n_dim, n_quantiles) 或 (N_samples, n_quantiles)\n",
    "                注意：最后一维必须是按从小到大排序的分位数\n",
    "    Returns:\n",
    "        rate: 发生交叉的样本百分比 (0.0 - 100.0)\n",
    "    \"\"\"\n",
    "    # 统一维度为 (N, ..., M)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # 沿分位数轴计算差分: q_{j+1} - q_j\n",
    "    # 如果没交叉，应该全部 >= 0\n",
    "    diff = np.diff(y_pred, axis=-1)\n",
    "    \n",
    "    # 检查是否有负值 (交叉)\n",
    "    # is_crossing_elementwise: 形状同 diff\n",
    "    is_crossing_anywhere = diff < -1e-6 # 加一个小 epsilon 防止浮点误差\n",
    "    \n",
    "    if y_pred.ndim == 3: # 多变量 (N, D, M)\n",
    "        # 逻辑：对于某个样本，只要有【任意维度】发生了【任意分位数对】的交叉，该样本就算 Failed\n",
    "        # 先沿 M 聚合，再沿 D 聚合\n",
    "        sample_failed = np.any(is_crossing_anywhere, axis=(1, 2))\n",
    "    else: # 单变量 (N, M)\n",
    "        sample_failed = np.any(is_crossing_anywhere, axis=1)\n",
    "        \n",
    "    rate = np.mean(sample_failed) * 100.0\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "829e5658-7a9c-4991-9557-253ba3c8c18c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1\n",
      "\t Scenario 1\n",
      "\t\tN=500\n",
      "\t\t\tQuantile Forest\n",
      "[[-3.84 -0.52  0.5   1.58  4.29]\n",
      " [-2.14 -0.02  1.9   3.6  12.59]\n",
      " [-2.27 -0.08  1.56  3.72  5.67]\n",
      " [-5.14 -0.41  1.24  2.51  3.98]\n",
      " [-3.14 -0.51  1.31  2.26  5.55]\n",
      " [-2.72  0.12  1.19  2.3   4.86]\n",
      " [-2.81  1.03  2.34  3.85  7.24]\n",
      " [-2.38  0.98  2.16  3.69  4.74]\n",
      " [-4.06 -0.12  1.52  3.08  4.6 ]\n",
      " [-0.76  0.64  1.37  2.73  4.76]\n",
      " [-3.13 -0.33  1.47  3.5   5.06]\n",
      " [-8.06 -0.39  0.82  2.39 11.93]\n",
      " [-2.02  0.57  1.52  2.75  4.43]\n",
      " [-2.62  1.02  2.28  3.48  5.09]\n",
      " [-2.74  1.36  2.97  9.08 12.2 ]\n",
      " [-2.38 -0.21  1.27  2.2   4.73]\n",
      " [-2.93  0.76  1.64  2.61  4.35]\n",
      " [-3.86 -0.06  1.39  3.18  5.03]\n",
      " [-3.8  -0.31  0.63  2.24  3.98]\n",
      " [-9.17 -3.37 -1.02  1.49  3.9 ]\n",
      " [-2.4   0.35  1.81  2.53 12.46]\n",
      " [-1.44  0.13  0.94  1.88  4.  ]\n",
      " [-3.61 -0.    2.12  3.63  8.03]\n",
      " [-1.85  0.83  1.94  3.45 12.21]\n",
      " [-0.89  1.04  1.57  2.38  4.51]\n",
      " [-2.28 -0.12  1.47  2.39  3.65]\n",
      " [-5.91  0.4   2.28  3.7   7.06]\n",
      " [-0.78  1.46  2.78  3.69  9.53]\n",
      " [-3.71 -0.04  0.8   2.14  4.71]\n",
      " [-6.58 -0.88  0.33  2.06  5.95]\n",
      " [-9.23 -3.06 -0.36  2.39  6.39]\n",
      " [-1.43 -0.29  0.2   1.59  3.63]\n",
      " [-3.45 -0.1   0.88  2.08  4.08]\n",
      " [-0.44  0.99  2.54  4.    8.25]\n",
      " [-1.6   0.01  1.32  2.92  7.95]\n",
      " [-4.09 -1.71  0.71  1.9   4.2 ]\n",
      " [-3.83  0.53  1.72  2.6   4.29]\n",
      " [-2.94  0.76  1.54  2.88  4.65]\n",
      " [-1.33  0.6   2.2   4.08  5.71]\n",
      " [-3.05  0.58  2.32  3.65  5.44]\n",
      " [-2.36  1.34  1.83  2.42  7.08]\n",
      " [-6.98 -0.83  0.55  1.65  4.94]\n",
      " [-2.76 -0.05  1.03  1.92  3.61]\n",
      " [-4.25 -0.95  0.92  2.21  3.78]\n",
      " [-1.09  0.46  1.38  2.28  4.77]\n",
      " [-3.98  0.84  1.82  3.51  5.62]\n",
      " [-0.73  0.26  1.34  2.68  4.87]\n",
      " [-2.68  0.72  1.43  2.27  4.46]\n",
      " [-5.19 -0.59  0.2   2.43  5.58]\n",
      " [-3.09 -0.34  2.81  3.71  4.86]\n",
      " [-2.45  0.13  1.42  2.74  4.41]\n",
      " [-3.84 -2.42 -0.25  0.82  3.45]\n",
      " [-3.12 -0.98  1.29  2.65  7.24]\n",
      " [-2.14 -0.29  0.9   1.77  8.4 ]\n",
      " [-1.91  0.5   1.75  2.89  5.31]\n",
      " [-2.14  0.48  1.73  2.57  3.65]\n",
      " [-2.79  0.84  2.23  3.56  6.27]\n",
      " [-2.36 -0.16  0.71  3.36  5.55]\n",
      " [-1.75  0.56  1.63  3.5  12.08]\n",
      " [-4.26 -2.42  1.12  2.23  5.64]\n",
      " [-2.03 -0.    1.04  2.53  4.28]\n",
      " [-3.27  0.07  0.95  3.16  4.71]\n",
      " [-4.   -0.71  1.84  4.22 12.2 ]\n",
      " [-0.8   0.9   1.47  2.15  4.93]\n",
      " [-2.15  0.65  2.14  3.24  4.83]\n",
      " [-1.02 -0.28  0.87  1.77  5.69]\n",
      " [-6.13 -1.44  0.11  2.    4.75]\n",
      " [-2.95 -0.28  1.29  2.09  5.34]\n",
      " [-9.01 -2.19  0.87  1.85  3.71]\n",
      " [-3.13  0.28  1.56  3.14  7.31]\n",
      " [-3.8  -0.28  0.82  2.16  5.13]\n",
      " [-1.24  0.87  2.08  3.83  7.84]\n",
      " [-0.08  1.97  3.09  4.06  8.73]\n",
      " [-2.41 -0.2   0.86  3.52  5.69]\n",
      " [-5.67 -0.39  1.67  3.86 10.87]\n",
      " [-4.4  -0.88  0.84  3.11  9.74]\n",
      " [-1.76  0.49  1.99  3.78  9.66]\n",
      " [-1.44  1.34  2.42  3.89  5.76]\n",
      " [-3.3   0.19  1.77  4.04  4.74]\n",
      " [-2.41  0.39  2.33  4.29  4.98]\n",
      " [-1.38  1.54  3.18  4.09  5.72]\n",
      " [-2.43  0.11  2.06  4.31 12.21]\n",
      " [-2.72 -0.25  1.42  3.52  5.72]\n",
      " [-2.83  1.1   1.98  2.53  4.25]\n",
      " [-2.24  0.18  1.33  2.16  4.51]\n",
      " [-3.64 -0.56  0.75  1.95  4.29]\n",
      " [-0.99 -0.06  1.54  3.31  5.76]\n",
      " [-2.69 -0.43  1.73  3.64  5.98]\n",
      " [-4.21 -0.04  1.79  4.01  7.42]\n",
      " [-2.93  0.14  1.56  3.77  5.48]\n",
      " [-3.91  0.56  1.36  2.42  3.8 ]\n",
      " [-9.91 -3.76 -0.72  1.74  4.21]\n",
      " [-1.15  1.03  2.5   3.71  5.64]\n",
      " [ 0.29  2.13  3.14  3.68  4.97]\n",
      " [-4.47 -0.2   1.74  4.2   6.42]\n",
      " [-2.73  0.06  1.63  3.09  5.16]\n",
      " [-1.04  1.21  1.89  3.25  4.61]\n",
      " [-3.11 -0.55  1.88  3.9   6.86]\n",
      " [-3.35 -0.12  1.53  2.77  5.6 ]\n",
      " [-2.92  1.5   3.13  3.99  9.43]]\n",
      "\t [[[2.34 2.34 2.34 2.34 2.34]]]\n",
      "\t [[[0. 0. 0. 0. 0.]]]\n",
      "Mean mse [[[[2.34 2.34 2.34 2.34 2.34]]]]\n",
      "Mean cross rate [[[[[0. 0. 0. 0. 0.]]]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    run_benchmarks(False)\n",
    "\n",
    "    #run_multivariate_benchmarks(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "887b5cc0-2f7d-41e2-8fc7-4a111ebec6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "1.96 2.13 2.84 2.31 1.92 2.06 2.02 1.87 1.83 1.72 1.84 2.52\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from pandas import DataFrame\n",
    "grunfeld_data=sm.datasets.grunfeld.load_pandas()# print(type(scotland data))# print(scotland data)df =scotland data.data\n",
    "df=grunfeld_data.data\n",
    "print(type(df))# DataFrame 类型的数据print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "507f488e-3be3-489d-9598-dce7421ee96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,)\n",
      "[ 3.92  1.87  2.08  3.04  3.77  4.99  5.79 10.    9.47 17.74 12.92 19.96\n",
      " 26.66 16.31 27.83 39.52 48.92 53.1  57.66 59.48]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "raw = [[[[[3.92]]]], [[[[1.87]]]], [[[[2.08]]]], [[[[3.04]]]], [[[[3.77]]]],\n",
    "       [[[[4.99]]]], [[[[5.79]]]], [[[[10.00]]]], [[[[9.47]]]], [[[[17.74]]]],\n",
    "       [[[[12.92]]]], [[[[19.96]]]], [[[[26.66]]]], [[[[16.31]]]], [[[[27.83]]]],\n",
    "       [[[[39.52]]]], [[[[48.92]]]], [[[[53.10]]]], [[[[57.66]]]], [[[[59.48]]]]]\n",
    "\n",
    "arr = np.array(raw).ravel()\n",
    "print(arr.shape)   # (20,)\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d73d667-2428-4f4e-aef0-f698dd499c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 参数\n",
    "h_values = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "mse_values = [1.33, 2.1,2.12, 1.58, 3.49, 1.98, 1.31, 2.17,2.17]\n",
    "\n",
    "minih=np.linspace(0.1, 10, 20)\n",
    "minimse=arr\n",
    "# 画图\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(minih , minimse, marker='o')\n",
    "plt.xscale('log')               # h 跨度大，用对数坐标\n",
    "plt.xlabel('Bandwidth h (log scale)')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE vs Bandwidth h tau=0.75 high')\n",
    "plt.grid(True, which='both', ls='--')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3e082b-e134-4f6d-89b0-dd7d6524cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "real_estate_valuation = fetch_ucirepo(id=477) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = real_estate_valuation.data.features \n",
    "y = real_estate_valuation.data.targets \n",
    "  \n",
    "x_ini=np.array(X)\n",
    "y_ini=np.array(y).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d304d2e-afb5-48d0-863d-bb7314e667e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "student_performance = fetch_ucirepo(id=320) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "\n",
    "y = student_performance.data.targets \n",
    "y=np.array(y)\n",
    "X= pd.read_csv('Xstu.csv',header=0)\n",
    "x_ini=np.array(X)\n",
    "y_ini=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d2967e76-b4a4-4633-a113-86fac5479112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件，跳过第一行（通常是标题行）\n",
    "# 同时跳过第一列，使用列索引从0开始的其余列\n",
    "data = pd.read_csv('gasoline.csv')\n",
    "\n",
    "# 分离特征和目标变量\n",
    "X = data.iloc[:, 0:]  # 选择除了最后一列的所有列\n",
    "y = data.iloc[:, 0]   # 选择最后一列\n",
    "x_ini=np.array(X)\n",
    "y_ini=np.array(y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7693d501-1712-44b9-8f42-3cd70bcf2442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件，跳过第一行（通常是标题行）\n",
    "# 同时跳过第一列，使用列索引从0开始的其余列\n",
    "data = pd.read_csv('barro.csv')\n",
    "\n",
    "# 分离特征和目标变量\n",
    "X = data.iloc[:, 2:]  # 选择除了最后一列的所有列\n",
    "y = data.iloc[:, 0:2]   # 选择最后一列\n",
    "x_ini=np.array(X)\n",
    "y_ini=np.array(y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00d0f4f1-fe30-4975-b266-f3bb13e651a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 读取CSV文件，跳过第一行（通常是标题行）\n",
    "# 同时跳过第一列，使用列索引从0开始的其余列\n",
    "data = pd.read_csv('gasoline.csv')\n",
    "\n",
    "# 分离特征和目标变量\n",
    "X = data.iloc[:, 0]  # 选择除了最后一列的所有列\n",
    "y = data.iloc[:, -1]   # 选择最后一列\n",
    "x_ini=np.array(X).reshape(-1,1)\n",
    "y_ini=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a0753ea-5f21-4bbf-be78-b29398f48ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.7/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "data = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "data['PRICE'] = boston.target\n",
    "\n",
    "# 分离特征和目标\n",
    "X = data.drop('PRICE', axis=1)\n",
    "y = data['PRICE']\n",
    "x_ini=np.array(X)\n",
    "y_ini=np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aa940e22-9143-4a88-95d4-b2fe5ebde139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "wine_quality = fetch_ucirepo(id=186) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = wine_quality.data.features \n",
    "y = wine_quality.data.targets \n",
    "  \n",
    "x_ini=np.array(X)\n",
    "y_ini=np.array(y).squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43dd1f63-44ce-4c04-a26b-e46aaccac161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "energy_efficiency = fetch_ucirepo(id=242) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = energy_efficiency.data.features \n",
    "y = energy_efficiency.data.targets \n",
    "  \n",
    "x_ini=np.array(X)\n",
    "y_ini=np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c9bf38f1-d5c5-477f-baa7-cd49f8afd3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# 1. 读数据\n",
    "communities_and_crime = fetch_ucirepo(id=183)\n",
    "X = communities_and_crime.data.features\n",
    "y = communities_and_crime.data.targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "caa9ba36-7cf6-4626-a71e-75fa8fc96f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 X 已经是 DataFrame，但里面像 \"123\"\n",
    "X = X.apply(lambda col: pd.to_numeric(col.astype(str).str.replace('\"', ''), errors='coerce'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d4e4253-96f2-4052-8e2b-5e83ce0f7e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始样本数: 1994\n",
      "清洗后样本数: 123\n"
     ]
    }
   ],
   "source": [
    "# 2. 合并成一张大表（方便一次性按行清洗）\n",
    "df_all = pd.concat([X, y], axis=1)          # 特征 + 目标\n",
    "\n",
    "# 3. 剔除任何含缺失值的样本\n",
    "df_all = df_all.replace('?', np.nan).dropna()             # 默认 axis=0，即按行删\n",
    "\n",
    "# 4. 再拆回 X 和 y\n",
    "X_clean = df_all[X.columns]\n",
    "y_clean = df_all[y.columns]\n",
    "\n",
    "print(\"原始样本数:\", X.shape[0])\n",
    "print(\"清洗后样本数:\", X_clean.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d4512e3-698d-4bc2-ad5d-2a17de469d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 X 已经是 DataFrame，但里面像 \"123\"\n",
    "X_clean = X_clean.apply(lambda col: pd.to_numeric(col.astype(str).str.replace('\"', ''), errors='coerce'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1137cde0-f329-4eec-8352-91be8a752f46",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始样本数: 1994\n",
      "清洗后样本数: 123\n",
      "      state county community     communityname  fold  population  \\\n",
      "16       36      1      1000        Albanycity     1        0.15   \n",
      "23       19    193     93926     SiouxCitycity     1        0.11   \n",
      "33       51    680     47672     Lynchburgcity     1        0.09   \n",
      "68       34     23     58200    PerthAmboycity     1        0.05   \n",
      "74        9      9     46520       Meridentown     1        0.08   \n",
      "...     ...    ...       ...               ...   ...         ...   \n",
      "1880     34     39     40350        Lindencity    10        0.04   \n",
      "1963     36     27     59641  Poughkeepsiecity    10        0.03   \n",
      "1981      9      9     35650        Hamdentown    10        0.07   \n",
      "1991      9      9     80070     Waterburytown    10        0.16   \n",
      "1992     25     17     72600       Walthamcity    10        0.08   \n",
      "\n",
      "      householdsize  racepctblack  racePctWhite  racePctAsian  ...  \\\n",
      "16             0.31          0.40          0.63          0.14  ...   \n",
      "23             0.43          0.04          0.89          0.09  ...   \n",
      "33             0.43          0.51          0.58          0.04  ...   \n",
      "68             0.59          0.23          0.39          0.09  ...   \n",
      "74             0.39          0.08          0.85          0.04  ...   \n",
      "...             ...           ...           ...           ...  ...   \n",
      "1880           0.39          0.39          0.65          0.09  ...   \n",
      "1963           0.32          0.61          0.47          0.09  ...   \n",
      "1981           0.38          0.17          0.84          0.11  ...   \n",
      "1991           0.37          0.25          0.69          0.04  ...   \n",
      "1992           0.51          0.06          0.87          0.22  ...   \n",
      "\n",
      "      PolicAveOTWorked  LandArea  PopDens  PctUsePubTrans  PolicCars  \\\n",
      "16                0.22      0.06     0.39            0.84       0.06   \n",
      "23                0.29      0.16     0.12            0.07       0.04   \n",
      "33                 0.1      0.14     0.11            0.19       0.05   \n",
      "68                0.31      0.01     0.73            0.28          0   \n",
      "74                0.31      0.07     0.21            0.04       0.02   \n",
      "...                ...       ...      ...             ...        ...   \n",
      "1880              0.32      0.03     0.28            0.32       0.02   \n",
      "1963              0.44      0.01     0.47            0.42       0.07   \n",
      "1981              0.25      0.09     0.13            0.17       0.02   \n",
      "1991              0.25      0.08     0.32            0.18       0.08   \n",
      "1992              0.19      0.03     0.38            0.33       0.02   \n",
      "\n",
      "      PolicOperBudg  LemasPctPolicOnPatr  LemasGangUnitDeploy  \\\n",
      "16             0.06                 0.91                  0.5   \n",
      "23             0.01                 0.81                    1   \n",
      "33             0.01                 0.75                    0   \n",
      "68             0.02                 0.64                    0   \n",
      "74             0.01                  0.7                    1   \n",
      "...             ...                  ...                  ...   \n",
      "1880           0.01                 0.85                    0   \n",
      "1963           0.08                 0.49                    0   \n",
      "1981           0.01                 0.72                    0   \n",
      "1991           0.06                 0.78                    0   \n",
      "1992           0.02                 0.79                    0   \n",
      "\n",
      "      LemasPctOfficDrugUn  PolicBudgPerPop  \n",
      "16                   0.88             0.26  \n",
      "23                   0.56             0.09  \n",
      "33                   0.60              0.1  \n",
      "68                   1.00             0.23  \n",
      "74                   0.44             0.11  \n",
      "...                   ...              ...  \n",
      "1880                 0.99             0.19  \n",
      "1963                 0.37                1  \n",
      "1981                 0.62             0.15  \n",
      "1991                 0.91             0.28  \n",
      "1992                 0.22             0.18  \n",
      "\n",
      "[123 rows x 127 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1. 读数据\n",
    "communities_and_crime = fetch_ucirepo(id=183)\n",
    "X = communities_and_crime.data.features\n",
    "y = communities_and_crime.data.targets\n",
    "\n",
    "# 2. 合并成一张大表（方便一次性按行清洗）\n",
    "df_all = pd.concat([X, y], axis=1)          # 特征 + 目标\n",
    "\n",
    "# 3. 剔除任何含缺失值的样本\n",
    "df_all = df_all.replace('?', np.nan).dropna()             # 默认 axis=0，即按行删\n",
    "\n",
    "# 4. 再拆回 X 和 y\n",
    "X_clean = df_all[X.columns]\n",
    "y_clean = df_all[y.columns]\n",
    "\n",
    "print(\"原始样本数:\", X.shape[0])\n",
    "print(\"清洗后样本数:\", X_clean.shape[0])\n",
    "\n",
    "le = LabelEncoder()\n",
    "X_enc = X_clean.copy()\n",
    "X_enc.iloc[:, 3] = le.fit_transform(X_clean.iloc[:, 3])\n",
    "\n",
    "X_e = X_enc.apply(lambda col: pd.to_numeric(col.astype(str).str.replace('\"', ''), errors='coerce'))\n",
    "\n",
    "\n",
    "x_ini=np.array(X_e.astype(np.float32).values)\n",
    "y_ini=np.array(y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "82f942ad-5dd1-46e7-8c9b-89ced73fa2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "#from neural_sqerr import SqErrNetwork\n",
    "from RF_neural_model import QuantileNetwork\n",
    "#from neural_model import QuantileNetwork\n",
    "from spline_model import QuantileSpline\n",
    "from forest_model import QuantileForest\n",
    "#from visualize import heatmap_from_points\n",
    "def run_benchmarks(demo=True):\n",
    "    N_trials = 1\n",
    "    N_test = 1000\n",
    "    sample_sizes = [1000]\n",
    "    quantiles = np.array([0.5])\n",
    "    functions = [Scenario1()]\n",
    "    tau=0.01\n",
    "    models = [#lambda: SqErrNetwork(),\n",
    "               lambda: QuantileNetwork(quantiles=quantiles),]\n",
    "               #lambda: QuantileSpline(quantiles=quantiles),]\n",
    "            #lambda: QuantileForest(quantiles=quantiles)]\n",
    " \n",
    "    for trial in range(N_trials):\n",
    "        print(f'Trial {trial+1}')\n",
    "        for scenario, func in enumerate(functions):\n",
    "            print(f'\\t Scenario {scenario+1}')\n",
    "            mseresults=[]\n",
    "            #mceresults=[]\n",
    "            #maeresults=[]\n",
    "            kf = KFold(n_splits=10,shuffle=True,random_state=40)\n",
    "            for train, test in kf.split(x_ini):\n",
    "\n",
    "                # Sample test set covariates and response\n",
    "\n",
    "                X_test=np.array(x_ini)[test]\n",
    "                y_test=np.array(y_ini)[test]\n",
    "                y_test=np.squeeze(y_test)\n",
    "\n",
    "\n",
    "                # Demo plotting\n",
    "                if demo:\n",
    "                    for qidx, q in enumerate((quantiles*100).astype(int)):\n",
    "                        heatmap_from_points(f'plots/scenario{scenario+1}-quantile{q}-truth.pdf', X_test[:,:2], y_quantiles[:,qidx], vmin=y_quantiles.min(), vmax=y_quantiles.max())\n",
    "\n",
    "                for nidx, N_train in enumerate(sample_sizes):\n",
    "                    #print(f'\\t\\tN={N_train}')\n",
    "                    # Sample training covariates and response\n",
    "                    X_train=np.array(x_ini)[train]\n",
    "                    y_train=np.array(y_ini)[train]\n",
    "                    #y_train=np.squeeze(y_train)\n",
    "                    print(X_train.shape)\n",
    "                    # Evaluate each of the quantile models\n",
    "                    # Note: we generate a new model each time so as to not\n",
    "                    # accidentally cheat by warm-starting from the last point\n",
    "                    for midx, model in enumerate([m() for m in models]):\n",
    "                        print(f'\\t\\t\\t{model.label}')\n",
    "\n",
    "                        if X_train.shape[1] > 3 and model.filename == 'spline':\n",
    "                            print('Too many covariates. Skipping...')\n",
    "                            continue\n",
    "\n",
    "                        model.fit(X_train, y_train,tau)\n",
    "                        #model.fit(X_train, y_train)\n",
    "                        preds = model.predict(X_test)\n",
    "                        y_test=y_test.reshape(-1,1)\n",
    "\n",
    "                        #mce1= mce(y_test,preds,quantiles)\n",
    "                    \n",
    "                        \n",
    "                        #mae=np.abs(y_test - preds).mean(axis=0)                        \n",
    "                        mse= ((y_test - preds)**2).mean()\n",
    "                        print(mse)\n",
    "\n",
    "                        \n",
    "                        mseresults.append(mse)\n",
    "                        #mceresults.append(mce1)\n",
    "                        #maeresults.append(mae)\n",
    "                        \n",
    "            mseresults=np.array(mseresults)\n",
    "            #mceresults=np.array(mceresults)\n",
    "            #maeresults=np.array(maeresults)\n",
    "\n",
    "            print(f'Mean mse {np.mean(mseresults)}')\n",
    "            #print(f'Mean mce {np.mean(mceresults)}')\n",
    "            #print(f'Mean mae {np.mean(maeresults)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b8aac376-802d-4e2a-af3f-06e043e614f1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1\n",
      "\t Scenario 1\n",
      "(54, 402)\n",
      "\t\t\tQuantile Network\n",
      "Use MondrianForestRegressor\n",
      "ymax and min: tensor(1.6101) tensor(-2.3756)\n",
      "\t\tEpoch 1\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 1\n",
      "Validation loss: 0.0007507091504521668 Best: 0.0007507091504521668\n",
      "\t\tEpoch 2\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 2\n",
      "Validation loss: 0.0007496097241528332 Best: 0.0007496097241528332\n",
      "\t\tEpoch 3\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 3\n",
      "Validation loss: 0.0007450884440913796 Best: 0.0007450884440913796\n",
      "\t\tEpoch 4\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 4\n",
      "Validation loss: 0.0007399782771244645 Best: 0.0007399782771244645\n",
      "\t\tEpoch 5\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0007496573962271214 Best: 0.0007399782771244645\n",
      "\t\tEpoch 6\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0007467357208952308 Best: 0.0007399782771244645\n",
      "\t\tEpoch 7\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 7\n",
      "Validation loss: 0.0007363967597484589 Best: 0.0007363967597484589\n",
      "\t\tEpoch 8\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 8\n",
      "Validation loss: 0.0007312745437957346 Best: 0.0007312745437957346\n",
      "\t\tEpoch 9\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 9\n",
      "Validation loss: 0.0007249435293488204 Best: 0.0007249435293488204\n",
      "\t\tEpoch 10\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 10\n",
      "Validation loss: 0.0007036655442789197 Best: 0.0007036655442789197\n",
      "\t\tEpoch 11\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 11\n",
      "Validation loss: 0.0006767610320821404 Best: 0.0006767610320821404\n",
      "\t\tEpoch 12\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 12\n",
      "Validation loss: 0.0006344763096421957 Best: 0.0006344763096421957\n",
      "\t\tEpoch 13\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 13\n",
      "Validation loss: 0.0006065320922061801 Best: 0.0006065320922061801\n",
      "\t\tEpoch 14\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 14\n",
      "Validation loss: 0.0005779153434559703 Best: 0.0005779153434559703\n",
      "\t\tEpoch 15\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 15\n",
      "Validation loss: 0.0005610074149444699 Best: 0.0005610074149444699\n",
      "\t\tEpoch 16\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 16\n",
      "Validation loss: 0.0005410810699686408 Best: 0.0005410810699686408\n",
      "\t\tEpoch 17\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 17\n",
      "Validation loss: 0.0005242096958681941 Best: 0.0005242096958681941\n",
      "\t\tEpoch 18\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 18\n",
      "Validation loss: 0.0004993424518033862 Best: 0.0004993424518033862\n",
      "\t\tEpoch 19\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 19\n",
      "Validation loss: 0.0004815140855498612 Best: 0.0004815140855498612\n",
      "\t\tEpoch 20\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 20\n",
      "Validation loss: 0.0004583757254295051 Best: 0.0004583757254295051\n",
      "\t\tEpoch 21\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 21\n",
      "Validation loss: 0.0004501876828726381 Best: 0.0004501876828726381\n",
      "\t\tEpoch 22\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 22\n",
      "Validation loss: 0.0004452611901797354 Best: 0.0004452611901797354\n",
      "\t\tEpoch 23\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 23\n",
      "Validation loss: 0.0004264951276127249 Best: 0.0004264951276127249\n",
      "\t\tEpoch 24\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 24\n",
      "Validation loss: 0.0004169862368144095 Best: 0.0004169862368144095\n",
      "\t\tEpoch 25\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00042724754894152284 Best: 0.0004169862368144095\n",
      "\t\tEpoch 26\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 26\n",
      "Validation loss: 0.0004095574840903282 Best: 0.0004095574840903282\n",
      "\t\tEpoch 27\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0004114996991120279 Best: 0.0004095574840903282\n",
      "\t\tEpoch 28\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00041272788075730205 Best: 0.0004095574840903282\n",
      "\t\tEpoch 29\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 29\n",
      "Validation loss: 0.00038949199370108545 Best: 0.00038949199370108545\n",
      "\t\tEpoch 30\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 30\n",
      "Validation loss: 0.0003688170691020787 Best: 0.0003688170691020787\n",
      "\t\tEpoch 31\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 31\n",
      "Validation loss: 0.0003436644037719816 Best: 0.0003436644037719816\n",
      "\t\tEpoch 32\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 32\n",
      "Validation loss: 0.00031604059040546417 Best: 0.00031604059040546417\n",
      "\t\tEpoch 33\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 33\n",
      "Validation loss: 0.0002992489608004689 Best: 0.0002992489608004689\n",
      "\t\tEpoch 34\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 34\n",
      "Validation loss: 0.0002647470682859421 Best: 0.0002647470682859421\n",
      "\t\tEpoch 35\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 35\n",
      "Validation loss: 0.00025944109074771404 Best: 0.00025944109074771404\n",
      "\t\tEpoch 36\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 36\n",
      "Validation loss: 0.00025235055363737047 Best: 0.00025235055363737047\n",
      "\t\tEpoch 37\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 37\n",
      "Validation loss: 0.00025103281950578094 Best: 0.00025103281950578094\n",
      "\t\tEpoch 38\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 38\n",
      "Validation loss: 0.00021628386457450688 Best: 0.00021628386457450688\n",
      "\t\tEpoch 39\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 39\n",
      "Validation loss: 0.00020598999981302768 Best: 0.00020598999981302768\n",
      "\t\tEpoch 40\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 40\n",
      "Validation loss: 0.00020558552932925522 Best: 0.00020558552932925522\n",
      "\t\tEpoch 41\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 41\n",
      "Validation loss: 0.00017816564650274813 Best: 0.00017816564650274813\n",
      "\t\tEpoch 42\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 42\n",
      "Validation loss: 0.0001636949455132708 Best: 0.0001636949455132708\n",
      "\t\tEpoch 43\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 43\n",
      "Validation loss: 0.00015512471145484596 Best: 0.00015512471145484596\n",
      "\t\tEpoch 44\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 44\n",
      "Validation loss: 0.00014992068463470787 Best: 0.00014992068463470787\n",
      "\t\tEpoch 45\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 45\n",
      "Validation loss: 0.00014015447231940925 Best: 0.00014015447231940925\n",
      "\t\tEpoch 46\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 46\n",
      "Validation loss: 0.00012848470942117274 Best: 0.00012848470942117274\n",
      "\t\tEpoch 47\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 47\n",
      "Validation loss: 0.00011856701166834682 Best: 0.00011856701166834682\n",
      "\t\tEpoch 48\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00014908501179888844 Best: 0.00011856701166834682\n",
      "\t\tEpoch 49\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.05\n",
      "Validation loss: 0.00016887181845959276 Best: 0.00011856701166834682\n",
      "\t\tEpoch 50\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00018731143791228533 Best: 0.00011856701166834682\n",
      "\t\tEpoch 51\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00019216715008951724 Best: 0.00011856701166834682\n",
      "\t\tEpoch 52\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00020376576867420226 Best: 0.00011856701166834682\n",
      "\t\tEpoch 53\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00021074216056149453 Best: 0.00011856701166834682\n",
      "\t\tEpoch 54\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001921769289765507 Best: 0.00011856701166834682\n",
      "\t\tEpoch 55\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.025\n",
      "Validation loss: 0.0002032890188274905 Best: 0.00011856701166834682\n",
      "\t\tEpoch 56\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001738662540446967 Best: 0.00011856701166834682\n",
      "\t\tEpoch 57\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00014488020678982139 Best: 0.00011856701166834682\n",
      "\t\tEpoch 58\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00012895348481833935 Best: 0.00011856701166834682\n",
      "\t\tEpoch 59\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00012245270772837102 Best: 0.00011856701166834682\n",
      "\t\tEpoch 60\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00012045267794746906 Best: 0.00011856701166834682\n",
      "\t\tEpoch 61\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0125\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 61\n",
      "Validation loss: 0.00011008698493242264 Best: 0.00011008698493242264\n",
      "\t\tEpoch 62\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00013166837743483484 Best: 0.00011008698493242264\n",
      "\t\tEpoch 63\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001225477026309818 Best: 0.00011008698493242264\n",
      "\t\tEpoch 64\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001571050815982744 Best: 0.00011008698493242264\n",
      "\t\tEpoch 65\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00016873846470844 Best: 0.00011008698493242264\n",
      "\t\tEpoch 66\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001508960558567196 Best: 0.00011008698493242264\n",
      "\t\tEpoch 67\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00011314298899378628 Best: 0.00011008698493242264\n",
      "\t\tEpoch 68\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00625\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 68\n",
      "Validation loss: 0.00010597737127682194 Best: 0.00010597737127682194\n",
      "\t\tEpoch 69\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 69\n",
      "Validation loss: 0.00010132473107660189 Best: 0.00010132473107660189\n",
      "\t\tEpoch 70\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 70\n",
      "Validation loss: 0.0001010083724395372 Best: 0.0001010083724395372\n",
      "\t\tEpoch 71\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 71\n",
      "Validation loss: 9.852234506979585e-05 Best: 9.852234506979585e-05\n",
      "\t\tEpoch 72\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00010426398512208834 Best: 9.852234506979585e-05\n",
      "\t\tEpoch 73\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00011683411139529198 Best: 9.852234506979585e-05\n",
      "\t\tEpoch 74\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 74\n",
      "Validation loss: 9.80742261162959e-05 Best: 9.80742261162959e-05\n",
      "\t\tEpoch 75\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00010470481356605887 Best: 9.80742261162959e-05\n",
      "\t\tEpoch 76\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00011776600877055898 Best: 9.80742261162959e-05\n",
      "\t\tEpoch 77\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00012393377255648375 Best: 9.80742261162959e-05\n",
      "\t\tEpoch 78\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001349307131022215 Best: 9.80742261162959e-05\n",
      "\t\tEpoch 79\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.003125\n",
      "Validation loss: 0.00013671712076757103 Best: 9.80742261162959e-05\n",
      "\t\tEpoch 80\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00012113626871723682 Best: 9.80742261162959e-05\n",
      "\t\tEpoch 81\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 81\n",
      "Validation loss: 9.42889746511355e-05 Best: 9.42889746511355e-05\n",
      "\t\tEpoch 82\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 82\n",
      "Validation loss: 8.327946125064045e-05 Best: 8.327946125064045e-05\n",
      "\t\tEpoch 83\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 83\n",
      "Validation loss: 7.958461355883628e-05 Best: 7.958461355883628e-05\n",
      "\t\tEpoch 84\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 8.840402006171644e-05 Best: 7.958461355883628e-05\n",
      "\t\tEpoch 85\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001015263405861333 Best: 7.958461355883628e-05\n",
      "\t\tEpoch 86\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 8.629544754512608e-05 Best: 7.958461355883628e-05\n",
      "\t\tEpoch 87\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 87\n",
      "Validation loss: 7.835795258870348e-05 Best: 7.835795258870348e-05\n",
      "\t\tEpoch 88\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 88\n",
      "Validation loss: 6.058832514099777e-05 Best: 6.058832514099777e-05\n",
      "\t\tEpoch 89\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 9.39264937187545e-05 Best: 6.058832514099777e-05\n",
      "\t\tEpoch 90\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0015625\n",
      "Validation loss: 0.00013765119365416467 Best: 6.058832514099777e-05\n",
      "\t\tEpoch 91\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00013349668006412685 Best: 6.058832514099777e-05\n",
      "\t\tEpoch 92\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00010497365292394534 Best: 6.058832514099777e-05\n",
      "\t\tEpoch 93\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00011443600669736043 Best: 6.058832514099777e-05\n",
      "\t\tEpoch 94\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00010701357678044587 Best: 6.058832514099777e-05\n",
      "\t\tEpoch 95\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00010526292317081243 Best: 6.058832514099777e-05\n",
      "\t\tEpoch 96\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00078125\n",
      "Validation loss: 7.93476210674271e-05 Best: 6.058832514099777e-05\n",
      "\t\tEpoch 97\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 9.083650365937501e-05 Best: 6.058832514099777e-05\n",
      "\t\tEpoch 98\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 7.576512871310115e-05 Best: 6.058832514099777e-05\n",
      "\t\tEpoch 99\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 8.22271904326044e-05 Best: 6.058832514099777e-05\n",
      "\t\tEpoch 100\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 8.94196709850803e-05 Best: 6.058832514099777e-05\n",
      "0.1533487636241587\n",
      "(54, 402)\n",
      "\t\t\tQuantile Network\n",
      "Use MondrianForestRegressor\n",
      "ymax and min: tensor(1.5571) tensor(-2.4763)\n",
      "\t\tEpoch 1\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 1\n",
      "Validation loss: 0.0010121396044269204 Best: 0.0010121396044269204\n",
      "\t\tEpoch 2\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 2\n",
      "Validation loss: 0.001002211240120232 Best: 0.001002211240120232\n",
      "\t\tEpoch 3\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 3\n",
      "Validation loss: 0.0009940990712493658 Best: 0.0009940990712493658\n",
      "\t\tEpoch 4\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 4\n",
      "Validation loss: 0.0009831914212554693 Best: 0.0009831914212554693\n",
      "\t\tEpoch 5\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 5\n",
      "Validation loss: 0.0009654149180278182 Best: 0.0009654149180278182\n",
      "\t\tEpoch 6\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 6\n",
      "Validation loss: 0.0009398387046530843 Best: 0.0009398387046530843\n",
      "\t\tEpoch 7\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 7\n",
      "Validation loss: 0.00091092090588063 Best: 0.00091092090588063\n",
      "\t\tEpoch 8\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 8\n",
      "Validation loss: 0.0008800314972177148 Best: 0.0008800314972177148\n",
      "\t\tEpoch 9\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 9\n",
      "Validation loss: 0.000850185111630708 Best: 0.000850185111630708\n",
      "\t\tEpoch 10\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 10\n",
      "Validation loss: 0.000829794560559094 Best: 0.000829794560559094\n",
      "\t\tEpoch 11\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 11\n",
      "Validation loss: 0.000801265356130898 Best: 0.000801265356130898\n",
      "\t\tEpoch 12\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 12\n",
      "Validation loss: 0.000776790315285325 Best: 0.000776790315285325\n",
      "\t\tEpoch 13\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 13\n",
      "Validation loss: 0.0007586217834614217 Best: 0.0007586217834614217\n",
      "\t\tEpoch 14\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 14\n",
      "Validation loss: 0.0007462670328095555 Best: 0.0007462670328095555\n",
      "\t\tEpoch 15\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 15\n",
      "Validation loss: 0.0007374977576546371 Best: 0.0007374977576546371\n",
      "\t\tEpoch 16\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0007376142893917859 Best: 0.0007374977576546371\n",
      "\t\tEpoch 17\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 17\n",
      "Validation loss: 0.0007327208295464516 Best: 0.0007327208295464516\n",
      "\t\tEpoch 18\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 18\n",
      "Validation loss: 0.0007276524556800723 Best: 0.0007276524556800723\n",
      "\t\tEpoch 19\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 19\n",
      "Validation loss: 0.0007155366474762559 Best: 0.0007155366474762559\n",
      "\t\tEpoch 20\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 20\n",
      "Validation loss: 0.0006969068781472743 Best: 0.0006969068781472743\n",
      "\t\tEpoch 21\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 21\n",
      "Validation loss: 0.0006767078302800655 Best: 0.0006767078302800655\n",
      "\t\tEpoch 22\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 22\n",
      "Validation loss: 0.000641884165816009 Best: 0.000641884165816009\n",
      "\t\tEpoch 23\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 23\n",
      "Validation loss: 0.0006110030808486044 Best: 0.0006110030808486044\n",
      "\t\tEpoch 24\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 24\n",
      "Validation loss: 0.0005879694363102317 Best: 0.0005879694363102317\n",
      "\t\tEpoch 25\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 25\n",
      "Validation loss: 0.0005738150211982429 Best: 0.0005738150211982429\n",
      "\t\tEpoch 26\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 26\n",
      "Validation loss: 0.0005484602297656238 Best: 0.0005484602297656238\n",
      "\t\tEpoch 27\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 27\n",
      "Validation loss: 0.0005214049597270787 Best: 0.0005214049597270787\n",
      "\t\tEpoch 28\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 28\n",
      "Validation loss: 0.00048230105312541127 Best: 0.00048230105312541127\n",
      "\t\tEpoch 29\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 29\n",
      "Validation loss: 0.0004562631365843117 Best: 0.0004562631365843117\n",
      "\t\tEpoch 30\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 30\n",
      "Validation loss: 0.0004392151313368231 Best: 0.0004392151313368231\n",
      "\t\tEpoch 31\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 31\n",
      "Validation loss: 0.00042478227987885475 Best: 0.00042478227987885475\n",
      "\t\tEpoch 32\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 32\n",
      "Validation loss: 0.00041657709516584873 Best: 0.00041657709516584873\n",
      "\t\tEpoch 33\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 33\n",
      "Validation loss: 0.00039998310967348516 Best: 0.00039998310967348516\n",
      "\t\tEpoch 34\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 34\n",
      "Validation loss: 0.0003842075529973954 Best: 0.0003842075529973954\n",
      "\t\tEpoch 35\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 35\n",
      "Validation loss: 0.00038004753878340125 Best: 0.00038004753878340125\n",
      "\t\tEpoch 36\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 36\n",
      "Validation loss: 0.0003643158415798098 Best: 0.0003643158415798098\n",
      "\t\tEpoch 37\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00037153431912884116 Best: 0.0003643158415798098\n",
      "\t\tEpoch 38\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00036949728382751346 Best: 0.0003643158415798098\n",
      "\t\tEpoch 39\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00036635115975514054 Best: 0.0003643158415798098\n",
      "\t\tEpoch 40\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 40\n",
      "Validation loss: 0.00035673106322064996 Best: 0.00035673106322064996\n",
      "\t\tEpoch 41\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 41\n",
      "Validation loss: 0.0003326566074974835 Best: 0.0003326566074974835\n",
      "\t\tEpoch 42\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 42\n",
      "Validation loss: 0.0003243890532758087 Best: 0.0003243890532758087\n",
      "\t\tEpoch 43\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003252744791097939 Best: 0.0003243890532758087\n",
      "\t\tEpoch 44\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 44\n",
      "Validation loss: 0.0003201215877197683 Best: 0.0003201215877197683\n",
      "\t\tEpoch 45\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00033510191133245826 Best: 0.0003201215877197683\n",
      "\t\tEpoch 46\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.05\n",
      "Validation loss: 0.0003466343623585999 Best: 0.0003201215877197683\n",
      "\t\tEpoch 47\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003641960211098194 Best: 0.0003201215877197683\n",
      "\t\tEpoch 48\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00038696284173056483 Best: 0.0003201215877197683\n",
      "\t\tEpoch 49\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00042329131974838674 Best: 0.0003201215877197683\n",
      "\t\tEpoch 50\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00043206807458773255 Best: 0.0003201215877197683\n",
      "\t\tEpoch 51\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00042203193879686296 Best: 0.0003201215877197683\n",
      "\t\tEpoch 52\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.025\n",
      "Validation loss: 0.00040592355071567 Best: 0.0003201215877197683\n",
      "\t\tEpoch 53\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00035346453660167754 Best: 0.0003201215877197683\n",
      "\t\tEpoch 54\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003646071127150208 Best: 0.0003201215877197683\n",
      "\t\tEpoch 55\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00036487707984633744 Best: 0.0003201215877197683\n",
      "\t\tEpoch 56\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00036654897849075496 Best: 0.0003201215877197683\n",
      "\t\tEpoch 57\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003797388926614076 Best: 0.0003201215877197683\n",
      "\t\tEpoch 58\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0125\n",
      "Validation loss: 0.00037035829154774547 Best: 0.0003201215877197683\n",
      "\t\tEpoch 59\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00035825654049403965 Best: 0.0003201215877197683\n",
      "\t\tEpoch 60\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003563127538654953 Best: 0.0003201215877197683\n",
      "\t\tEpoch 61\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003304150886833668 Best: 0.0003201215877197683\n",
      "\t\tEpoch 62\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00032821495551615953 Best: 0.0003201215877197683\n",
      "\t\tEpoch 63\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003454802790656686 Best: 0.0003201215877197683\n",
      "\t\tEpoch 64\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00625\n",
      "Validation loss: 0.0003624343662522733 Best: 0.0003201215877197683\n",
      "\t\tEpoch 65\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003728986484929919 Best: 0.0003201215877197683\n",
      "\t\tEpoch 66\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003935971180908382 Best: 0.0003201215877197683\n",
      "\t\tEpoch 67\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0004173576307948679 Best: 0.0003201215877197683\n",
      "\t\tEpoch 68\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.000419821182731539 Best: 0.0003201215877197683\n",
      "\t\tEpoch 69\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00041258492274209857 Best: 0.0003201215877197683\n",
      "\t\tEpoch 70\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.003125\n",
      "Validation loss: 0.00039564940379932523 Best: 0.0003201215877197683\n",
      "\t\tEpoch 71\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00038825577939860523 Best: 0.0003201215877197683\n",
      "\t\tEpoch 72\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003755228244699538 Best: 0.0003201215877197683\n",
      "\t\tEpoch 73\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003636149631347507 Best: 0.0003201215877197683\n",
      "\t\tEpoch 74\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00036557321436703205 Best: 0.0003201215877197683\n",
      "\t\tEpoch 75\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00035830881097353995 Best: 0.0003201215877197683\n",
      "\t\tEpoch 76\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0015625\n",
      "Validation loss: 0.00035226024920120835 Best: 0.0003201215877197683\n",
      "\t\tEpoch 77\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00035605073207989335 Best: 0.0003201215877197683\n",
      "\t\tEpoch 78\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003497673023957759 Best: 0.0003201215877197683\n",
      "\t\tEpoch 79\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003525916545186192 Best: 0.0003201215877197683\n",
      "\t\tEpoch 80\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00034602603409439325 Best: 0.0003201215877197683\n",
      "\t\tEpoch 81\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00034538289764896035 Best: 0.0003201215877197683\n",
      "\t\tEpoch 82\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00078125\n",
      "Validation loss: 0.00035019073402509093 Best: 0.0003201215877197683\n",
      "\t\tEpoch 83\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003513213596306741 Best: 0.0003201215877197683\n",
      "\t\tEpoch 84\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003497231809888035 Best: 0.0003201215877197683\n",
      "\t\tEpoch 85\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003347382298670709 Best: 0.0003201215877197683\n",
      "\t\tEpoch 86\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00033658361644484103 Best: 0.0003201215877197683\n",
      "\t\tEpoch 87\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003398211265448481 Best: 0.0003201215877197683\n",
      "\t\tEpoch 88\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.000390625\n",
      "Validation loss: 0.0003431841905694455 Best: 0.0003201215877197683\n",
      "\t\tEpoch 89\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.000346056476701051 Best: 0.0003201215877197683\n",
      "\t\tEpoch 90\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00034092721762135625 Best: 0.0003201215877197683\n",
      "\t\tEpoch 91\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003289909800514579 Best: 0.0003201215877197683\n",
      "\t\tEpoch 92\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 92\n",
      "Validation loss: 0.0003166022361256182 Best: 0.0003166022361256182\n",
      "\t\tEpoch 93\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 93\n",
      "Validation loss: 0.00030608734232373536 Best: 0.00030608734232373536\n",
      "\t\tEpoch 94\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 94\n",
      "Validation loss: 0.0002952603972516954 Best: 0.0002952603972516954\n",
      "\t\tEpoch 95\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 95\n",
      "Validation loss: 0.00028824922628700733 Best: 0.00028824922628700733\n",
      "\t\tEpoch 96\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 96\n",
      "Validation loss: 0.00028189440490677953 Best: 0.00028189440490677953\n",
      "\t\tEpoch 97\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00029172920039854944 Best: 0.00028189440490677953\n",
      "\t\tEpoch 98\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003015414986293763 Best: 0.00028189440490677953\n",
      "\t\tEpoch 99\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0001953125\n",
      "Validation loss: 0.0003181294887326658 Best: 0.00028189440490677953\n",
      "\t\tEpoch 100\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00033466663444414735 Best: 0.00028189440490677953\n",
      "0.09842139863315431\n",
      "(54, 402)\n",
      "\t\t\tQuantile Network\n",
      "Use MondrianForestRegressor\n",
      "ymax and min: tensor(1.5820) tensor(-2.5852)\n",
      "\t\tEpoch 1\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 1\n",
      "Validation loss: 0.0008663475746288896 Best: 0.0008663475746288896\n",
      "\t\tEpoch 2\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0008801347576081753 Best: 0.0008663475746288896\n",
      "\t\tEpoch 3\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00089269905583933 Best: 0.0008663475746288896\n",
      "\t\tEpoch 4\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0009032431989908218 Best: 0.0008663475746288896\n",
      "\t\tEpoch 5\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0009082502801902592 Best: 0.0008663475746288896\n",
      "\t\tEpoch 6\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.000912549439817667 Best: 0.0008663475746288896\n",
      "\t\tEpoch 7\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0009085467318072915 Best: 0.0008663475746288896\n",
      "\t\tEpoch 8\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.05\n",
      "Validation loss: 0.0009012830560095608 Best: 0.0008663475746288896\n",
      "\t\tEpoch 9\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0008875861531123519 Best: 0.0008663475746288896\n",
      "\t\tEpoch 10\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0008711580303497612 Best: 0.0008663475746288896\n",
      "\t\tEpoch 11\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 11\n",
      "Validation loss: 0.0008581133442930877 Best: 0.0008581133442930877\n",
      "\t\tEpoch 12\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 12\n",
      "Validation loss: 0.000835329934488982 Best: 0.000835329934488982\n",
      "\t\tEpoch 13\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 13\n",
      "Validation loss: 0.0008189999498426914 Best: 0.0008189999498426914\n",
      "\t\tEpoch 14\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 14\n",
      "Validation loss: 0.0008034853381104767 Best: 0.0008034853381104767\n",
      "\t\tEpoch 15\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 15\n",
      "Validation loss: 0.0007812164258211851 Best: 0.0007812164258211851\n",
      "\t\tEpoch 16\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 16\n",
      "Validation loss: 0.0007582844118587673 Best: 0.0007582844118587673\n",
      "\t\tEpoch 17\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 17\n",
      "Validation loss: 0.0007435375591740012 Best: 0.0007435375591740012\n",
      "\t\tEpoch 18\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 18\n",
      "Validation loss: 0.0007207413436844945 Best: 0.0007207413436844945\n",
      "\t\tEpoch 19\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 19\n",
      "Validation loss: 0.0007012260612100363 Best: 0.0007012260612100363\n",
      "\t\tEpoch 20\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 20\n",
      "Validation loss: 0.0006751117762178183 Best: 0.0006751117762178183\n",
      "\t\tEpoch 21\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 21\n",
      "Validation loss: 0.0006622158689424396 Best: 0.0006622158689424396\n",
      "\t\tEpoch 22\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 22\n",
      "Validation loss: 0.0006334506906569004 Best: 0.0006334506906569004\n",
      "\t\tEpoch 23\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 23\n",
      "Validation loss: 0.0005966848693788052 Best: 0.0005966848693788052\n",
      "\t\tEpoch 24\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 24\n",
      "Validation loss: 0.0005596698028966784 Best: 0.0005596698028966784\n",
      "\t\tEpoch 25\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 25\n",
      "Validation loss: 0.0005249588866718113 Best: 0.0005249588866718113\n",
      "\t\tEpoch 26\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 26\n",
      "Validation loss: 0.0005156401894055307 Best: 0.0005156401894055307\n",
      "\t\tEpoch 27\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 27\n",
      "Validation loss: 0.0005005497951060534 Best: 0.0005005497951060534\n",
      "\t\tEpoch 28\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 28\n",
      "Validation loss: 0.0004901899374090135 Best: 0.0004901899374090135\n",
      "\t\tEpoch 29\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 29\n",
      "Validation loss: 0.00046270721941255033 Best: 0.00046270721941255033\n",
      "\t\tEpoch 30\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 30\n",
      "Validation loss: 0.0004390400426927954 Best: 0.0004390400426927954\n",
      "\t\tEpoch 31\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 31\n",
      "Validation loss: 0.0004278684500604868 Best: 0.0004278684500604868\n",
      "\t\tEpoch 32\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 32\n",
      "Validation loss: 0.00039534535608254373 Best: 0.00039534535608254373\n",
      "\t\tEpoch 33\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 33\n",
      "Validation loss: 0.00037530524423345923 Best: 0.00037530524423345923\n",
      "\t\tEpoch 34\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 34\n",
      "Validation loss: 0.00035409245174378157 Best: 0.00035409245174378157\n",
      "\t\tEpoch 35\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 35\n",
      "Validation loss: 0.00033822638215497136 Best: 0.00033822638215497136\n",
      "\t\tEpoch 36\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 36\n",
      "Validation loss: 0.0003110881079919636 Best: 0.0003110881079919636\n",
      "\t\tEpoch 37\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 37\n",
      "Validation loss: 0.00028941151686012745 Best: 0.00028941151686012745\n",
      "\t\tEpoch 38\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 38\n",
      "Validation loss: 0.0002701682678889483 Best: 0.0002701682678889483\n",
      "\t\tEpoch 39\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 39\n",
      "Validation loss: 0.00025151632144115865 Best: 0.00025151632144115865\n",
      "\t\tEpoch 40\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 40\n",
      "Validation loss: 0.00023594126105308533 Best: 0.00023594126105308533\n",
      "\t\tEpoch 41\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 41\n",
      "Validation loss: 0.00023581544519402087 Best: 0.00023581544519402087\n",
      "\t\tEpoch 42\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 42\n",
      "Validation loss: 0.00022630122839473188 Best: 0.00022630122839473188\n",
      "\t\tEpoch 43\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 43\n",
      "Validation loss: 0.00022148541756905615 Best: 0.00022148541756905615\n",
      "\t\tEpoch 44\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 44\n",
      "Validation loss: 0.00021511076192837209 Best: 0.00021511076192837209\n",
      "\t\tEpoch 45\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 45\n",
      "Validation loss: 0.00021486131299752742 Best: 0.00021486131299752742\n",
      "\t\tEpoch 46\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0002280470507685095 Best: 0.00021486131299752742\n",
      "\t\tEpoch 47\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00022625489509664476 Best: 0.00021486131299752742\n",
      "\t\tEpoch 48\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00022370829537976533 Best: 0.00021486131299752742\n",
      "\t\tEpoch 49\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.025\n",
      "Validation loss: 0.0002235287392977625 Best: 0.00021486131299752742\n",
      "\t\tEpoch 50\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00022543102386407554 Best: 0.00021486131299752742\n",
      "\t\tEpoch 51\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00022781228472013026 Best: 0.00021486131299752742\n",
      "\t\tEpoch 52\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00022096489556133747 Best: 0.00021486131299752742\n",
      "\t\tEpoch 53\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 53\n",
      "Validation loss: 0.00020970783953089267 Best: 0.00020970783953089267\n",
      "\t\tEpoch 54\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 54\n",
      "Validation loss: 0.0002093523507937789 Best: 0.0002093523507937789\n",
      "\t\tEpoch 55\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 55\n",
      "Validation loss: 0.0002011250180657953 Best: 0.0002011250180657953\n",
      "\t\tEpoch 56\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 56\n",
      "Validation loss: 0.00019029188842978328 Best: 0.00019029188842978328\n",
      "\t\tEpoch 57\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 57\n",
      "Validation loss: 0.0001788211229722947 Best: 0.0001788211229722947\n",
      "\t\tEpoch 58\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001821686455514282 Best: 0.0001788211229722947\n",
      "\t\tEpoch 59\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 59\n",
      "Validation loss: 0.00017299465253017843 Best: 0.00017299465253017843\n",
      "\t\tEpoch 60\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 60\n",
      "Validation loss: 0.00016868923557922244 Best: 0.00016868923557922244\n",
      "\t\tEpoch 61\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 61\n",
      "Validation loss: 0.00016595244233030826 Best: 0.00016595244233030826\n",
      "\t\tEpoch 62\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 62\n",
      "Validation loss: 0.0001623604621272534 Best: 0.0001623604621272534\n",
      "\t\tEpoch 63\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 63\n",
      "Validation loss: 0.00016142375534400344 Best: 0.00016142375534400344\n",
      "\t\tEpoch 64\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00016183109255507588 Best: 0.00016142375534400344\n",
      "\t\tEpoch 65\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0125\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 65\n",
      "Validation loss: 0.00015637758770026267 Best: 0.00015637758770026267\n",
      "\t\tEpoch 66\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001591646869201213 Best: 0.00015637758770026267\n",
      "\t\tEpoch 67\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00016221075202338398 Best: 0.00015637758770026267\n",
      "\t\tEpoch 68\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001670439523877576 Best: 0.00015637758770026267\n",
      "\t\tEpoch 69\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00016629639139864594 Best: 0.00015637758770026267\n",
      "\t\tEpoch 70\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00016164386761374772 Best: 0.00015637758770026267\n",
      "\t\tEpoch 71\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00016187150322366506 Best: 0.00015637758770026267\n",
      "\t\tEpoch 72\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00625\n",
      "Validation loss: 0.00015655826427973807 Best: 0.00015637758770026267\n",
      "\t\tEpoch 73\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001604601857252419 Best: 0.00015637758770026267\n",
      "\t\tEpoch 74\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001651736383792013 Best: 0.00015637758770026267\n",
      "\t\tEpoch 75\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001678138942224905 Best: 0.00015637758770026267\n",
      "\t\tEpoch 76\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00016505412349943072 Best: 0.00015637758770026267\n",
      "\t\tEpoch 77\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00016493527800776064 Best: 0.00015637758770026267\n",
      "\t\tEpoch 78\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.003125\n",
      "Validation loss: 0.00016709677584003657 Best: 0.00015637758770026267\n",
      "\t\tEpoch 79\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00017352843133267015 Best: 0.00015637758770026267\n",
      "\t\tEpoch 80\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00016873065032996237 Best: 0.00015637758770026267\n",
      "\t\tEpoch 81\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001606121368240565 Best: 0.00015637758770026267\n",
      "\t\tEpoch 82\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00016373893595300615 Best: 0.00015637758770026267\n",
      "\t\tEpoch 83\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00016304566815961152 Best: 0.00015637758770026267\n",
      "\t\tEpoch 84\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0015625\n",
      "Validation loss: 0.00016924671945162117 Best: 0.00015637758770026267\n",
      "\t\tEpoch 85\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001639268739381805 Best: 0.00015637758770026267\n",
      "\t\tEpoch 86\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001708519266685471 Best: 0.00015637758770026267\n",
      "\t\tEpoch 87\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.000181173236342147 Best: 0.00015637758770026267\n",
      "\t\tEpoch 88\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.000182283140020445 Best: 0.00015637758770026267\n",
      "\t\tEpoch 89\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00018676517356652766 Best: 0.00015637758770026267\n",
      "\t\tEpoch 90\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00078125\n",
      "Validation loss: 0.00019310854258947074 Best: 0.00015637758770026267\n",
      "\t\tEpoch 91\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00020061945542693138 Best: 0.00015637758770026267\n",
      "\t\tEpoch 92\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00020752921409439296 Best: 0.00015637758770026267\n",
      "\t\tEpoch 93\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00020397119806148112 Best: 0.00015637758770026267\n",
      "\t\tEpoch 94\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00019793487444985658 Best: 0.00015637758770026267\n",
      "\t\tEpoch 95\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00019049356342293322 Best: 0.00015637758770026267\n",
      "\t\tEpoch 96\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.000390625\n",
      "Validation loss: 0.0001890182902570814 Best: 0.00015637758770026267\n",
      "\t\tEpoch 97\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00018639348854776472 Best: 0.00015637758770026267\n",
      "\t\tEpoch 98\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00018649088451638818 Best: 0.00015637758770026267\n",
      "\t\tEpoch 99\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001812553236959502 Best: 0.00015637758770026267\n",
      "\t\tEpoch 100\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001746553898556158 Best: 0.00015637758770026267\n",
      "0.054575467439622584\n",
      "(54, 402)\n",
      "\t\t\tQuantile Network\n",
      "Use MondrianForestRegressor\n",
      "ymax and min: tensor(1.5790) tensor(-2.4259)\n",
      "\t\tEpoch 1\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 1\n",
      "Validation loss: 0.0009067620849236846 Best: 0.0009067620849236846\n",
      "\t\tEpoch 2\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 2\n",
      "Validation loss: 0.0009009542991407216 Best: 0.0009009542991407216\n",
      "\t\tEpoch 3\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 3\n",
      "Validation loss: 0.0008929132600314915 Best: 0.0008929132600314915\n",
      "\t\tEpoch 4\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 4\n",
      "Validation loss: 0.000882417312823236 Best: 0.000882417312823236\n",
      "\t\tEpoch 5\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 5\n",
      "Validation loss: 0.0008702532504685223 Best: 0.0008702532504685223\n",
      "\t\tEpoch 6\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 6\n",
      "Validation loss: 0.000859370338730514 Best: 0.000859370338730514\n",
      "\t\tEpoch 7\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 7\n",
      "Validation loss: 0.0008450577151961625 Best: 0.0008450577151961625\n",
      "\t\tEpoch 8\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 8\n",
      "Validation loss: 0.000829063996206969 Best: 0.000829063996206969\n",
      "\t\tEpoch 9\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 9\n",
      "Validation loss: 0.0008190762018784881 Best: 0.0008190762018784881\n",
      "\t\tEpoch 10\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 10\n",
      "Validation loss: 0.0008082735585048795 Best: 0.0008082735585048795\n",
      "\t\tEpoch 11\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 11\n",
      "Validation loss: 0.0007958733476698399 Best: 0.0007958733476698399\n",
      "\t\tEpoch 12\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 12\n",
      "Validation loss: 0.0007856530137360096 Best: 0.0007856530137360096\n",
      "\t\tEpoch 13\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 13\n",
      "Validation loss: 0.0007741658482700586 Best: 0.0007741658482700586\n",
      "\t\tEpoch 14\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 14\n",
      "Validation loss: 0.0007632431224919856 Best: 0.0007632431224919856\n",
      "\t\tEpoch 15\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 15\n",
      "Validation loss: 0.0007481072098016739 Best: 0.0007481072098016739\n",
      "\t\tEpoch 16\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 16\n",
      "Validation loss: 0.0007324746111407876 Best: 0.0007324746111407876\n",
      "\t\tEpoch 17\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 17\n",
      "Validation loss: 0.0007199654937721789 Best: 0.0007199654937721789\n",
      "\t\tEpoch 18\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 18\n",
      "Validation loss: 0.0007164732669480145 Best: 0.0007164732669480145\n",
      "\t\tEpoch 19\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 19\n",
      "Validation loss: 0.0006941193132661283 Best: 0.0006941193132661283\n",
      "\t\tEpoch 20\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 20\n",
      "Validation loss: 0.0006748939049430192 Best: 0.0006748939049430192\n",
      "\t\tEpoch 21\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 21\n",
      "Validation loss: 0.0006456468836404383 Best: 0.0006456468836404383\n",
      "\t\tEpoch 22\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 22\n",
      "Validation loss: 0.0006261486560106277 Best: 0.0006261486560106277\n",
      "\t\tEpoch 23\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 23\n",
      "Validation loss: 0.0006070270319469273 Best: 0.0006070270319469273\n",
      "\t\tEpoch 24\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 24\n",
      "Validation loss: 0.0005850191228091717 Best: 0.0005850191228091717\n",
      "\t\tEpoch 25\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 25\n",
      "Validation loss: 0.0005732006393373013 Best: 0.0005732006393373013\n",
      "\t\tEpoch 26\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 26\n",
      "Validation loss: 0.0005662744515575469 Best: 0.0005662744515575469\n",
      "\t\tEpoch 27\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0005737993633374572 Best: 0.0005662744515575469\n",
      "\t\tEpoch 28\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0005663710762746632 Best: 0.0005662744515575469\n",
      "\t\tEpoch 29\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 29\n",
      "Validation loss: 0.0005571732763200998 Best: 0.0005571732763200998\n",
      "\t\tEpoch 30\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 30\n",
      "Validation loss: 0.0005377248744480312 Best: 0.0005377248744480312\n",
      "\t\tEpoch 31\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 31\n",
      "Validation loss: 0.0005371899460442364 Best: 0.0005371899460442364\n",
      "\t\tEpoch 32\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 32\n",
      "Validation loss: 0.0005223354091867805 Best: 0.0005223354091867805\n",
      "\t\tEpoch 33\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 33\n",
      "Validation loss: 0.000511840102262795 Best: 0.000511840102262795\n",
      "\t\tEpoch 34\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 34\n",
      "Validation loss: 0.0004956962075084448 Best: 0.0004956962075084448\n",
      "\t\tEpoch 35\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 35\n",
      "Validation loss: 0.0004667842877097428 Best: 0.0004667842877097428\n",
      "\t\tEpoch 36\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 36\n",
      "Validation loss: 0.00045354251051321626 Best: 0.00045354251051321626\n",
      "\t\tEpoch 37\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 37\n",
      "Validation loss: 0.0004426707455422729 Best: 0.0004426707455422729\n",
      "\t\tEpoch 38\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0004498812777455896 Best: 0.0004426707455422729\n",
      "\t\tEpoch 39\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.000449988292530179 Best: 0.0004426707455422729\n",
      "\t\tEpoch 40\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00045635076821781695 Best: 0.0004426707455422729\n",
      "\t\tEpoch 41\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00047658057883381844 Best: 0.0004426707455422729\n",
      "\t\tEpoch 42\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.05\n",
      "Validation loss: 0.0004968555876985192 Best: 0.0004426707455422729\n",
      "\t\tEpoch 43\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0005055232322774827 Best: 0.0004426707455422729\n",
      "\t\tEpoch 44\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0005074600921943784 Best: 0.0004426707455422729\n",
      "\t\tEpoch 45\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0004914675955660641 Best: 0.0004426707455422729\n",
      "\t\tEpoch 46\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00047328873188234866 Best: 0.0004426707455422729\n",
      "\t\tEpoch 47\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00045052534551359713 Best: 0.0004426707455422729\n",
      "\t\tEpoch 48\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.025\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 48\n",
      "Validation loss: 0.00043951423140242696 Best: 0.00043951423140242696\n",
      "\t\tEpoch 49\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00044170781620778143 Best: 0.00043951423140242696\n",
      "\t\tEpoch 50\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 50\n",
      "Validation loss: 0.0004362969193607569 Best: 0.0004362969193607569\n",
      "\t\tEpoch 51\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0004425228980835527 Best: 0.0004362969193607569\n",
      "\t\tEpoch 52\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0004549214500002563 Best: 0.0004362969193607569\n",
      "\t\tEpoch 53\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0004548655415419489 Best: 0.0004362969193607569\n",
      "\t\tEpoch 54\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0004731239750981331 Best: 0.0004362969193607569\n",
      "\t\tEpoch 55\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0004758852592203766 Best: 0.0004362969193607569\n",
      "\t\tEpoch 56\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0125\n",
      "Validation loss: 0.00046814250526949763 Best: 0.0004362969193607569\n",
      "\t\tEpoch 57\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.000466460915049538 Best: 0.0004362969193607569\n",
      "\t\tEpoch 58\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00044832247658632696 Best: 0.0004362969193607569\n",
      "\t\tEpoch 59\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00046267424477264285 Best: 0.0004362969193607569\n",
      "\t\tEpoch 60\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00046051101526245475 Best: 0.0004362969193607569\n",
      "\t\tEpoch 61\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0004571288009174168 Best: 0.0004362969193607569\n",
      "\t\tEpoch 62\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00625\n",
      "Validation loss: 0.00045148571371100843 Best: 0.0004362969193607569\n",
      "\t\tEpoch 63\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0004507324192672968 Best: 0.0004362969193607569\n",
      "\t\tEpoch 64\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00044185551814734936 Best: 0.0004362969193607569\n",
      "\t\tEpoch 65\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00043672011815942824 Best: 0.0004362969193607569\n",
      "\t\tEpoch 66\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0004464793892111629 Best: 0.0004362969193607569\n",
      "\t\tEpoch 67\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00044898519990965724 Best: 0.0004362969193607569\n",
      "\t\tEpoch 68\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.003125\n",
      "Validation loss: 0.00044935932965017855 Best: 0.0004362969193607569\n",
      "\t\tEpoch 69\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00045309410779736936 Best: 0.0004362969193607569\n",
      "\t\tEpoch 70\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00045101382420398295 Best: 0.0004362969193607569\n",
      "\t\tEpoch 71\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0004630773910321295 Best: 0.0004362969193607569\n",
      "\t\tEpoch 72\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00046561285853385925 Best: 0.0004362969193607569\n",
      "\t\tEpoch 73\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0004563581314869225 Best: 0.0004362969193607569\n",
      "\t\tEpoch 74\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0015625\n",
      "Validation loss: 0.0004455100861378014 Best: 0.0004362969193607569\n",
      "\t\tEpoch 75\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00044192891800776124 Best: 0.0004362969193607569\n",
      "\t\tEpoch 76\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 76\n",
      "Validation loss: 0.0004280067514628172 Best: 0.0004280067514628172\n",
      "\t\tEpoch 77\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 77\n",
      "Validation loss: 0.0004255057720001787 Best: 0.0004255057720001787\n",
      "\t\tEpoch 78\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00043635041220113635 Best: 0.0004255057720001787\n",
      "\t\tEpoch 79\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00044676513061858714 Best: 0.0004255057720001787\n",
      "\t\tEpoch 80\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0004491349682211876 Best: 0.0004255057720001787\n",
      "\t\tEpoch 81\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0004505063989199698 Best: 0.0004255057720001787\n",
      "\t\tEpoch 82\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00078125\n",
      "Validation loss: 0.00046141864731907845 Best: 0.0004255057720001787\n",
      "\t\tEpoch 83\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.000461858871858567 Best: 0.0004255057720001787\n",
      "\t\tEpoch 84\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0004672918585129082 Best: 0.0004255057720001787\n",
      "\t\tEpoch 85\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00047318023280240595 Best: 0.0004255057720001787\n",
      "\t\tEpoch 86\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00046201905934140086 Best: 0.0004255057720001787\n",
      "\t\tEpoch 87\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00045745837269350886 Best: 0.0004255057720001787\n",
      "\t\tEpoch 88\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.000390625\n",
      "Validation loss: 0.0004435766604728997 Best: 0.0004255057720001787\n",
      "\t\tEpoch 89\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00045159761793911457 Best: 0.0004255057720001787\n",
      "\t\tEpoch 90\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0004486239922698587 Best: 0.0004255057720001787\n",
      "\t\tEpoch 91\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00044905178947374225 Best: 0.0004255057720001787\n",
      "\t\tEpoch 92\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.000461207062471658 Best: 0.0004255057720001787\n",
      "\t\tEpoch 93\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.000472657207865268 Best: 0.0004255057720001787\n",
      "\t\tEpoch 94\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0001953125\n",
      "Validation loss: 0.00046226437552832067 Best: 0.0004255057720001787\n",
      "\t\tEpoch 95\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0004710446228273213 Best: 0.0004255057720001787\n",
      "\t\tEpoch 96\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00046205343096517026 Best: 0.0004255057720001787\n",
      "\t\tEpoch 97\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0004667911562137306 Best: 0.0004255057720001787\n",
      "\t\tEpoch 98\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0004628072492778301 Best: 0.0004255057720001787\n",
      "\t\tEpoch 99\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0004704603925347328 Best: 0.0004255057720001787\n",
      "\t\tEpoch 100\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 9.765625e-05\n",
      "Validation loss: 0.0004681643040385097 Best: 0.0004255057720001787\n",
      "0.14348463125732036\n",
      "(54, 402)\n",
      "\t\t\tQuantile Network\n",
      "Use MondrianForestRegressor\n",
      "ymax and min: tensor(1.5784) tensor(-2.5566)\n",
      "\t\tEpoch 1\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 1\n",
      "Validation loss: 0.0005666055949404836 Best: 0.0005666055949404836\n",
      "\t\tEpoch 2\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 2\n",
      "Validation loss: 0.0005623909528367221 Best: 0.0005623909528367221\n",
      "\t\tEpoch 3\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 3\n",
      "Validation loss: 0.0005563587765209377 Best: 0.0005563587765209377\n",
      "\t\tEpoch 4\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 4\n",
      "Validation loss: 0.0005488895694725215 Best: 0.0005488895694725215\n",
      "\t\tEpoch 5\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 5\n",
      "Validation loss: 0.0005438820226117969 Best: 0.0005438820226117969\n",
      "\t\tEpoch 6\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 6\n",
      "Validation loss: 0.000530609511770308 Best: 0.000530609511770308\n",
      "\t\tEpoch 7\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 7\n",
      "Validation loss: 0.0005187437636777759 Best: 0.0005187437636777759\n",
      "\t\tEpoch 8\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 8\n",
      "Validation loss: 0.000503833289258182 Best: 0.000503833289258182\n",
      "\t\tEpoch 9\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 9\n",
      "Validation loss: 0.0004947339766658843 Best: 0.0004947339766658843\n",
      "\t\tEpoch 10\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 10\n",
      "Validation loss: 0.00048191583482548594 Best: 0.00048191583482548594\n",
      "\t\tEpoch 11\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 11\n",
      "Validation loss: 0.0004640767292585224 Best: 0.0004640767292585224\n",
      "\t\tEpoch 12\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 12\n",
      "Validation loss: 0.0004547070129774511 Best: 0.0004547070129774511\n",
      "\t\tEpoch 13\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 13\n",
      "Validation loss: 0.000449631130322814 Best: 0.000449631130322814\n",
      "\t\tEpoch 14\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 14\n",
      "Validation loss: 0.0004376551660243422 Best: 0.0004376551660243422\n",
      "\t\tEpoch 15\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 15\n",
      "Validation loss: 0.00043234616168774664 Best: 0.00043234616168774664\n",
      "\t\tEpoch 16\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 16\n",
      "Validation loss: 0.0004285523609723896 Best: 0.0004285523609723896\n",
      "\t\tEpoch 17\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 17\n",
      "Validation loss: 0.0004249061457812786 Best: 0.0004249061457812786\n",
      "\t\tEpoch 18\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 18\n",
      "Validation loss: 0.00041990960016846657 Best: 0.00041990960016846657\n",
      "\t\tEpoch 19\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00042346789268776774 Best: 0.00041990960016846657\n",
      "\t\tEpoch 20\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 20\n",
      "Validation loss: 0.0004198671376798302 Best: 0.0004198671376798302\n",
      "\t\tEpoch 21\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 21\n",
      "Validation loss: 0.0004117712960578501 Best: 0.0004117712960578501\n",
      "\t\tEpoch 22\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 22\n",
      "Validation loss: 0.00040570582496002316 Best: 0.00040570582496002316\n",
      "\t\tEpoch 23\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 23\n",
      "Validation loss: 0.0003885609039571136 Best: 0.0003885609039571136\n",
      "\t\tEpoch 24\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 24\n",
      "Validation loss: 0.0003851474612019956 Best: 0.0003851474612019956\n",
      "\t\tEpoch 25\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 25\n",
      "Validation loss: 0.0003685167757794261 Best: 0.0003685167757794261\n",
      "\t\tEpoch 26\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003818674595095217 Best: 0.0003685167757794261\n",
      "\t\tEpoch 27\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.000375403615180403 Best: 0.0003685167757794261\n",
      "\t\tEpoch 28\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 28\n",
      "Validation loss: 0.0003508357622195035 Best: 0.0003508357622195035\n",
      "\t\tEpoch 29\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 29\n",
      "Validation loss: 0.0003374853986315429 Best: 0.0003374853986315429\n",
      "\t\tEpoch 30\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 30\n",
      "Validation loss: 0.0003089842211920768 Best: 0.0003089842211920768\n",
      "\t\tEpoch 31\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 31\n",
      "Validation loss: 0.00028845324413850904 Best: 0.00028845324413850904\n",
      "\t\tEpoch 32\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 32\n",
      "Validation loss: 0.00027120421873405576 Best: 0.00027120421873405576\n",
      "\t\tEpoch 33\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 33\n",
      "Validation loss: 0.00024284662504214793 Best: 0.00024284662504214793\n",
      "\t\tEpoch 34\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 34\n",
      "Validation loss: 0.00020986274466849864 Best: 0.00020986274466849864\n",
      "\t\tEpoch 35\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 35\n",
      "Validation loss: 0.00017679552547633648 Best: 0.00017679552547633648\n",
      "\t\tEpoch 36\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 36\n",
      "Validation loss: 0.00013753084931522608 Best: 0.00013753084931522608\n",
      "\t\tEpoch 37\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 37\n",
      "Validation loss: 0.00012043237074976787 Best: 0.00012043237074976787\n",
      "\t\tEpoch 38\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 38\n",
      "Validation loss: 0.00011286632798146456 Best: 0.00011286632798146456\n",
      "\t\tEpoch 39\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 39\n",
      "Validation loss: 0.00010048246622318402 Best: 0.00010048246622318402\n",
      "\t\tEpoch 40\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00010331490921089426 Best: 0.00010048246622318402\n",
      "\t\tEpoch 41\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00010522615775698796 Best: 0.00010048246622318402\n",
      "\t\tEpoch 42\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00013248171308077872 Best: 0.00010048246622318402\n",
      "\t\tEpoch 43\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.05\n",
      "Validation loss: 0.00017306310473941267 Best: 0.00010048246622318402\n",
      "\t\tEpoch 44\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00018974582781083882 Best: 0.00010048246622318402\n",
      "\t\tEpoch 45\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001681522116996348 Best: 0.00010048246622318402\n",
      "\t\tEpoch 46\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00015925770276226103 Best: 0.00010048246622318402\n",
      "\t\tEpoch 47\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00015732250176370144 Best: 0.00010048246622318402\n",
      "\t\tEpoch 48\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00017744199431035668 Best: 0.00010048246622318402\n",
      "\t\tEpoch 49\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.025\n",
      "Validation loss: 0.0001574414927745238 Best: 0.00010048246622318402\n",
      "\t\tEpoch 50\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001585522695677355 Best: 0.00010048246622318402\n",
      "\t\tEpoch 51\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00016164392582140863 Best: 0.00010048246622318402\n",
      "\t\tEpoch 52\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00017284229397773743 Best: 0.00010048246622318402\n",
      "\t\tEpoch 53\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00018185432418249547 Best: 0.00010048246622318402\n",
      "\t\tEpoch 54\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001791392278391868 Best: 0.00010048246622318402\n",
      "\t\tEpoch 55\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0125\n",
      "Validation loss: 0.0001679709821473807 Best: 0.00010048246622318402\n",
      "\t\tEpoch 56\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00017354679584968835 Best: 0.00010048246622318402\n",
      "\t\tEpoch 57\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00017999812553171068 Best: 0.00010048246622318402\n",
      "\t\tEpoch 58\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00018574872228782624 Best: 0.00010048246622318402\n",
      "\t\tEpoch 59\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.000214110899833031 Best: 0.00010048246622318402\n",
      "\t\tEpoch 60\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00022470942349173129 Best: 0.00010048246622318402\n",
      "\t\tEpoch 61\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00625\n",
      "Validation loss: 0.00023562325804959983 Best: 0.00010048246622318402\n",
      "\t\tEpoch 62\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00023431160661857575 Best: 0.00010048246622318402\n",
      "\t\tEpoch 63\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00022374212858267128 Best: 0.00010048246622318402\n",
      "\t\tEpoch 64\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0002133638336090371 Best: 0.00010048246622318402\n",
      "\t\tEpoch 65\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00020120567933190614 Best: 0.00010048246622318402\n",
      "\t\tEpoch 66\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00017950314213521779 Best: 0.00010048246622318402\n",
      "\t\tEpoch 67\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.003125\n",
      "Validation loss: 0.00016452078125439584 Best: 0.00010048246622318402\n",
      "\t\tEpoch 68\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00015195026935543865 Best: 0.00010048246622318402\n",
      "\t\tEpoch 69\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.000144336445373483 Best: 0.00010048246622318402\n",
      "\t\tEpoch 70\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00013884546933695674 Best: 0.00010048246622318402\n",
      "\t\tEpoch 71\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001198504469357431 Best: 0.00010048246622318402\n",
      "\t\tEpoch 72\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00011587579501792789 Best: 0.00010048246622318402\n",
      "\t\tEpoch 73\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0015625\n",
      "Validation loss: 0.00011249589442741126 Best: 0.00010048246622318402\n",
      "\t\tEpoch 74\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00011401886877138168 Best: 0.00010048246622318402\n",
      "\t\tEpoch 75\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00010883262439165264 Best: 0.00010048246622318402\n",
      "\t\tEpoch 76\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00012545855133794248 Best: 0.00010048246622318402\n",
      "\t\tEpoch 77\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001259115379070863 Best: 0.00010048246622318402\n",
      "\t\tEpoch 78\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00013503954687621444 Best: 0.00010048246622318402\n",
      "\t\tEpoch 79\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00078125\n",
      "Validation loss: 0.00013107177801430225 Best: 0.00010048246622318402\n",
      "\t\tEpoch 80\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00013684167061001062 Best: 0.00010048246622318402\n",
      "\t\tEpoch 81\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00014203405589796603 Best: 0.00010048246622318402\n",
      "\t\tEpoch 82\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00015002289728727192 Best: 0.00010048246622318402\n",
      "\t\tEpoch 83\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001463087828597054 Best: 0.00010048246622318402\n",
      "\t\tEpoch 84\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00014021738024894148 Best: 0.00010048246622318402\n",
      "\t\tEpoch 85\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.000390625\n",
      "Validation loss: 0.00014404385001398623 Best: 0.00010048246622318402\n",
      "\t\tEpoch 86\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001435914309695363 Best: 0.00010048246622318402\n",
      "\t\tEpoch 87\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00014189393550623208 Best: 0.00010048246622318402\n",
      "\t\tEpoch 88\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001344919000985101 Best: 0.00010048246622318402\n",
      "\t\tEpoch 89\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00013069473789073527 Best: 0.00010048246622318402\n",
      "\t\tEpoch 90\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00011971824278589338 Best: 0.00010048246622318402\n",
      "\t\tEpoch 91\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0001953125\n",
      "Validation loss: 0.00011694105342030525 Best: 0.00010048246622318402\n",
      "\t\tEpoch 92\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00012258157948963344 Best: 0.00010048246622318402\n",
      "\t\tEpoch 93\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00013302975276019424 Best: 0.00010048246622318402\n",
      "\t\tEpoch 94\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00013020672486163676 Best: 0.00010048246622318402\n",
      "\t\tEpoch 95\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00013464820221997797 Best: 0.00010048246622318402\n",
      "\t\tEpoch 96\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001560240052640438 Best: 0.00010048246622318402\n",
      "\t\tEpoch 97\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 9.765625e-05\n",
      "Validation loss: 0.00016144869732670486 Best: 0.00010048246622318402\n",
      "\t\tEpoch 98\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00017664002371020615 Best: 0.00010048246622318402\n",
      "\t\tEpoch 99\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00018551723042037338 Best: 0.00010048246622318402\n",
      "\t\tEpoch 100\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00019756080291699618 Best: 0.00010048246622318402\n",
      "0.12991091164819718\n",
      "(54, 402)\n",
      "\t\t\tQuantile Network\n",
      "Use MondrianForestRegressor\n",
      "ymax and min: tensor(1.6452) tensor(-1.8799)\n",
      "\t\tEpoch 1\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 1\n",
      "Validation loss: 0.0008378873462788761 Best: 0.0008378873462788761\n",
      "\t\tEpoch 2\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 2\n",
      "Validation loss: 0.000835190701764077 Best: 0.000835190701764077\n",
      "\t\tEpoch 3\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 3\n",
      "Validation loss: 0.0008301185443997383 Best: 0.0008301185443997383\n",
      "\t\tEpoch 4\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 4\n",
      "Validation loss: 0.0008221429889090359 Best: 0.0008221429889090359\n",
      "\t\tEpoch 5\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 5\n",
      "Validation loss: 0.000811066129244864 Best: 0.000811066129244864\n",
      "\t\tEpoch 6\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 6\n",
      "Validation loss: 0.0007975782500579953 Best: 0.0007975782500579953\n",
      "\t\tEpoch 7\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 7\n",
      "Validation loss: 0.0007806838257238269 Best: 0.0007806838257238269\n",
      "\t\tEpoch 8\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 8\n",
      "Validation loss: 0.0007616890943609178 Best: 0.0007616890943609178\n",
      "\t\tEpoch 9\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 9\n",
      "Validation loss: 0.0007431200938299298 Best: 0.0007431200938299298\n",
      "\t\tEpoch 10\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 10\n",
      "Validation loss: 0.00072565145092085 Best: 0.00072565145092085\n",
      "\t\tEpoch 11\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 11\n",
      "Validation loss: 0.0007085794350132346 Best: 0.0007085794350132346\n",
      "\t\tEpoch 12\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 12\n",
      "Validation loss: 0.0006880165310576558 Best: 0.0006880165310576558\n",
      "\t\tEpoch 13\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 13\n",
      "Validation loss: 0.0006744966376572847 Best: 0.0006744966376572847\n",
      "\t\tEpoch 14\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 14\n",
      "Validation loss: 0.0006604756927117705 Best: 0.0006604756927117705\n",
      "\t\tEpoch 15\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 15\n",
      "Validation loss: 0.000648255052510649 Best: 0.000648255052510649\n",
      "\t\tEpoch 16\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 16\n",
      "Validation loss: 0.000628529698587954 Best: 0.000628529698587954\n",
      "\t\tEpoch 17\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 17\n",
      "Validation loss: 0.0006068283692002296 Best: 0.0006068283692002296\n",
      "\t\tEpoch 18\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 18\n",
      "Validation loss: 0.0005873945192433894 Best: 0.0005873945192433894\n",
      "\t\tEpoch 19\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 19\n",
      "Validation loss: 0.0005654973210766912 Best: 0.0005654973210766912\n",
      "\t\tEpoch 20\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 20\n",
      "Validation loss: 0.0005466673756018281 Best: 0.0005466673756018281\n",
      "\t\tEpoch 21\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 21\n",
      "Validation loss: 0.000533771701157093 Best: 0.000533771701157093\n",
      "\t\tEpoch 22\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 22\n",
      "Validation loss: 0.000525466981343925 Best: 0.000525466981343925\n",
      "\t\tEpoch 23\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 23\n",
      "Validation loss: 0.0005074276705272496 Best: 0.0005074276705272496\n",
      "\t\tEpoch 24\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 24\n",
      "Validation loss: 0.00049173878505826 Best: 0.00049173878505826\n",
      "\t\tEpoch 25\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 25\n",
      "Validation loss: 0.00047852759598754346 Best: 0.00047852759598754346\n",
      "\t\tEpoch 26\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 26\n",
      "Validation loss: 0.00046621006913483143 Best: 0.00046621006913483143\n",
      "\t\tEpoch 27\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 27\n",
      "Validation loss: 0.0004544807015918195 Best: 0.0004544807015918195\n",
      "\t\tEpoch 28\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 28\n",
      "Validation loss: 0.0004494200402405113 Best: 0.0004494200402405113\n",
      "\t\tEpoch 29\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 29\n",
      "Validation loss: 0.0004365138302091509 Best: 0.0004365138302091509\n",
      "\t\tEpoch 30\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 30\n",
      "Validation loss: 0.0004218117392156273 Best: 0.0004218117392156273\n",
      "\t\tEpoch 31\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 31\n",
      "Validation loss: 0.0004123831749893725 Best: 0.0004123831749893725\n",
      "\t\tEpoch 32\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 32\n",
      "Validation loss: 0.0003891658561769873 Best: 0.0003891658561769873\n",
      "\t\tEpoch 33\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 33\n",
      "Validation loss: 0.00037485032225959003 Best: 0.00037485032225959003\n",
      "\t\tEpoch 34\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 34\n",
      "Validation loss: 0.0003557711315806955 Best: 0.0003557711315806955\n",
      "\t\tEpoch 35\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 35\n",
      "Validation loss: 0.00033357375650666654 Best: 0.00033357375650666654\n",
      "\t\tEpoch 36\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 36\n",
      "Validation loss: 0.00031582306837663054 Best: 0.00031582306837663054\n",
      "\t\tEpoch 37\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 37\n",
      "Validation loss: 0.00030560445156879723 Best: 0.00030560445156879723\n",
      "\t\tEpoch 38\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 38\n",
      "Validation loss: 0.0002950497146230191 Best: 0.0002950497146230191\n",
      "\t\tEpoch 39\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 39\n",
      "Validation loss: 0.0002887179725803435 Best: 0.0002887179725803435\n",
      "\t\tEpoch 40\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 40\n",
      "Validation loss: 0.00027901417342945933 Best: 0.00027901417342945933\n",
      "\t\tEpoch 41\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 41\n",
      "Validation loss: 0.0002741446951404214 Best: 0.0002741446951404214\n",
      "\t\tEpoch 42\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 42\n",
      "Validation loss: 0.00025936804013326764 Best: 0.00025936804013326764\n",
      "\t\tEpoch 43\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 43\n",
      "Validation loss: 0.00025336656835861504 Best: 0.00025336656835861504\n",
      "\t\tEpoch 44\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 44\n",
      "Validation loss: 0.00024414772633463144 Best: 0.00024414772633463144\n",
      "\t\tEpoch 45\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 45\n",
      "Validation loss: 0.00022228035959415138 Best: 0.00022228035959415138\n",
      "\t\tEpoch 46\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 46\n",
      "Validation loss: 0.00019719309057109058 Best: 0.00019719309057109058\n",
      "\t\tEpoch 47\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 47\n",
      "Validation loss: 0.00018481476581655443 Best: 0.00018481476581655443\n",
      "\t\tEpoch 48\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 48\n",
      "Validation loss: 0.00018422903667669743 Best: 0.00018422903667669743\n",
      "\t\tEpoch 49\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00018930960504803807 Best: 0.00018422903667669743\n",
      "\t\tEpoch 50\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001952163001988083 Best: 0.00018422903667669743\n",
      "\t\tEpoch 51\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0002033960772678256 Best: 0.00018422903667669743\n",
      "\t\tEpoch 52\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00021410311455838382 Best: 0.00018422903667669743\n",
      "\t\tEpoch 53\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00021056512196082622 Best: 0.00018422903667669743\n",
      "\t\tEpoch 54\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00020050987950526178 Best: 0.00018422903667669743\n",
      "\t\tEpoch 55\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.05\n",
      "Validation loss: 0.00018999019812326878 Best: 0.00018422903667669743\n",
      "\t\tEpoch 56\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001905247918330133 Best: 0.00018422903667669743\n",
      "\t\tEpoch 57\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 57\n",
      "Validation loss: 0.000184093281859532 Best: 0.000184093281859532\n",
      "\t\tEpoch 58\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 58\n",
      "Validation loss: 0.00017335810116492212 Best: 0.00017335810116492212\n",
      "\t\tEpoch 59\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 59\n",
      "Validation loss: 0.000165968929650262 Best: 0.000165968929650262\n",
      "\t\tEpoch 60\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 60\n",
      "Validation loss: 0.00015184414223767817 Best: 0.00015184414223767817\n",
      "\t\tEpoch 61\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 61\n",
      "Validation loss: 0.00014076707884669304 Best: 0.00014076707884669304\n",
      "\t\tEpoch 62\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 62\n",
      "Validation loss: 0.0001273296948056668 Best: 0.0001273296948056668\n",
      "\t\tEpoch 63\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 63\n",
      "Validation loss: 0.00011931724293390289 Best: 0.00011931724293390289\n",
      "\t\tEpoch 64\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 64\n",
      "Validation loss: 0.0001133318874053657 Best: 0.0001133318874053657\n",
      "\t\tEpoch 65\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001194533979287371 Best: 0.0001133318874053657\n",
      "\t\tEpoch 66\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001283081655856222 Best: 0.0001133318874053657\n",
      "\t\tEpoch 67\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00014711784024257213 Best: 0.0001133318874053657\n",
      "\t\tEpoch 68\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00015715102199465036 Best: 0.0001133318874053657\n",
      "\t\tEpoch 69\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.025\n",
      "Validation loss: 0.00016188276640605181 Best: 0.0001133318874053657\n",
      "\t\tEpoch 70\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00016464406508021057 Best: 0.0001133318874053657\n",
      "\t\tEpoch 71\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00016633709310553968 Best: 0.0001133318874053657\n",
      "\t\tEpoch 72\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001667561591602862 Best: 0.0001133318874053657\n",
      "\t\tEpoch 73\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00016589174629189074 Best: 0.0001133318874053657\n",
      "\t\tEpoch 74\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001626414741622284 Best: 0.0001133318874053657\n",
      "\t\tEpoch 75\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0125\n",
      "Validation loss: 0.00014961548731662333 Best: 0.0001133318874053657\n",
      "\t\tEpoch 76\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00014346587704494596 Best: 0.0001133318874053657\n",
      "\t\tEpoch 77\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00013500415661837906 Best: 0.0001133318874053657\n",
      "\t\tEpoch 78\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001377750450046733 Best: 0.0001133318874053657\n",
      "\t\tEpoch 79\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001438465405954048 Best: 0.0001133318874053657\n",
      "\t\tEpoch 80\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001461204665247351 Best: 0.0001133318874053657\n",
      "\t\tEpoch 81\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00625\n",
      "Validation loss: 0.00014426517009269446 Best: 0.0001133318874053657\n",
      "\t\tEpoch 82\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00014981009007897228 Best: 0.0001133318874053657\n",
      "\t\tEpoch 83\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00015426980098709464 Best: 0.0001133318874053657\n",
      "\t\tEpoch 84\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00016038726607803255 Best: 0.0001133318874053657\n",
      "\t\tEpoch 85\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001610090839676559 Best: 0.0001133318874053657\n",
      "\t\tEpoch 86\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00016626811702735722 Best: 0.0001133318874053657\n",
      "\t\tEpoch 87\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.003125\n",
      "Validation loss: 0.00016500423953402787 Best: 0.0001133318874053657\n",
      "\t\tEpoch 88\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00016358889115508646 Best: 0.0001133318874053657\n",
      "\t\tEpoch 89\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00016617814253550023 Best: 0.0001133318874053657\n",
      "\t\tEpoch 90\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00016220704128500074 Best: 0.0001133318874053657\n",
      "\t\tEpoch 91\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00016438101010862738 Best: 0.0001133318874053657\n",
      "\t\tEpoch 92\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001596280199009925 Best: 0.0001133318874053657\n",
      "\t\tEpoch 93\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0015625\n",
      "Validation loss: 0.00015793059719726443 Best: 0.0001133318874053657\n",
      "\t\tEpoch 94\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00016006871010176837 Best: 0.0001133318874053657\n",
      "\t\tEpoch 95\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00015644139784853905 Best: 0.0001133318874053657\n",
      "\t\tEpoch 96\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00015298405196517706 Best: 0.0001133318874053657\n",
      "\t\tEpoch 97\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00015730479208286852 Best: 0.0001133318874053657\n",
      "\t\tEpoch 98\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00015659302880521864 Best: 0.0001133318874053657\n",
      "\t\tEpoch 99\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00078125\n",
      "Validation loss: 0.0001528708089608699 Best: 0.0001133318874053657\n",
      "\t\tEpoch 100\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.000156666777911596 Best: 0.0001133318874053657\n",
      "0.2993001263385405\n",
      "(54, 402)\n",
      "\t\t\tQuantile Network\n",
      "Use MondrianForestRegressor\n",
      "ymax and min: tensor(1.5533) tensor(-2.4517)\n",
      "\t\tEpoch 1\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 1\n",
      "Validation loss: 0.0010957366321235895 Best: 0.0010957366321235895\n",
      "\t\tEpoch 2\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 2\n",
      "Validation loss: 0.0010876554297283292 Best: 0.0010876554297283292\n",
      "\t\tEpoch 3\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 3\n",
      "Validation loss: 0.0010766859631985426 Best: 0.0010766859631985426\n",
      "\t\tEpoch 4\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 4\n",
      "Validation loss: 0.0010707692708820105 Best: 0.0010707692708820105\n",
      "\t\tEpoch 5\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 5\n",
      "Validation loss: 0.0010518347844481468 Best: 0.0010518347844481468\n",
      "\t\tEpoch 6\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 6\n",
      "Validation loss: 0.0010284201707690954 Best: 0.0010284201707690954\n",
      "\t\tEpoch 7\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 7\n",
      "Validation loss: 0.001002246281132102 Best: 0.001002246281132102\n",
      "\t\tEpoch 8\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 8\n",
      "Validation loss: 0.0009745515999384224 Best: 0.0009745515999384224\n",
      "\t\tEpoch 9\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 9\n",
      "Validation loss: 0.0009501010063104331 Best: 0.0009501010063104331\n",
      "\t\tEpoch 10\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 10\n",
      "Validation loss: 0.0009205108508467674 Best: 0.0009205108508467674\n",
      "\t\tEpoch 11\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 11\n",
      "Validation loss: 0.0008969451300799847 Best: 0.0008969451300799847\n",
      "\t\tEpoch 12\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 12\n",
      "Validation loss: 0.0008725899970158935 Best: 0.0008725899970158935\n",
      "\t\tEpoch 13\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 13\n",
      "Validation loss: 0.0008602592861279845 Best: 0.0008602592861279845\n",
      "\t\tEpoch 14\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 14\n",
      "Validation loss: 0.0008552016806788743 Best: 0.0008552016806788743\n",
      "\t\tEpoch 15\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 15\n",
      "Validation loss: 0.0008487769518978894 Best: 0.0008487769518978894\n",
      "\t\tEpoch 16\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 16\n",
      "Validation loss: 0.0008446622523479164 Best: 0.0008446622523479164\n",
      "\t\tEpoch 17\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 17\n",
      "Validation loss: 0.0008321716450154781 Best: 0.0008321716450154781\n",
      "\t\tEpoch 18\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 18\n",
      "Validation loss: 0.0008186991326510906 Best: 0.0008186991326510906\n",
      "\t\tEpoch 19\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 19\n",
      "Validation loss: 0.0008010939927771688 Best: 0.0008010939927771688\n",
      "\t\tEpoch 20\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 20\n",
      "Validation loss: 0.0007778574945405126 Best: 0.0007778574945405126\n",
      "\t\tEpoch 21\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 21\n",
      "Validation loss: 0.0007590476889163256 Best: 0.0007590476889163256\n",
      "\t\tEpoch 22\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 22\n",
      "Validation loss: 0.0007146706338971853 Best: 0.0007146706338971853\n",
      "\t\tEpoch 23\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 23\n",
      "Validation loss: 0.0006898114806972444 Best: 0.0006898114806972444\n",
      "\t\tEpoch 24\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 24\n",
      "Validation loss: 0.0006585136870853603 Best: 0.0006585136870853603\n",
      "\t\tEpoch 25\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 25\n",
      "Validation loss: 0.0006331736221909523 Best: 0.0006331736221909523\n",
      "\t\tEpoch 26\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 26\n",
      "Validation loss: 0.000608303991612047 Best: 0.000608303991612047\n",
      "\t\tEpoch 27\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 27\n",
      "Validation loss: 0.0005765857640653849 Best: 0.0005765857640653849\n",
      "\t\tEpoch 28\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 28\n",
      "Validation loss: 0.0005473578348755836 Best: 0.0005473578348755836\n",
      "\t\tEpoch 29\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 29\n",
      "Validation loss: 0.0005305056110955775 Best: 0.0005305056110955775\n",
      "\t\tEpoch 30\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0005432340549305081 Best: 0.0005305056110955775\n",
      "\t\tEpoch 31\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0005368362180888653 Best: 0.0005305056110955775\n",
      "\t\tEpoch 32\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0005416694330051541 Best: 0.0005305056110955775\n",
      "\t\tEpoch 33\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 33\n",
      "Validation loss: 0.0005262993508949876 Best: 0.0005262993508949876\n",
      "\t\tEpoch 34\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 34\n",
      "Validation loss: 0.0004984642728231847 Best: 0.0004984642728231847\n",
      "\t\tEpoch 35\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 35\n",
      "Validation loss: 0.0004573776968754828 Best: 0.0004573776968754828\n",
      "\t\tEpoch 36\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 36\n",
      "Validation loss: 0.0003950113896280527 Best: 0.0003950113896280527\n",
      "\t\tEpoch 37\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 37\n",
      "Validation loss: 0.0003692038881126791 Best: 0.0003692038881126791\n",
      "\t\tEpoch 38\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 38\n",
      "Validation loss: 0.0003005664038937539 Best: 0.0003005664038937539\n",
      "\t\tEpoch 39\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 39\n",
      "Validation loss: 0.00026958456146530807 Best: 0.00026958456146530807\n",
      "\t\tEpoch 40\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 40\n",
      "Validation loss: 0.00023827115364838392 Best: 0.00023827115364838392\n",
      "\t\tEpoch 41\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 41\n",
      "Validation loss: 0.0002334175369469449 Best: 0.0002334175369469449\n",
      "\t\tEpoch 42\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00025615651975385845 Best: 0.0002334175369469449\n",
      "\t\tEpoch 43\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00023896995116956532 Best: 0.0002334175369469449\n",
      "\t\tEpoch 44\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00023865602270234376 Best: 0.0002334175369469449\n",
      "\t\tEpoch 45\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.05\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 45\n",
      "Validation loss: 0.0001967411517398432 Best: 0.0001967411517398432\n",
      "\t\tEpoch 46\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00021903810556977987 Best: 0.0001967411517398432\n",
      "\t\tEpoch 47\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00026837014593183994 Best: 0.0001967411517398432\n",
      "\t\tEpoch 48\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.000291142932837829 Best: 0.0001967411517398432\n",
      "\t\tEpoch 49\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00033500808058306575 Best: 0.0001967411517398432\n",
      "\t\tEpoch 50\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00035218411358073354 Best: 0.0001967411517398432\n",
      "\t\tEpoch 51\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00034709853935055435 Best: 0.0001967411517398432\n",
      "\t\tEpoch 52\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.025\n",
      "Validation loss: 0.0003324974386487156 Best: 0.0001967411517398432\n",
      "\t\tEpoch 53\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00030454786610789597 Best: 0.0001967411517398432\n",
      "\t\tEpoch 54\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00030222293571569026 Best: 0.0001967411517398432\n",
      "\t\tEpoch 55\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.000312078685965389 Best: 0.0001967411517398432\n",
      "\t\tEpoch 56\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00032795031438581645 Best: 0.0001967411517398432\n",
      "\t\tEpoch 57\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00031424264307133853 Best: 0.0001967411517398432\n",
      "\t\tEpoch 58\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0125\n",
      "Validation loss: 0.0002705921360757202 Best: 0.0001967411517398432\n",
      "\t\tEpoch 59\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.000247277959715575 Best: 0.0001967411517398432\n",
      "\t\tEpoch 60\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0002313479344593361 Best: 0.0001967411517398432\n",
      "\t\tEpoch 61\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.000228196891839616 Best: 0.0001967411517398432\n",
      "\t\tEpoch 62\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00022696421365253627 Best: 0.0001967411517398432\n",
      "\t\tEpoch 63\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0002162143646273762 Best: 0.0001967411517398432\n",
      "\t\tEpoch 64\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00625\n",
      "Validation loss: 0.0002312622673343867 Best: 0.0001967411517398432\n",
      "\t\tEpoch 65\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0002857572690118104 Best: 0.0001967411517398432\n",
      "\t\tEpoch 66\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00026904320111498237 Best: 0.0001967411517398432\n",
      "\t\tEpoch 67\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0002736813039518893 Best: 0.0001967411517398432\n",
      "\t\tEpoch 68\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0002899479877669364 Best: 0.0001967411517398432\n",
      "\t\tEpoch 69\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00031221332028508186 Best: 0.0001967411517398432\n",
      "\t\tEpoch 70\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.003125\n",
      "Validation loss: 0.0002677561133168638 Best: 0.0001967411517398432\n",
      "\t\tEpoch 71\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0002769351121969521 Best: 0.0001967411517398432\n",
      "\t\tEpoch 72\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00031570706050843 Best: 0.0001967411517398432\n",
      "\t\tEpoch 73\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00034963246434926987 Best: 0.0001967411517398432\n",
      "\t\tEpoch 74\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003684887196868658 Best: 0.0001967411517398432\n",
      "\t\tEpoch 75\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003717133076861501 Best: 0.0001967411517398432\n",
      "\t\tEpoch 76\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0015625\n",
      "Validation loss: 0.0003874624671880156 Best: 0.0001967411517398432\n",
      "\t\tEpoch 77\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003779420512728393 Best: 0.0001967411517398432\n",
      "\t\tEpoch 78\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00036076558171771467 Best: 0.0001967411517398432\n",
      "\t\tEpoch 79\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00033383717527613044 Best: 0.0001967411517398432\n",
      "\t\tEpoch 80\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00031490979017689824 Best: 0.0001967411517398432\n",
      "\t\tEpoch 81\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00028510141419246793 Best: 0.0001967411517398432\n",
      "\t\tEpoch 82\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00078125\n",
      "Validation loss: 0.0002717912429943681 Best: 0.0001967411517398432\n",
      "\t\tEpoch 83\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00028920292970724404 Best: 0.0001967411517398432\n",
      "\t\tEpoch 84\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0002854224294424057 Best: 0.0001967411517398432\n",
      "\t\tEpoch 85\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003129153628833592 Best: 0.0001967411517398432\n",
      "\t\tEpoch 86\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003351883788127452 Best: 0.0001967411517398432\n",
      "\t\tEpoch 87\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003282654215581715 Best: 0.0001967411517398432\n",
      "\t\tEpoch 88\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.000390625\n",
      "Validation loss: 0.00032449100399389863 Best: 0.0001967411517398432\n",
      "\t\tEpoch 89\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003323456912767142 Best: 0.0001967411517398432\n",
      "\t\tEpoch 90\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003313437628094107 Best: 0.0001967411517398432\n",
      "\t\tEpoch 91\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00031507370295003057 Best: 0.0001967411517398432\n",
      "\t\tEpoch 92\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00029732700204476714 Best: 0.0001967411517398432\n",
      "\t\tEpoch 93\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003205866669304669 Best: 0.0001967411517398432\n",
      "\t\tEpoch 94\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0001953125\n",
      "Validation loss: 0.0003140270127914846 Best: 0.0001967411517398432\n",
      "\t\tEpoch 95\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00032891539740376174 Best: 0.0001967411517398432\n",
      "\t\tEpoch 96\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003259684599470347 Best: 0.0001967411517398432\n",
      "\t\tEpoch 97\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003093509003520012 Best: 0.0001967411517398432\n",
      "\t\tEpoch 98\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0002880043175537139 Best: 0.0001967411517398432\n",
      "\t\tEpoch 99\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00028372518136166036 Best: 0.0001967411517398432\n",
      "\t\tEpoch 100\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 9.765625e-05\n",
      "Validation loss: 0.0002673266571946442 Best: 0.0001967411517398432\n",
      "0.059555682361064406\n",
      "(54, 402)\n",
      "\t\t\tQuantile Network\n",
      "Use MondrianForestRegressor\n",
      "ymax and min: tensor(1.2055) tensor(-2.4760)\n",
      "\t\tEpoch 1\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 1\n",
      "Validation loss: 0.0008297160384245217 Best: 0.0008297160384245217\n",
      "\t\tEpoch 2\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 2\n",
      "Validation loss: 0.0008277333108708262 Best: 0.0008277333108708262\n",
      "\t\tEpoch 3\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 3\n",
      "Validation loss: 0.0008245048229582608 Best: 0.0008245048229582608\n",
      "\t\tEpoch 4\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 4\n",
      "Validation loss: 0.0008197373826988041 Best: 0.0008197373826988041\n",
      "\t\tEpoch 5\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 5\n",
      "Validation loss: 0.0008117001270875335 Best: 0.0008117001270875335\n",
      "\t\tEpoch 6\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 6\n",
      "Validation loss: 0.0008034614147618413 Best: 0.0008034614147618413\n",
      "\t\tEpoch 7\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 7\n",
      "Validation loss: 0.0007953106542117894 Best: 0.0007953106542117894\n",
      "\t\tEpoch 8\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 8\n",
      "Validation loss: 0.0007823367486707866 Best: 0.0007823367486707866\n",
      "\t\tEpoch 9\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 9\n",
      "Validation loss: 0.0007673296495340765 Best: 0.0007673296495340765\n",
      "\t\tEpoch 10\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 10\n",
      "Validation loss: 0.0007466747192665935 Best: 0.0007466747192665935\n",
      "\t\tEpoch 11\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 11\n",
      "Validation loss: 0.0007257767720147967 Best: 0.0007257767720147967\n",
      "\t\tEpoch 12\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 12\n",
      "Validation loss: 0.0006990503752604127 Best: 0.0006990503752604127\n",
      "\t\tEpoch 13\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 13\n",
      "Validation loss: 0.0006711923633702099 Best: 0.0006711923633702099\n",
      "\t\tEpoch 14\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 14\n",
      "Validation loss: 0.0006387508474290371 Best: 0.0006387508474290371\n",
      "\t\tEpoch 15\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 15\n",
      "Validation loss: 0.0006085657514631748 Best: 0.0006085657514631748\n",
      "\t\tEpoch 16\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 16\n",
      "Validation loss: 0.0005872626206837595 Best: 0.0005872626206837595\n",
      "\t\tEpoch 17\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 17\n",
      "Validation loss: 0.0005744454101659358 Best: 0.0005744454101659358\n",
      "\t\tEpoch 18\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 18\n",
      "Validation loss: 0.0005547790788114071 Best: 0.0005547790788114071\n",
      "\t\tEpoch 19\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 19\n",
      "Validation loss: 0.0005457293009385467 Best: 0.0005457293009385467\n",
      "\t\tEpoch 20\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 20\n",
      "Validation loss: 0.0005360416835173965 Best: 0.0005360416835173965\n",
      "\t\tEpoch 21\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 21\n",
      "Validation loss: 0.0005103018484078348 Best: 0.0005103018484078348\n",
      "\t\tEpoch 22\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 22\n",
      "Validation loss: 0.0005029645981267095 Best: 0.0005029645981267095\n",
      "\t\tEpoch 23\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 23\n",
      "Validation loss: 0.0004991312744095922 Best: 0.0004991312744095922\n",
      "\t\tEpoch 24\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 24\n",
      "Validation loss: 0.0004849733377341181 Best: 0.0004849733377341181\n",
      "\t\tEpoch 25\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 25\n",
      "Validation loss: 0.00047488402924500406 Best: 0.00047488402924500406\n",
      "\t\tEpoch 26\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 26\n",
      "Validation loss: 0.00045451970072463155 Best: 0.00045451970072463155\n",
      "\t\tEpoch 27\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 27\n",
      "Validation loss: 0.00044171465560793877 Best: 0.00044171465560793877\n",
      "\t\tEpoch 28\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 28\n",
      "Validation loss: 0.0004316892591305077 Best: 0.0004316892591305077\n",
      "\t\tEpoch 29\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 29\n",
      "Validation loss: 0.00041123401024378836 Best: 0.00041123401024378836\n",
      "\t\tEpoch 30\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 30\n",
      "Validation loss: 0.00039519654819741845 Best: 0.00039519654819741845\n",
      "\t\tEpoch 31\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 31\n",
      "Validation loss: 0.0003661724040284753 Best: 0.0003661724040284753\n",
      "\t\tEpoch 32\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 32\n",
      "Validation loss: 0.00032336899312213063 Best: 0.00032336899312213063\n",
      "\t\tEpoch 33\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 33\n",
      "Validation loss: 0.0002785518881864846 Best: 0.0002785518881864846\n",
      "\t\tEpoch 34\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 34\n",
      "Validation loss: 0.00025143666425719857 Best: 0.00025143666425719857\n",
      "\t\tEpoch 35\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 35\n",
      "Validation loss: 0.00021808987366966903 Best: 0.00021808987366966903\n",
      "\t\tEpoch 36\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 36\n",
      "Validation loss: 0.0002076176751870662 Best: 0.0002076176751870662\n",
      "\t\tEpoch 37\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00021243041555862874 Best: 0.0002076176751870662\n",
      "\t\tEpoch 38\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00021788058802485466 Best: 0.0002076176751870662\n",
      "\t\tEpoch 39\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00022109001292847097 Best: 0.0002076176751870662\n",
      "\t\tEpoch 40\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0002096880052704364 Best: 0.0002076176751870662\n",
      "\t\tEpoch 41\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 41\n",
      "Validation loss: 0.00019833786063827574 Best: 0.00019833786063827574\n",
      "\t\tEpoch 42\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 42\n",
      "Validation loss: 0.00019125228573102504 Best: 0.00019125228573102504\n",
      "\t\tEpoch 43\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001919848145917058 Best: 0.00019125228573102504\n",
      "\t\tEpoch 44\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 44\n",
      "Validation loss: 0.00018602728960104287 Best: 0.00018602728960104287\n",
      "\t\tEpoch 45\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00019088874978478998 Best: 0.00018602728960104287\n",
      "\t\tEpoch 46\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.05\n",
      "Validation loss: 0.0001922440860653296 Best: 0.00018602728960104287\n",
      "\t\tEpoch 47\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00019054570293519646 Best: 0.00018602728960104287\n",
      "\t\tEpoch 48\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00019385424093343318 Best: 0.00018602728960104287\n",
      "\t\tEpoch 49\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00018879650451708585 Best: 0.00018602728960104287\n",
      "\t\tEpoch 50\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 50\n",
      "Validation loss: 0.0001757113786879927 Best: 0.0001757113786879927\n",
      "\t\tEpoch 51\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 51\n",
      "Validation loss: 0.00015621517377439886 Best: 0.00015621517377439886\n",
      "\t\tEpoch 52\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 52\n",
      "Validation loss: 0.00014590524369850755 Best: 0.00014590524369850755\n",
      "\t\tEpoch 53\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 53\n",
      "Validation loss: 0.0001375535794068128 Best: 0.0001375535794068128\n",
      "\t\tEpoch 54\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 54\n",
      "Validation loss: 0.00012660687207244337 Best: 0.00012660687207244337\n",
      "\t\tEpoch 55\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 55\n",
      "Validation loss: 0.00011785661627072841 Best: 0.00011785661627072841\n",
      "\t\tEpoch 56\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 56\n",
      "Validation loss: 0.0001118551372201182 Best: 0.0001118551372201182\n",
      "\t\tEpoch 57\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 57\n",
      "Validation loss: 0.00010699885024223477 Best: 0.00010699885024223477\n",
      "\t\tEpoch 58\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00011121070565422997 Best: 0.00010699885024223477\n",
      "\t\tEpoch 59\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 59\n",
      "Validation loss: 0.00010498832853045315 Best: 0.00010498832853045315\n",
      "\t\tEpoch 60\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00011407535203034058 Best: 0.00010498832853045315\n",
      "\t\tEpoch 61\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.025\n",
      "Validation loss: 0.00011891259055119008 Best: 0.00010498832853045315\n",
      "\t\tEpoch 62\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00011845367407659069 Best: 0.00010498832853045315\n",
      "\t\tEpoch 63\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00011140185233671218 Best: 0.00010498832853045315\n",
      "\t\tEpoch 64\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00011627965432126075 Best: 0.00010498832853045315\n",
      "\t\tEpoch 65\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00012292827886994928 Best: 0.00010498832853045315\n",
      "\t\tEpoch 66\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00012176622112747282 Best: 0.00010498832853045315\n",
      "\t\tEpoch 67\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0125\n",
      "Validation loss: 0.00012727519788313657 Best: 0.00010498832853045315\n",
      "\t\tEpoch 68\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00014373713929671794 Best: 0.00010498832853045315\n",
      "\t\tEpoch 69\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00014714483404532075 Best: 0.00010498832853045315\n",
      "\t\tEpoch 70\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00014168457710184157 Best: 0.00010498832853045315\n",
      "\t\tEpoch 71\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00013930670684203506 Best: 0.00010498832853045315\n",
      "\t\tEpoch 72\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00013537648192141205 Best: 0.00010498832853045315\n",
      "\t\tEpoch 73\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00625\n",
      "Validation loss: 0.00013148828293196857 Best: 0.00010498832853045315\n",
      "\t\tEpoch 74\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00012125074863433838 Best: 0.00010498832853045315\n",
      "\t\tEpoch 75\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00011571531649678946 Best: 0.00010498832853045315\n",
      "\t\tEpoch 76\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00011024768173228949 Best: 0.00010498832853045315\n",
      "\t\tEpoch 77\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00011683930642902851 Best: 0.00010498832853045315\n",
      "\t\tEpoch 78\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00013235941878519952 Best: 0.00010498832853045315\n",
      "\t\tEpoch 79\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.003125\n",
      "Validation loss: 0.00014584594464395195 Best: 0.00010498832853045315\n",
      "\t\tEpoch 80\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00015181282651610672 Best: 0.00010498832853045315\n",
      "\t\tEpoch 81\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001682678412180394 Best: 0.00010498832853045315\n",
      "\t\tEpoch 82\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00017054604541044682 Best: 0.00010498832853045315\n",
      "\t\tEpoch 83\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00017015752382576466 Best: 0.00010498832853045315\n",
      "\t\tEpoch 84\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00015414351946674287 Best: 0.00010498832853045315\n",
      "\t\tEpoch 85\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0015625\n",
      "Validation loss: 0.0001442552311345935 Best: 0.00010498832853045315\n",
      "\t\tEpoch 86\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00012985861394554377 Best: 0.00010498832853045315\n",
      "\t\tEpoch 87\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00012751338363159448 Best: 0.00010498832853045315\n",
      "\t\tEpoch 88\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00012986824731342494 Best: 0.00010498832853045315\n",
      "\t\tEpoch 89\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00012770871398970485 Best: 0.00010498832853045315\n",
      "\t\tEpoch 90\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001291619846597314 Best: 0.00010498832853045315\n",
      "\t\tEpoch 91\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00078125\n",
      "Validation loss: 0.00013896428572479635 Best: 0.00010498832853045315\n",
      "\t\tEpoch 92\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001417833409504965 Best: 0.00010498832853045315\n",
      "\t\tEpoch 93\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001502141822129488 Best: 0.00010498832853045315\n",
      "\t\tEpoch 94\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00015799181710463017 Best: 0.00010498832853045315\n",
      "\t\tEpoch 95\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00015855221136007458 Best: 0.00010498832853045315\n",
      "\t\tEpoch 96\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00016361767484340817 Best: 0.00010498832853045315\n",
      "\t\tEpoch 97\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.000390625\n",
      "Validation loss: 0.0001613678759895265 Best: 0.00010498832853045315\n",
      "\t\tEpoch 98\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00015678988711442798 Best: 0.00010498832853045315\n",
      "\t\tEpoch 99\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00015466033073607832 Best: 0.00010498832853045315\n",
      "\t\tEpoch 100\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001584007404744625 Best: 0.00010498832853045315\n",
      "0.28926543073266714\n",
      "(54, 402)\n",
      "\t\t\tQuantile Network\n",
      "Use MondrianForestRegressor\n",
      "ymax and min: tensor(1.5960) tensor(-2.5753)\n",
      "\t\tEpoch 1\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 1\n",
      "Validation loss: 0.0012029767967760563 Best: 0.0012029767967760563\n",
      "\t\tEpoch 2\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 2\n",
      "Validation loss: 0.0011993953958153725 Best: 0.0011993953958153725\n",
      "\t\tEpoch 3\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 3\n",
      "Validation loss: 0.0011926552979275584 Best: 0.0011926552979275584\n",
      "\t\tEpoch 4\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 4\n",
      "Validation loss: 0.0011808440322056413 Best: 0.0011808440322056413\n",
      "\t\tEpoch 5\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 5\n",
      "Validation loss: 0.0011652575340121984 Best: 0.0011652575340121984\n",
      "\t\tEpoch 6\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 6\n",
      "Validation loss: 0.0011457091895863414 Best: 0.0011457091895863414\n",
      "\t\tEpoch 7\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 7\n",
      "Validation loss: 0.0011245348723605275 Best: 0.0011245348723605275\n",
      "\t\tEpoch 8\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 8\n",
      "Validation loss: 0.0011005098931491375 Best: 0.0011005098931491375\n",
      "\t\tEpoch 9\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 9\n",
      "Validation loss: 0.0010738421697169542 Best: 0.0010738421697169542\n",
      "\t\tEpoch 10\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 10\n",
      "Validation loss: 0.0010510331485420465 Best: 0.0010510331485420465\n",
      "\t\tEpoch 11\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 11\n",
      "Validation loss: 0.0010309794452041388 Best: 0.0010309794452041388\n",
      "\t\tEpoch 12\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 12\n",
      "Validation loss: 0.0010033351136371493 Best: 0.0010033351136371493\n",
      "\t\tEpoch 13\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 13\n",
      "Validation loss: 0.000977598363533616 Best: 0.000977598363533616\n",
      "\t\tEpoch 14\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 14\n",
      "Validation loss: 0.000951790832914412 Best: 0.000951790832914412\n",
      "\t\tEpoch 15\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 15\n",
      "Validation loss: 0.0009242064552381635 Best: 0.0009242064552381635\n",
      "\t\tEpoch 16\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 16\n",
      "Validation loss: 0.000893712043762207 Best: 0.000893712043762207\n",
      "\t\tEpoch 17\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 17\n",
      "Validation loss: 0.0008617563871666789 Best: 0.0008617563871666789\n",
      "\t\tEpoch 18\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 18\n",
      "Validation loss: 0.000837539613712579 Best: 0.000837539613712579\n",
      "\t\tEpoch 19\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 19\n",
      "Validation loss: 0.0008084452711045742 Best: 0.0008084452711045742\n",
      "\t\tEpoch 20\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 20\n",
      "Validation loss: 0.0007846482913009822 Best: 0.0007846482913009822\n",
      "\t\tEpoch 21\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 21\n",
      "Validation loss: 0.0007678333204239607 Best: 0.0007678333204239607\n",
      "\t\tEpoch 22\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 22\n",
      "Validation loss: 0.0007492766599170864 Best: 0.0007492766599170864\n",
      "\t\tEpoch 23\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 23\n",
      "Validation loss: 0.0007320250733755529 Best: 0.0007320250733755529\n",
      "\t\tEpoch 24\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 24\n",
      "Validation loss: 0.0007083727396093309 Best: 0.0007083727396093309\n",
      "\t\tEpoch 25\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 25\n",
      "Validation loss: 0.0006772002088837326 Best: 0.0006772002088837326\n",
      "\t\tEpoch 26\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 26\n",
      "Validation loss: 0.0006529859383590519 Best: 0.0006529859383590519\n",
      "\t\tEpoch 27\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 27\n",
      "Validation loss: 0.000631411443464458 Best: 0.000631411443464458\n",
      "\t\tEpoch 28\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 28\n",
      "Validation loss: 0.0005978139815852046 Best: 0.0005978139815852046\n",
      "\t\tEpoch 29\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 29\n",
      "Validation loss: 0.0005667967488989234 Best: 0.0005667967488989234\n",
      "\t\tEpoch 30\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 30\n",
      "Validation loss: 0.0005341814830899239 Best: 0.0005341814830899239\n",
      "\t\tEpoch 31\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 31\n",
      "Validation loss: 0.000511639635078609 Best: 0.000511639635078609\n",
      "\t\tEpoch 32\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 32\n",
      "Validation loss: 0.0004954898613505065 Best: 0.0004954898613505065\n",
      "\t\tEpoch 33\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 33\n",
      "Validation loss: 0.0004671833594329655 Best: 0.0004671833594329655\n",
      "\t\tEpoch 34\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 34\n",
      "Validation loss: 0.0004409108660183847 Best: 0.0004409108660183847\n",
      "\t\tEpoch 35\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 35\n",
      "Validation loss: 0.00042236456647515297 Best: 0.00042236456647515297\n",
      "\t\tEpoch 36\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 36\n",
      "Validation loss: 0.00040480674942955375 Best: 0.00040480674942955375\n",
      "\t\tEpoch 37\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 37\n",
      "Validation loss: 0.00038467702688649297 Best: 0.00038467702688649297\n",
      "\t\tEpoch 38\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 38\n",
      "Validation loss: 0.000371003698091954 Best: 0.000371003698091954\n",
      "\t\tEpoch 39\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 39\n",
      "Validation loss: 0.00034715267247520387 Best: 0.00034715267247520387\n",
      "\t\tEpoch 40\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 40\n",
      "Validation loss: 0.0003172580909449607 Best: 0.0003172580909449607\n",
      "\t\tEpoch 41\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 41\n",
      "Validation loss: 0.0002952046343125403 Best: 0.0002952046343125403\n",
      "\t\tEpoch 42\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 42\n",
      "Validation loss: 0.00027560145827010274 Best: 0.00027560145827010274\n",
      "\t\tEpoch 43\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 43\n",
      "Validation loss: 0.00025202817050740123 Best: 0.00025202817050740123\n",
      "\t\tEpoch 44\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 44\n",
      "Validation loss: 0.00022834799892734736 Best: 0.00022834799892734736\n",
      "\t\tEpoch 45\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 45\n",
      "Validation loss: 0.00020936259534209967 Best: 0.00020936259534209967\n",
      "\t\tEpoch 46\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 46\n",
      "Validation loss: 0.00020142478751949966 Best: 0.00020142478751949966\n",
      "\t\tEpoch 47\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 47\n",
      "Validation loss: 0.0001992134639294818 Best: 0.0001992134639294818\n",
      "\t\tEpoch 48\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 48\n",
      "Validation loss: 0.00019418151350691915 Best: 0.00019418151350691915\n",
      "\t\tEpoch 49\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 49\n",
      "Validation loss: 0.0001851989363785833 Best: 0.0001851989363785833\n",
      "\t\tEpoch 50\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 50\n",
      "Validation loss: 0.00017433408356737345 Best: 0.00017433408356737345\n",
      "\t\tEpoch 51\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 51\n",
      "Validation loss: 0.00017278811719734222 Best: 0.00017278811719734222\n",
      "\t\tEpoch 52\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 52\n",
      "Validation loss: 0.00017145655874628574 Best: 0.00017145655874628574\n",
      "\t\tEpoch 53\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 53\n",
      "Validation loss: 0.00015824122237972915 Best: 0.00015824122237972915\n",
      "\t\tEpoch 54\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 54\n",
      "Validation loss: 0.00015332267503254116 Best: 0.00015332267503254116\n",
      "\t\tEpoch 55\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 55\n",
      "Validation loss: 0.00014137216203380376 Best: 0.00014137216203380376\n",
      "\t\tEpoch 56\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 56\n",
      "Validation loss: 0.00013207297888584435 Best: 0.00013207297888584435\n",
      "\t\tEpoch 57\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 57\n",
      "Validation loss: 0.00012252727174200118 Best: 0.00012252727174200118\n",
      "\t\tEpoch 58\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 58\n",
      "Validation loss: 0.00011849160364363343 Best: 0.00011849160364363343\n",
      "\t\tEpoch 59\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00012164752843091264 Best: 0.00011849160364363343\n",
      "\t\tEpoch 60\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001420793414581567 Best: 0.00011849160364363343\n",
      "\t\tEpoch 61\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00015586212975904346 Best: 0.00011849160364363343\n",
      "\t\tEpoch 62\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001720669970382005 Best: 0.00011849160364363343\n",
      "\t\tEpoch 63\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001908197154989466 Best: 0.00011849160364363343\n",
      "\t\tEpoch 64\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00021378758538048714 Best: 0.00011849160364363343\n",
      "\t\tEpoch 65\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.05\n",
      "Validation loss: 0.00022772091324441135 Best: 0.00011849160364363343\n",
      "\t\tEpoch 66\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00024159210443031043 Best: 0.00011849160364363343\n",
      "\t\tEpoch 67\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0002356267359573394 Best: 0.00011849160364363343\n",
      "\t\tEpoch 68\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00022097642067819834 Best: 0.00011849160364363343\n",
      "\t\tEpoch 69\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00021895745885558426 Best: 0.00011849160364363343\n",
      "\t\tEpoch 70\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.000211466642213054 Best: 0.00011849160364363343\n",
      "\t\tEpoch 71\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.025\n",
      "Validation loss: 0.00021364384156186134 Best: 0.00011849160364363343\n",
      "\t\tEpoch 72\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00021553885017056018 Best: 0.00011849160364363343\n",
      "\t\tEpoch 73\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00020942915580235422 Best: 0.00011849160364363343\n",
      "\t\tEpoch 74\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.000200305919861421 Best: 0.00011849160364363343\n",
      "\t\tEpoch 75\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00019639567472040653 Best: 0.00011849160364363343\n",
      "\t\tEpoch 76\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00019399169832468033 Best: 0.00011849160364363343\n",
      "\t\tEpoch 77\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0125\n",
      "Validation loss: 0.00019774900283664465 Best: 0.00011849160364363343\n",
      "\t\tEpoch 78\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001947435666806996 Best: 0.00011849160364363343\n",
      "\t\tEpoch 79\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00018823191931005567 Best: 0.00011849160364363343\n",
      "\t\tEpoch 80\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00017723863129504025 Best: 0.00011849160364363343\n",
      "\t\tEpoch 81\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00015773052291478962 Best: 0.00011849160364363343\n",
      "\t\tEpoch 82\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00014717187150381505 Best: 0.00011849160364363343\n",
      "\t\tEpoch 83\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00625\n",
      "Validation loss: 0.00013294286327436566 Best: 0.00011849160364363343\n",
      "\t\tEpoch 84\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00012898397108074278 Best: 0.00011849160364363343\n",
      "\t\tEpoch 85\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00013109865540172905 Best: 0.00011849160364363343\n",
      "\t\tEpoch 86\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001233210350619629 Best: 0.00011849160364363343\n",
      "\t\tEpoch 87\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00012559966125991195 Best: 0.00011849160364363343\n",
      "\t\tEpoch 88\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00013837368169333786 Best: 0.00011849160364363343\n",
      "\t\tEpoch 89\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.003125\n",
      "Validation loss: 0.0001388109230902046 Best: 0.00011849160364363343\n",
      "\t\tEpoch 90\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00015368085587397218 Best: 0.00011849160364363343\n",
      "\t\tEpoch 91\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00015974263078533113 Best: 0.00011849160364363343\n",
      "\t\tEpoch 92\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001733979006530717 Best: 0.00011849160364363343\n",
      "\t\tEpoch 93\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0001776166318450123 Best: 0.00011849160364363343\n",
      "\t\tEpoch 94\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00018651293066795915 Best: 0.00011849160364363343\n",
      "\t\tEpoch 95\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0015625\n",
      "Validation loss: 0.00019560931832529604 Best: 0.00011849160364363343\n",
      "\t\tEpoch 96\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00019764559692703187 Best: 0.00011849160364363343\n",
      "\t\tEpoch 97\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00020126564777456224 Best: 0.00011849160364363343\n",
      "\t\tEpoch 98\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00020424506510607898 Best: 0.00011849160364363343\n",
      "\t\tEpoch 99\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00020860767108388245 Best: 0.00011849160364363343\n",
      "\t\tEpoch 100\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.000208552650292404 Best: 0.00011849160364363343\n",
      "0.26064764218191383\n",
      "(54, 402)\n",
      "\t\t\tQuantile Network\n",
      "Use MondrianForestRegressor\n",
      "ymax and min: tensor(1.6063) tensor(-2.4463)\n",
      "\t\tEpoch 1\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 1\n",
      "Validation loss: 0.0007933159358799458 Best: 0.0007933159358799458\n",
      "\t\tEpoch 2\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 2\n",
      "Validation loss: 0.0007871305570006371 Best: 0.0007871305570006371\n",
      "\t\tEpoch 3\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 3\n",
      "Validation loss: 0.0007797376601956785 Best: 0.0007797376601956785\n",
      "\t\tEpoch 4\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 4\n",
      "Validation loss: 0.0007712059887126088 Best: 0.0007712059887126088\n",
      "\t\tEpoch 5\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 5\n",
      "Validation loss: 0.0007591668982058764 Best: 0.0007591668982058764\n",
      "\t\tEpoch 6\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 6\n",
      "Validation loss: 0.0007484102388843894 Best: 0.0007484102388843894\n",
      "\t\tEpoch 7\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 7\n",
      "Validation loss: 0.0007378823356702924 Best: 0.0007378823356702924\n",
      "\t\tEpoch 8\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 8\n",
      "Validation loss: 0.000722511438652873 Best: 0.000722511438652873\n",
      "\t\tEpoch 9\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 9\n",
      "Validation loss: 0.0007057637558318675 Best: 0.0007057637558318675\n",
      "\t\tEpoch 10\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 10\n",
      "Validation loss: 0.0006818444235250354 Best: 0.0006818444235250354\n",
      "\t\tEpoch 11\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 11\n",
      "Validation loss: 0.0006552900886163116 Best: 0.0006552900886163116\n",
      "\t\tEpoch 12\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 12\n",
      "Validation loss: 0.0006351008778437972 Best: 0.0006351008778437972\n",
      "\t\tEpoch 13\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 13\n",
      "Validation loss: 0.0006160807097330689 Best: 0.0006160807097330689\n",
      "\t\tEpoch 14\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 14\n",
      "Validation loss: 0.0005912791821174324 Best: 0.0005912791821174324\n",
      "\t\tEpoch 15\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 15\n",
      "Validation loss: 0.000555971753783524 Best: 0.000555971753783524\n",
      "\t\tEpoch 16\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 16\n",
      "Validation loss: 0.0005326314712874591 Best: 0.0005326314712874591\n",
      "\t\tEpoch 17\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 17\n",
      "Validation loss: 0.0005068976897746325 Best: 0.0005068976897746325\n",
      "\t\tEpoch 18\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 18\n",
      "Validation loss: 0.00047641951823607087 Best: 0.00047641951823607087\n",
      "\t\tEpoch 19\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 19\n",
      "Validation loss: 0.000450274528702721 Best: 0.000450274528702721\n",
      "\t\tEpoch 20\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 20\n",
      "Validation loss: 0.00043410673970356584 Best: 0.00043410673970356584\n",
      "\t\tEpoch 21\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 21\n",
      "Validation loss: 0.00042144101462326944 Best: 0.00042144101462326944\n",
      "\t\tEpoch 22\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 22\n",
      "Validation loss: 0.00038937729550525546 Best: 0.00038937729550525546\n",
      "\t\tEpoch 23\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 23\n",
      "Validation loss: 0.0003640475624706596 Best: 0.0003640475624706596\n",
      "\t\tEpoch 24\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 24\n",
      "Validation loss: 0.000344158528605476 Best: 0.000344158528605476\n",
      "\t\tEpoch 25\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 25\n",
      "Validation loss: 0.00033103564055636525 Best: 0.00033103564055636525\n",
      "\t\tEpoch 26\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 26\n",
      "Validation loss: 0.00031299112015403807 Best: 0.00031299112015403807\n",
      "\t\tEpoch 27\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 27\n",
      "Validation loss: 0.00030548093491233885 Best: 0.00030548093491233885\n",
      "\t\tEpoch 28\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 28\n",
      "Validation loss: 0.00029418355552479625 Best: 0.00029418355552479625\n",
      "\t\tEpoch 29\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 29\n",
      "Validation loss: 0.0002765930548775941 Best: 0.0002765930548775941\n",
      "\t\tEpoch 30\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 30\n",
      "Validation loss: 0.0002570584474597126 Best: 0.0002570584474597126\n",
      "\t\tEpoch 31\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 31\n",
      "Validation loss: 0.000253641715971753 Best: 0.000253641715971753\n",
      "\t\tEpoch 32\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0002548064512666315 Best: 0.000253641715971753\n",
      "\t\tEpoch 33\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 33\n",
      "Validation loss: 0.0002306939277332276 Best: 0.0002306939277332276\n",
      "\t\tEpoch 34\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 34\n",
      "Validation loss: 0.00021340290550142527 Best: 0.00021340290550142527\n",
      "\t\tEpoch 35\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 35\n",
      "Validation loss: 0.000201954782824032 Best: 0.000201954782824032\n",
      "\t\tEpoch 36\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 36\n",
      "Validation loss: 0.00019556729239411652 Best: 0.00019556729239411652\n",
      "\t\tEpoch 37\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00019810631056316197 Best: 0.00019556729239411652\n",
      "\t\tEpoch 38\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00020292711269576102 Best: 0.00019556729239411652\n",
      "\t\tEpoch 39\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0002085904561681673 Best: 0.00019556729239411652\n",
      "\t\tEpoch 40\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0002128547348547727 Best: 0.00019556729239411652\n",
      "\t\tEpoch 41\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0002112456422764808 Best: 0.00019556729239411652\n",
      "\t\tEpoch 42\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.05\n",
      "Validation loss: 0.00019736394460778683 Best: 0.00019556729239411652\n",
      "\t\tEpoch 43\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 43\n",
      "Validation loss: 0.00019230811449233443 Best: 0.00019230811449233443\n",
      "\t\tEpoch 44\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 44\n",
      "Validation loss: 0.0001879865158116445 Best: 0.0001879865158116445\n",
      "\t\tEpoch 45\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00021742959506809711 Best: 0.0001879865158116445\n",
      "\t\tEpoch 46\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00023853543098084629 Best: 0.0001879865158116445\n",
      "\t\tEpoch 47\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0002330901043023914 Best: 0.0001879865158116445\n",
      "\t\tEpoch 48\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00021924657630734146 Best: 0.0001879865158116445\n",
      "\t\tEpoch 49\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00019299447012599558 Best: 0.0001879865158116445\n",
      "\t\tEpoch 50\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.025\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 50\n",
      "Validation loss: 0.00015918070857878774 Best: 0.00015918070857878774\n",
      "\t\tEpoch 51\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00016220819088630378 Best: 0.00015918070857878774\n",
      "\t\tEpoch 52\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00018350317259319127 Best: 0.00015918070857878774\n",
      "\t\tEpoch 53\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00018448385526426136 Best: 0.00015918070857878774\n",
      "\t\tEpoch 54\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00019379827426746488 Best: 0.00015918070857878774\n",
      "\t\tEpoch 55\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00018741328676696867 Best: 0.00015918070857878774\n",
      "\t\tEpoch 56\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00019946465909015387 Best: 0.00015918070857878774\n",
      "\t\tEpoch 57\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0125\n",
      "Validation loss: 0.00019805843476206064 Best: 0.00015918070857878774\n",
      "\t\tEpoch 58\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00019779670401476324 Best: 0.00015918070857878774\n",
      "\t\tEpoch 59\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00020155201491434127 Best: 0.00015918070857878774\n",
      "\t\tEpoch 60\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00021043149172328413 Best: 0.00015918070857878774\n",
      "\t\tEpoch 61\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00019832114048767835 Best: 0.00015918070857878774\n",
      "\t\tEpoch 62\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00019938140758313239 Best: 0.00015918070857878774\n",
      "\t\tEpoch 63\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00625\n",
      "Validation loss: 0.00019674953364301473 Best: 0.00015918070857878774\n",
      "\t\tEpoch 64\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00019354498363099992 Best: 0.00015918070857878774\n",
      "\t\tEpoch 65\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00021497388661373407 Best: 0.00015918070857878774\n",
      "\t\tEpoch 66\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00020471455354709178 Best: 0.00015918070857878774\n",
      "\t\tEpoch 67\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00020919919188600034 Best: 0.00015918070857878774\n",
      "\t\tEpoch 68\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0002256548759760335 Best: 0.00015918070857878774\n",
      "\t\tEpoch 69\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.003125\n",
      "Validation loss: 0.00023848548880778253 Best: 0.00015918070857878774\n",
      "\t\tEpoch 70\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00024584942730143666 Best: 0.00015918070857878774\n",
      "\t\tEpoch 71\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0002614868280943483 Best: 0.00015918070857878774\n",
      "\t\tEpoch 72\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00026251887902617455 Best: 0.00015918070857878774\n",
      "\t\tEpoch 73\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0002831746533047408 Best: 0.00015918070857878774\n",
      "\t\tEpoch 74\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003110985562670976 Best: 0.00015918070857878774\n",
      "\t\tEpoch 75\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0015625\n",
      "Validation loss: 0.0003457037382759154 Best: 0.00015918070857878774\n",
      "\t\tEpoch 76\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00034168330603279173 Best: 0.00015918070857878774\n",
      "\t\tEpoch 77\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00033339729998260736 Best: 0.00015918070857878774\n",
      "\t\tEpoch 78\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0003151129058096558 Best: 0.00015918070857878774\n",
      "\t\tEpoch 79\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0002984037564601749 Best: 0.00015918070857878774\n",
      "\t\tEpoch 80\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0002680161560419947 Best: 0.00015918070857878774\n",
      "\t\tEpoch 81\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00078125\n",
      "Validation loss: 0.00024428596952930093 Best: 0.00015918070857878774\n",
      "\t\tEpoch 82\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00022938658366911113 Best: 0.00015918070857878774\n",
      "\t\tEpoch 83\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00020704737107735127 Best: 0.00015918070857878774\n",
      "\t\tEpoch 84\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00020747647795360535 Best: 0.00015918070857878774\n",
      "\t\tEpoch 85\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0002093625080306083 Best: 0.00015918070857878774\n",
      "\t\tEpoch 86\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.000210162514122203 Best: 0.00015918070857878774\n",
      "\t\tEpoch 87\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.000390625\n",
      "Validation loss: 0.0002217430592281744 Best: 0.00015918070857878774\n",
      "\t\tEpoch 88\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0002205771888839081 Best: 0.00015918070857878774\n",
      "\t\tEpoch 89\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00021416367962956429 Best: 0.00015918070857878774\n",
      "\t\tEpoch 90\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00021304826077539474 Best: 0.00015918070857878774\n",
      "\t\tEpoch 91\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00020559206313919276 Best: 0.00015918070857878774\n",
      "\t\tEpoch 92\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00021133318659849465 Best: 0.00015918070857878774\n",
      "\t\tEpoch 93\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0001953125\n",
      "Validation loss: 0.0002343454834772274 Best: 0.00015918070857878774\n",
      "\t\tEpoch 94\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00024747702991589904 Best: 0.00015918070857878774\n",
      "\t\tEpoch 95\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0002434053021715954 Best: 0.00015918070857878774\n",
      "\t\tEpoch 96\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00023032622993923724 Best: 0.00015918070857878774\n",
      "\t\tEpoch 97\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00021538005967158824 Best: 0.00015918070857878774\n",
      "\t\tEpoch 98\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00020840852812398225 Best: 0.00015918070857878774\n",
      "\t\tEpoch 99\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 9.765625e-05\n",
      "Validation loss: 0.0002114894159603864 Best: 0.00015918070857878774\n",
      "\t\tEpoch 100\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.00022300510318018496 Best: 0.00015918070857878774\n",
      "0.07640570704472428\n",
      "Mean mse 0.15649157612613634\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    run_benchmarks(False)\n",
    "\n",
    "    #run_multivariate_benchmarks(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "33804620-e931-4280-9b12-736ced2c01d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from neural_sqerr import SqErrNetwork\n",
    "from RF_neural_model import QuantileNetwork\n",
    "#from neural_model import QuantileNetwork\n",
    "from spline_model import QuantileSpline\n",
    "from forest_model import QuantileForest\n",
    "#from visualize import heatmap_from_points\n",
    "def run_multivariate_benchmarks(demo=True):\n",
    "    N_trials = 1\n",
    "    N_test = 1000\n",
    "    sample_sizes = [1000]\n",
    "    quantiles = np.array([0.5])\n",
    "    functions = [Scenario1()]\n",
    "    tau=0.99\n",
    "    models = [#lambda: SqErrNetwork(),\n",
    "            lambda: QuantileNetwork(quantiles=quantiles,loss=\"marginal\"),]\n",
    "               #lambda: QuantileSpline(quantiles=quantiles),]\n",
    "              #lambda: QuantileForest(quantiles=quantiles)]\n",
    " \n",
    "    for trial in range(N_trials):\n",
    "        print(f'Trial {trial+1}')\n",
    "        for scenario, func in enumerate(functions):\n",
    "            print(f'\\t Scenario {scenario+1}')\n",
    "            mseresults=[]\n",
    "            kf = KFold(n_splits=10,shuffle=True,random_state=40)\n",
    "            for train, test in kf.split(x_ini):\n",
    "\n",
    "                # Sample test set covariates and response\n",
    "\n",
    "                X_test=np.array(x_ini)[test]\n",
    "                y_test=np.array(y_ini)[test]\n",
    "                #y_test=np.squeeze(y_test)\n",
    "\n",
    "\n",
    "                # Demo plotting\n",
    "                if demo:\n",
    "                    for qidx, q in enumerate((quantiles*100).astype(int)):\n",
    "                        heatmap_from_points(f'plots/scenario{scenario+1}-quantile{q}-truth.pdf', X_test[:,:2], y_quantiles[:,qidx], vmin=y_quantiles.min(), vmax=y_quantiles.max())\n",
    "\n",
    "                for nidx, N_train in enumerate(sample_sizes):\n",
    "                    #print(f'\\t\\tN={N_train}')\n",
    "                    # Sample training covariates and response\n",
    "                    X_train=np.array(x_ini)[train]\n",
    "                    y_train=np.array(y_ini)[train]\n",
    "                    #y_train=np.squeeze(y_train)\n",
    "\n",
    "                    # Evaluate each of the quantile models\n",
    "                    # Note: we generate a new model each time so as to not\n",
    "                    # accidentally cheat by warm-starting from the last point\n",
    "                    for midx, model in enumerate([m() for m in models]):\n",
    "                        print(f'\\t\\t\\t{model.label}')\n",
    "\n",
    "                        if X_train.shape[1] > 3 and model.filename == 'spline':\n",
    "                            print('Too many covariates. Skipping...')\n",
    "                            continue\n",
    "\n",
    "                        model.fit(X_train, y_train,tau)\n",
    "                        #model.fit(X_train, y_train)\n",
    "                        preds = model.predict(X_test)\n",
    "                        #y_test=y_test.reshape(-1,1)\n",
    "                        y_test = np.expand_dims(y_test, axis=-1)                  \n",
    "                        mse= ((y_test - preds)**2).mean()\n",
    "                        print(mse)\n",
    "\n",
    "                        \n",
    "                        mseresults.append(mse)\n",
    "\n",
    "                        \n",
    "            mseresults=np.array(mseresults)\n",
    "\n",
    "            print(f'Mean mse {np.mean(mseresults)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0056f1f3-c8e1-4421-a778-11fbfc7128ae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1\n",
      "\t Scenario 1\n",
      "\t\t\tQuantile Network\n",
      "Use RandomForestRegressor\n",
      "ymax and min: tensor(2.5219) tensor(-3.0606)\n",
      "Using marginal loss\n",
      "\t\tEpoch 1\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 1\n",
      "Validation loss: 0.026358451694250107 Best: 0.026358451694250107\n",
      "\t\tEpoch 2\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 2\n",
      "Validation loss: 0.025869162753224373 Best: 0.025869162753224373\n",
      "\t\tEpoch 3\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 3\n",
      "Validation loss: 0.023864511400461197 Best: 0.023864511400461197\n",
      "\t\tEpoch 4\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 4\n",
      "Validation loss: 0.022150548174977303 Best: 0.022150548174977303\n",
      "\t\tEpoch 5\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 5\n",
      "Validation loss: 0.020165683701634407 Best: 0.020165683701634407\n",
      "\t\tEpoch 6\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.020991576835513115 Best: 0.020165683701634407\n",
      "\t\tEpoch 7\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 7\n",
      "Validation loss: 0.01838967204093933 Best: 0.01838967204093933\n",
      "\t\tEpoch 8\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 8\n",
      "Validation loss: 0.0167198795825243 Best: 0.0167198795825243\n",
      "\t\tEpoch 9\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 9\n",
      "Validation loss: 0.015262993052601814 Best: 0.015262993052601814\n",
      "\t\tEpoch 10\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01771533116698265 Best: 0.015262993052601814\n",
      "\t\tEpoch 11\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 11\n",
      "Validation loss: 0.01513770129531622 Best: 0.01513770129531622\n",
      "\t\tEpoch 12\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01573878340423107 Best: 0.01513770129531622\n",
      "\t\tEpoch 13\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.018687214702367783 Best: 0.01513770129531622\n",
      "\t\tEpoch 14\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 14\n",
      "Validation loss: 0.013480020686984062 Best: 0.013480020686984062\n",
      "\t\tEpoch 15\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 15\n",
      "Validation loss: 0.012454946525394917 Best: 0.012454946525394917\n",
      "\t\tEpoch 16\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01535906083881855 Best: 0.012454946525394917\n",
      "\t\tEpoch 17\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 17\n",
      "Validation loss: 0.01167889405041933 Best: 0.01167889405041933\n",
      "\t\tEpoch 18\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014134282246232033 Best: 0.01167889405041933\n",
      "\t\tEpoch 19\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.05\n",
      "Validation loss: 0.012851329520344734 Best: 0.01167889405041933\n",
      "\t\tEpoch 20\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015028975903987885 Best: 0.01167889405041933\n",
      "\t\tEpoch 21\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012515597976744175 Best: 0.01167889405041933\n",
      "\t\tEpoch 22\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01552288793027401 Best: 0.01167889405041933\n",
      "\t\tEpoch 23\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01284854393452406 Best: 0.01167889405041933\n",
      "\t\tEpoch 24\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012021869421005249 Best: 0.01167889405041933\n",
      "\t\tEpoch 25\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.025\n",
      "Validation loss: 0.012657849118113518 Best: 0.01167889405041933\n",
      "\t\tEpoch 26\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014534981921315193 Best: 0.01167889405041933\n",
      "\t\tEpoch 27\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.016008513048291206 Best: 0.01167889405041933\n",
      "\t\tEpoch 28\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01345662958920002 Best: 0.01167889405041933\n",
      "\t\tEpoch 29\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01233003381639719 Best: 0.01167889405041933\n",
      "\t\tEpoch 30\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01262381486594677 Best: 0.01167889405041933\n",
      "\t\tEpoch 31\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0125\n",
      "Validation loss: 0.012513288296759129 Best: 0.01167889405041933\n",
      "\t\tEpoch 32\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013504142872989178 Best: 0.01167889405041933\n",
      "\t\tEpoch 33\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013062470592558384 Best: 0.01167889405041933\n",
      "\t\tEpoch 34\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013775267638266087 Best: 0.01167889405041933\n",
      "\t\tEpoch 35\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014152184128761292 Best: 0.01167889405041933\n",
      "\t\tEpoch 36\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014166346751153469 Best: 0.01167889405041933\n",
      "\t\tEpoch 37\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00625\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 37\n",
      "Validation loss: 0.011666322126984596 Best: 0.011666322126984596\n",
      "\t\tEpoch 38\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 38\n",
      "Validation loss: 0.010689632967114449 Best: 0.010689632967114449\n",
      "\t\tEpoch 39\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011179400607943535 Best: 0.010689632967114449\n",
      "\t\tEpoch 40\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014264515601098537 Best: 0.010689632967114449\n",
      "\t\tEpoch 41\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012402990832924843 Best: 0.010689632967114449\n",
      "\t\tEpoch 42\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012622774578630924 Best: 0.010689632967114449\n",
      "\t\tEpoch 43\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01306464709341526 Best: 0.010689632967114449\n",
      "\t\tEpoch 44\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014045016840100288 Best: 0.010689632967114449\n",
      "\t\tEpoch 45\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.003125\n",
      "Validation loss: 0.013781926594674587 Best: 0.010689632967114449\n",
      "\t\tEpoch 46\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015134574845433235 Best: 0.010689632967114449\n",
      "\t\tEpoch 47\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014123906381428242 Best: 0.010689632967114449\n",
      "\t\tEpoch 48\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014135181903839111 Best: 0.010689632967114449\n",
      "\t\tEpoch 49\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013291333802044392 Best: 0.010689632967114449\n",
      "\t\tEpoch 50\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014189198613166809 Best: 0.010689632967114449\n",
      "\t\tEpoch 51\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0015625\n",
      "Validation loss: 0.014138385653495789 Best: 0.010689632967114449\n",
      "\t\tEpoch 52\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013994690962135792 Best: 0.010689632967114449\n",
      "\t\tEpoch 53\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01376695092767477 Best: 0.010689632967114449\n",
      "\t\tEpoch 54\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015120765194296837 Best: 0.010689632967114449\n",
      "\t\tEpoch 55\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014906161464750767 Best: 0.010689632967114449\n",
      "\t\tEpoch 56\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014049376361072063 Best: 0.010689632967114449\n",
      "\t\tEpoch 57\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00078125\n",
      "Validation loss: 0.013853222131729126 Best: 0.010689632967114449\n",
      "\t\tEpoch 58\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013274535536766052 Best: 0.010689632967114449\n",
      "\t\tEpoch 59\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013120422139763832 Best: 0.010689632967114449\n",
      "\t\tEpoch 60\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014779878780245781 Best: 0.010689632967114449\n",
      "\t\tEpoch 61\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01835234835743904 Best: 0.010689632967114449\n",
      "\t\tEpoch 62\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014501829631626606 Best: 0.010689632967114449\n",
      "\t\tEpoch 63\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.000390625\n",
      "Validation loss: 0.01325541827827692 Best: 0.010689632967114449\n",
      "\t\tEpoch 64\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.016398251056671143 Best: 0.010689632967114449\n",
      "\t\tEpoch 65\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013208718039095402 Best: 0.010689632967114449\n",
      "\t\tEpoch 66\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01488359272480011 Best: 0.010689632967114449\n",
      "\t\tEpoch 67\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01231814082711935 Best: 0.010689632967114449\n",
      "\t\tEpoch 68\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0140043580904603 Best: 0.010689632967114449\n",
      "\t\tEpoch 69\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0001953125\n",
      "Validation loss: 0.01293813344091177 Best: 0.010689632967114449\n",
      "\t\tEpoch 70\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012086896225810051 Best: 0.010689632967114449\n",
      "\t\tEpoch 71\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013436337932944298 Best: 0.010689632967114449\n",
      "\t\tEpoch 72\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013094155117869377 Best: 0.010689632967114449\n",
      "\t\tEpoch 73\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015868274495005608 Best: 0.010689632967114449\n",
      "\t\tEpoch 74\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014021513983607292 Best: 0.010689632967114449\n",
      "\t\tEpoch 75\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 9.765625e-05\n",
      "Validation loss: 0.013504670932888985 Best: 0.010689632967114449\n",
      "\t\tEpoch 76\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013696582987904549 Best: 0.010689632967114449\n",
      "\t\tEpoch 77\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014488892629742622 Best: 0.010689632967114449\n",
      "\t\tEpoch 78\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015385136939585209 Best: 0.010689632967114449\n",
      "\t\tEpoch 79\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013923847116529942 Best: 0.010689632967114449\n",
      "\t\tEpoch 80\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013109371066093445 Best: 0.010689632967114449\n",
      "\t\tEpoch 81\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 4.8828125e-05\n",
      "Validation loss: 0.01608726568520069 Best: 0.010689632967114449\n",
      "\t\tEpoch 82\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013581918552517891 Best: 0.010689632967114449\n",
      "\t\tEpoch 83\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015229957178235054 Best: 0.010689632967114449\n",
      "\t\tEpoch 84\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012368342839181423 Best: 0.010689632967114449\n",
      "\t\tEpoch 85\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012960972264409065 Best: 0.010689632967114449\n",
      "\t\tEpoch 86\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015638474375009537 Best: 0.010689632967114449\n",
      "\t\tEpoch 87\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 2.44140625e-05\n",
      "Validation loss: 0.012184999883174896 Best: 0.010689632967114449\n",
      "\t\tEpoch 88\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01488867960870266 Best: 0.010689632967114449\n",
      "\t\tEpoch 89\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014858714304864407 Best: 0.010689632967114449\n",
      "\t\tEpoch 90\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015020604245364666 Best: 0.010689632967114449\n",
      "\t\tEpoch 91\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013703365810215473 Best: 0.010689632967114449\n",
      "\t\tEpoch 92\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01364660169929266 Best: 0.010689632967114449\n",
      "\t\tEpoch 93\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 1.220703125e-05\n",
      "Validation loss: 0.013933411799371243 Best: 0.010689632967114449\n",
      "\t\tEpoch 94\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01126511674374342 Best: 0.010689632967114449\n",
      "\t\tEpoch 95\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011368506588041782 Best: 0.010689632967114449\n",
      "\t\tEpoch 96\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013665318489074707 Best: 0.010689632967114449\n",
      "\t\tEpoch 97\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015038629062473774 Best: 0.010689632967114449\n",
      "\t\tEpoch 98\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014027808792889118 Best: 0.010689632967114449\n",
      "\t\tEpoch 99\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 6.103515625e-06\n",
      "Validation loss: 0.014172217808663845 Best: 0.010689632967114449\n",
      "\t\tEpoch 100\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013531194999814034 Best: 0.010689632967114449\n",
      "0.07405781294130011\n",
      "\t\t\tQuantile Network\n",
      "Use RandomForestRegressor\n",
      "ymax and min: tensor(2.5522) tensor(-3.0764)\n",
      "Using marginal loss\n",
      "\t\tEpoch 1\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 1\n",
      "Validation loss: 0.019546080380678177 Best: 0.019546080380678177\n",
      "\t\tEpoch 2\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 2\n",
      "Validation loss: 0.01909896917641163 Best: 0.01909896917641163\n",
      "\t\tEpoch 3\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 3\n",
      "Validation loss: 0.01835591159760952 Best: 0.01835591159760952\n",
      "\t\tEpoch 4\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 4\n",
      "Validation loss: 0.01758609153330326 Best: 0.01758609153330326\n",
      "\t\tEpoch 5\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 5\n",
      "Validation loss: 0.017401209101080894 Best: 0.017401209101080894\n",
      "\t\tEpoch 6\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 6\n",
      "Validation loss: 0.016465967521071434 Best: 0.016465967521071434\n",
      "\t\tEpoch 7\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 7\n",
      "Validation loss: 0.014319449663162231 Best: 0.014319449663162231\n",
      "\t\tEpoch 8\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.017193520441651344 Best: 0.014319449663162231\n",
      "\t\tEpoch 9\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014756744727492332 Best: 0.014319449663162231\n",
      "\t\tEpoch 10\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 10\n",
      "Validation loss: 0.013096979819238186 Best: 0.013096979819238186\n",
      "\t\tEpoch 11\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014221950434148312 Best: 0.013096979819238186\n",
      "\t\tEpoch 12\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014519494958221912 Best: 0.013096979819238186\n",
      "\t\tEpoch 13\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.016116155311465263 Best: 0.013096979819238186\n",
      "\t\tEpoch 14\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01626739278435707 Best: 0.013096979819238186\n",
      "\t\tEpoch 15\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.05\n",
      "Validation loss: 0.015484964475035667 Best: 0.013096979819238186\n",
      "\t\tEpoch 16\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0165234562009573 Best: 0.013096979819238186\n",
      "\t\tEpoch 17\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.017539408057928085 Best: 0.013096979819238186\n",
      "\t\tEpoch 18\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.017521748319268227 Best: 0.013096979819238186\n",
      "\t\tEpoch 19\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0148005997762084 Best: 0.013096979819238186\n",
      "\t\tEpoch 20\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013230816461145878 Best: 0.013096979819238186\n",
      "\t\tEpoch 21\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.025\n",
      "Validation loss: 0.014793326146900654 Best: 0.013096979819238186\n",
      "\t\tEpoch 22\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014773412607610226 Best: 0.013096979819238186\n",
      "\t\tEpoch 23\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014043848030269146 Best: 0.013096979819238186\n",
      "\t\tEpoch 24\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014651493169367313 Best: 0.013096979819238186\n",
      "\t\tEpoch 25\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.017318101599812508 Best: 0.013096979819238186\n",
      "\t\tEpoch 26\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014616312459111214 Best: 0.013096979819238186\n",
      "\t\tEpoch 27\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0125\n",
      "Validation loss: 0.01591365784406662 Best: 0.013096979819238186\n",
      "\t\tEpoch 28\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.016196900978684425 Best: 0.013096979819238186\n",
      "\t\tEpoch 29\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.016092775389552116 Best: 0.013096979819238186\n",
      "\t\tEpoch 30\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01572885736823082 Best: 0.013096979819238186\n",
      "\t\tEpoch 31\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014911516569554806 Best: 0.013096979819238186\n",
      "\t\tEpoch 32\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01435659546405077 Best: 0.013096979819238186\n",
      "\t\tEpoch 33\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00625\n",
      "Validation loss: 0.016193201765418053 Best: 0.013096979819238186\n",
      "\t\tEpoch 34\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013551526702940464 Best: 0.013096979819238186\n",
      "\t\tEpoch 35\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0166180357336998 Best: 0.013096979819238186\n",
      "\t\tEpoch 36\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01449183002114296 Best: 0.013096979819238186\n",
      "\t\tEpoch 37\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01374023500829935 Best: 0.013096979819238186\n",
      "\t\tEpoch 38\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014729700051248074 Best: 0.013096979819238186\n",
      "\t\tEpoch 39\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.003125\n",
      "Validation loss: 0.016397416591644287 Best: 0.013096979819238186\n",
      "\t\tEpoch 40\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.017606662586331367 Best: 0.013096979819238186\n",
      "\t\tEpoch 41\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014703174121677876 Best: 0.013096979819238186\n",
      "\t\tEpoch 42\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015531285665929317 Best: 0.013096979819238186\n",
      "\t\tEpoch 43\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014317130669951439 Best: 0.013096979819238186\n",
      "\t\tEpoch 44\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.016108902171254158 Best: 0.013096979819238186\n",
      "\t\tEpoch 45\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0015625\n",
      "Validation loss: 0.013277137652039528 Best: 0.013096979819238186\n",
      "\t\tEpoch 46\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014010099694132805 Best: 0.013096979819238186\n",
      "\t\tEpoch 47\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.016630984842777252 Best: 0.013096979819238186\n",
      "\t\tEpoch 48\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01609181798994541 Best: 0.013096979819238186\n",
      "\t\tEpoch 49\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014012171886861324 Best: 0.013096979819238186\n",
      "\t\tEpoch 50\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.016880260780453682 Best: 0.013096979819238186\n",
      "\t\tEpoch 51\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00078125\n",
      "Validation loss: 0.01607775315642357 Best: 0.013096979819238186\n",
      "\t\tEpoch 52\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015203330665826797 Best: 0.013096979819238186\n",
      "\t\tEpoch 53\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.016363978385925293 Best: 0.013096979819238186\n",
      "\t\tEpoch 54\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0167250894010067 Best: 0.013096979819238186\n",
      "\t\tEpoch 55\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015267778187990189 Best: 0.013096979819238186\n",
      "\t\tEpoch 56\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014846750535070896 Best: 0.013096979819238186\n",
      "\t\tEpoch 57\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.000390625\n",
      "Validation loss: 0.015074651688337326 Best: 0.013096979819238186\n",
      "\t\tEpoch 58\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015032147988677025 Best: 0.013096979819238186\n",
      "\t\tEpoch 59\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014508062973618507 Best: 0.013096979819238186\n",
      "\t\tEpoch 60\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 60\n",
      "Validation loss: 0.012779236771166325 Best: 0.012779236771166325\n",
      "\t\tEpoch 61\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01406113151460886 Best: 0.012779236771166325\n",
      "\t\tEpoch 62\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01448745559900999 Best: 0.012779236771166325\n",
      "\t\tEpoch 63\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01618676446378231 Best: 0.012779236771166325\n",
      "\t\tEpoch 64\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0001953125\n",
      "Validation loss: 0.016779810190200806 Best: 0.012779236771166325\n",
      "\t\tEpoch 65\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014364023692905903 Best: 0.012779236771166325\n",
      "\t\tEpoch 66\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012950479052960873 Best: 0.012779236771166325\n",
      "\t\tEpoch 67\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013307361863553524 Best: 0.012779236771166325\n",
      "\t\tEpoch 68\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014262869022786617 Best: 0.012779236771166325\n",
      "\t\tEpoch 69\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01584251970052719 Best: 0.012779236771166325\n",
      "\t\tEpoch 70\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 9.765625e-05\n",
      "Validation loss: 0.013938608579337597 Best: 0.012779236771166325\n",
      "\t\tEpoch 71\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015135838650166988 Best: 0.012779236771166325\n",
      "\t\tEpoch 72\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01614764891564846 Best: 0.012779236771166325\n",
      "\t\tEpoch 73\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014196657575666904 Best: 0.012779236771166325\n",
      "\t\tEpoch 74\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014372114092111588 Best: 0.012779236771166325\n",
      "\t\tEpoch 75\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014275227673351765 Best: 0.012779236771166325\n",
      "\t\tEpoch 76\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 4.8828125e-05\n",
      "Validation loss: 0.014093327336013317 Best: 0.012779236771166325\n",
      "\t\tEpoch 77\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015284482389688492 Best: 0.012779236771166325\n",
      "\t\tEpoch 78\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014587972313165665 Best: 0.012779236771166325\n",
      "\t\tEpoch 79\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.016038330271840096 Best: 0.012779236771166325\n",
      "\t\tEpoch 80\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013966641388833523 Best: 0.012779236771166325\n",
      "\t\tEpoch 81\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013128606602549553 Best: 0.012779236771166325\n",
      "\t\tEpoch 82\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 2.44140625e-05\n",
      "Validation loss: 0.01485802698880434 Best: 0.012779236771166325\n",
      "\t\tEpoch 83\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014413269236683846 Best: 0.012779236771166325\n",
      "\t\tEpoch 84\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014511942863464355 Best: 0.012779236771166325\n",
      "\t\tEpoch 85\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 85\n",
      "Validation loss: 0.012527372688055038 Best: 0.012527372688055038\n",
      "\t\tEpoch 86\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.016714485362172127 Best: 0.012527372688055038\n",
      "\t\tEpoch 87\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014506627805531025 Best: 0.012527372688055038\n",
      "\t\tEpoch 88\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015098879113793373 Best: 0.012527372688055038\n",
      "\t\tEpoch 89\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 1.220703125e-05\n",
      "Validation loss: 0.016174137592315674 Best: 0.012527372688055038\n",
      "\t\tEpoch 90\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01489982195198536 Best: 0.012527372688055038\n",
      "\t\tEpoch 91\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012803063727915287 Best: 0.012527372688055038\n",
      "\t\tEpoch 92\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014607382006943226 Best: 0.012527372688055038\n",
      "\t\tEpoch 93\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013453277759253979 Best: 0.012527372688055038\n",
      "\t\tEpoch 94\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014010041952133179 Best: 0.012527372688055038\n",
      "\t\tEpoch 95\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 6.103515625e-06\n",
      "Validation loss: 0.014404487796127796 Best: 0.012527372688055038\n",
      "\t\tEpoch 96\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013477359898388386 Best: 0.012527372688055038\n",
      "\t\tEpoch 97\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012943009845912457 Best: 0.012527372688055038\n",
      "\t\tEpoch 98\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 98\n",
      "Validation loss: 0.012014553882181644 Best: 0.012014553882181644\n",
      "\t\tEpoch 99\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015218476764857769 Best: 0.012014553882181644\n",
      "\t\tEpoch 100\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014169260859489441 Best: 0.012014553882181644\n",
      "0.0829360347469598\n",
      "\t\t\tQuantile Network\n",
      "Use RandomForestRegressor\n",
      "ymax and min: tensor(2.5760) tensor(-2.7088)\n",
      "Using marginal loss\n",
      "\t\tEpoch 1\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 1\n",
      "Validation loss: 0.019223693758249283 Best: 0.019223693758249283\n",
      "\t\tEpoch 2\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 2\n",
      "Validation loss: 0.01915757730603218 Best: 0.01915757730603218\n",
      "\t\tEpoch 3\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 3\n",
      "Validation loss: 0.018175721168518066 Best: 0.018175721168518066\n",
      "\t\tEpoch 4\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.018781661987304688 Best: 0.018175721168518066\n",
      "\t\tEpoch 5\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 5\n",
      "Validation loss: 0.016925208270549774 Best: 0.016925208270549774\n",
      "\t\tEpoch 6\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 6\n",
      "Validation loss: 0.015521817840635777 Best: 0.015521817840635777\n",
      "\t\tEpoch 7\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 7\n",
      "Validation loss: 0.014192735776305199 Best: 0.014192735776305199\n",
      "\t\tEpoch 8\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015185747295618057 Best: 0.014192735776305199\n",
      "\t\tEpoch 9\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014617006294429302 Best: 0.014192735776305199\n",
      "\t\tEpoch 10\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 10\n",
      "Validation loss: 0.013684708625078201 Best: 0.013684708625078201\n",
      "\t\tEpoch 11\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 11\n",
      "Validation loss: 0.012956609018146992 Best: 0.012956609018146992\n",
      "\t\tEpoch 12\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01371984276920557 Best: 0.012956609018146992\n",
      "\t\tEpoch 13\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015047152526676655 Best: 0.012956609018146992\n",
      "\t\tEpoch 14\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014768351800739765 Best: 0.012956609018146992\n",
      "\t\tEpoch 15\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.05\n",
      "Validation loss: 0.014666632749140263 Best: 0.012956609018146992\n",
      "\t\tEpoch 16\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014490705914795399 Best: 0.012956609018146992\n",
      "\t\tEpoch 17\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014365282841026783 Best: 0.012956609018146992\n",
      "\t\tEpoch 18\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013014348223805428 Best: 0.012956609018146992\n",
      "\t\tEpoch 19\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014811458066105843 Best: 0.012956609018146992\n",
      "\t\tEpoch 20\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01344897411763668 Best: 0.012956609018146992\n",
      "\t\tEpoch 21\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.025\n",
      "Validation loss: 0.014184578321874142 Best: 0.012956609018146992\n",
      "\t\tEpoch 22\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014068638905882835 Best: 0.012956609018146992\n",
      "\t\tEpoch 23\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01374620757997036 Best: 0.012956609018146992\n",
      "\t\tEpoch 24\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015761127695441246 Best: 0.012956609018146992\n",
      "\t\tEpoch 25\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01437323447316885 Best: 0.012956609018146992\n",
      "\t\tEpoch 26\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015504644252359867 Best: 0.012956609018146992\n",
      "\t\tEpoch 27\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0125\n",
      "Validation loss: 0.01357153058052063 Best: 0.012956609018146992\n",
      "\t\tEpoch 28\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01374187134206295 Best: 0.012956609018146992\n",
      "\t\tEpoch 29\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014408848248422146 Best: 0.012956609018146992\n",
      "\t\tEpoch 30\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014551051892340183 Best: 0.012956609018146992\n",
      "\t\tEpoch 31\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015706000849604607 Best: 0.012956609018146992\n",
      "\t\tEpoch 32\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015475134365260601 Best: 0.012956609018146992\n",
      "\t\tEpoch 33\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00625\n",
      "Validation loss: 0.015327934175729752 Best: 0.012956609018146992\n",
      "\t\tEpoch 34\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014352882280945778 Best: 0.012956609018146992\n",
      "\t\tEpoch 35\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015820641070604324 Best: 0.012956609018146992\n",
      "\t\tEpoch 36\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014965484850108624 Best: 0.012956609018146992\n",
      "\t\tEpoch 37\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0167548768222332 Best: 0.012956609018146992\n",
      "\t\tEpoch 38\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014403847977519035 Best: 0.012956609018146992\n",
      "\t\tEpoch 39\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.003125\n",
      "Validation loss: 0.015891527757048607 Best: 0.012956609018146992\n",
      "\t\tEpoch 40\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01660127565264702 Best: 0.012956609018146992\n",
      "\t\tEpoch 41\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015537410043179989 Best: 0.012956609018146992\n",
      "\t\tEpoch 42\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01535319909453392 Best: 0.012956609018146992\n",
      "\t\tEpoch 43\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.016373874619603157 Best: 0.012956609018146992\n",
      "\t\tEpoch 44\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014039763249456882 Best: 0.012956609018146992\n",
      "\t\tEpoch 45\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0015625\n",
      "Validation loss: 0.013455350883305073 Best: 0.012956609018146992\n",
      "\t\tEpoch 46\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013731409795582294 Best: 0.012956609018146992\n",
      "\t\tEpoch 47\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014145705848932266 Best: 0.012956609018146992\n",
      "\t\tEpoch 48\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015239574946463108 Best: 0.012956609018146992\n",
      "\t\tEpoch 49\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01506885327398777 Best: 0.012956609018146992\n",
      "\t\tEpoch 50\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01531152706593275 Best: 0.012956609018146992\n",
      "\t\tEpoch 51\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00078125\n",
      "Validation loss: 0.014305760152637959 Best: 0.012956609018146992\n",
      "\t\tEpoch 52\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01446295902132988 Best: 0.012956609018146992\n",
      "\t\tEpoch 53\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013156936503946781 Best: 0.012956609018146992\n",
      "\t\tEpoch 54\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013298231177031994 Best: 0.012956609018146992\n",
      "\t\tEpoch 55\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014250627718865871 Best: 0.012956609018146992\n",
      "\t\tEpoch 56\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013506748713552952 Best: 0.012956609018146992\n",
      "\t\tEpoch 57\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.000390625\n",
      "Validation loss: 0.015209258534014225 Best: 0.012956609018146992\n",
      "\t\tEpoch 58\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014686288312077522 Best: 0.012956609018146992\n",
      "\t\tEpoch 59\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01488909125328064 Best: 0.012956609018146992\n",
      "\t\tEpoch 60\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013815557584166527 Best: 0.012956609018146992\n",
      "\t\tEpoch 61\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01617536135017872 Best: 0.012956609018146992\n",
      "\t\tEpoch 62\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.016551190987229347 Best: 0.012956609018146992\n",
      "\t\tEpoch 63\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0001953125\n",
      "Validation loss: 0.016641981899738312 Best: 0.012956609018146992\n",
      "\t\tEpoch 64\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015350417234003544 Best: 0.012956609018146992\n",
      "\t\tEpoch 65\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.017346365377306938 Best: 0.012956609018146992\n",
      "\t\tEpoch 66\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015650099143385887 Best: 0.012956609018146992\n",
      "\t\tEpoch 67\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014990333467721939 Best: 0.012956609018146992\n",
      "\t\tEpoch 68\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01685922034084797 Best: 0.012956609018146992\n",
      "\t\tEpoch 69\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 9.765625e-05\n",
      "Validation loss: 0.016912568360567093 Best: 0.012956609018146992\n",
      "\t\tEpoch 70\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0176423117518425 Best: 0.012956609018146992\n",
      "\t\tEpoch 71\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.016475889831781387 Best: 0.012956609018146992\n",
      "\t\tEpoch 72\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015328451991081238 Best: 0.012956609018146992\n",
      "\t\tEpoch 73\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014203025959432125 Best: 0.012956609018146992\n",
      "\t\tEpoch 74\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014072946272790432 Best: 0.012956609018146992\n",
      "\t\tEpoch 75\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 4.8828125e-05\n",
      "Validation loss: 0.014062621630728245 Best: 0.012956609018146992\n",
      "\t\tEpoch 76\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014494556002318859 Best: 0.012956609018146992\n",
      "\t\tEpoch 77\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01557683851569891 Best: 0.012956609018146992\n",
      "\t\tEpoch 78\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014688190072774887 Best: 0.012956609018146992\n",
      "\t\tEpoch 79\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014495575800538063 Best: 0.012956609018146992\n",
      "\t\tEpoch 80\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015695013105869293 Best: 0.012956609018146992\n",
      "\t\tEpoch 81\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 2.44140625e-05\n",
      "Validation loss: 0.01691105030477047 Best: 0.012956609018146992\n",
      "\t\tEpoch 82\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.02002902515232563 Best: 0.012956609018146992\n",
      "\t\tEpoch 83\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.016614880412817 Best: 0.012956609018146992\n",
      "\t\tEpoch 84\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015420029871165752 Best: 0.012956609018146992\n",
      "\t\tEpoch 85\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015488656237721443 Best: 0.012956609018146992\n",
      "\t\tEpoch 86\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.016014061868190765 Best: 0.012956609018146992\n",
      "\t\tEpoch 87\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 1.220703125e-05\n",
      "Validation loss: 0.014556852169334888 Best: 0.012956609018146992\n",
      "\t\tEpoch 88\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015169219113886356 Best: 0.012956609018146992\n",
      "\t\tEpoch 89\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.017167627811431885 Best: 0.012956609018146992\n",
      "\t\tEpoch 90\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015677059069275856 Best: 0.012956609018146992\n",
      "\t\tEpoch 91\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014772986993193626 Best: 0.012956609018146992\n",
      "\t\tEpoch 92\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015599310398101807 Best: 0.012956609018146992\n",
      "\t\tEpoch 93\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 6.103515625e-06\n",
      "Validation loss: 0.015146092511713505 Best: 0.012956609018146992\n",
      "\t\tEpoch 94\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.017583707347512245 Best: 0.012956609018146992\n",
      "\t\tEpoch 95\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015183071605861187 Best: 0.012956609018146992\n",
      "\t\tEpoch 96\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.016009805724024773 Best: 0.012956609018146992\n",
      "\t\tEpoch 97\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015625042840838432 Best: 0.012956609018146992\n",
      "\t\tEpoch 98\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01580100692808628 Best: 0.012956609018146992\n",
      "\t\tEpoch 99\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 3.0517578125e-06\n",
      "Validation loss: 0.016714371740818024 Best: 0.012956609018146992\n",
      "\t\tEpoch 100\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.017290357500314713 Best: 0.012956609018146992\n",
      "0.10165472361494615\n",
      "\t\t\tQuantile Network\n",
      "Use RandomForestRegressor\n",
      "ymax and min: tensor(2.4767) tensor(-3.0323)\n",
      "Using marginal loss\n",
      "\t\tEpoch 1\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 1\n",
      "Validation loss: 0.02174055203795433 Best: 0.02174055203795433\n",
      "\t\tEpoch 2\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.021863825619220734 Best: 0.02174055203795433\n",
      "\t\tEpoch 3\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.02221238985657692 Best: 0.02174055203795433\n",
      "\t\tEpoch 4\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 4\n",
      "Validation loss: 0.020821670070290565 Best: 0.020821670070290565\n",
      "\t\tEpoch 5\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 5\n",
      "Validation loss: 0.019669728353619576 Best: 0.019669728353619576\n",
      "\t\tEpoch 6\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 6\n",
      "Validation loss: 0.018198063597083092 Best: 0.018198063597083092\n",
      "\t\tEpoch 7\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 7\n",
      "Validation loss: 0.015389381907880306 Best: 0.015389381907880306\n",
      "\t\tEpoch 8\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 8\n",
      "Validation loss: 0.015179669484496117 Best: 0.015179669484496117\n",
      "\t\tEpoch 9\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 9\n",
      "Validation loss: 0.014825067482888699 Best: 0.014825067482888699\n",
      "\t\tEpoch 10\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 10\n",
      "Validation loss: 0.014055208303034306 Best: 0.014055208303034306\n",
      "\t\tEpoch 11\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 11\n",
      "Validation loss: 0.012388751842081547 Best: 0.012388751842081547\n",
      "\t\tEpoch 12\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013759504072368145 Best: 0.012388751842081547\n",
      "\t\tEpoch 13\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015636734664440155 Best: 0.012388751842081547\n",
      "\t\tEpoch 14\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01562504470348358 Best: 0.012388751842081547\n",
      "\t\tEpoch 15\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014888191595673561 Best: 0.012388751842081547\n",
      "\t\tEpoch 16\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.05\n",
      "Validation loss: 0.014081669971346855 Best: 0.012388751842081547\n",
      "\t\tEpoch 17\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 17\n",
      "Validation loss: 0.011484603397548199 Best: 0.011484603397548199\n",
      "\t\tEpoch 18\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01301359198987484 Best: 0.011484603397548199\n",
      "\t\tEpoch 19\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013019400648772717 Best: 0.011484603397548199\n",
      "\t\tEpoch 20\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012132328934967518 Best: 0.011484603397548199\n",
      "\t\tEpoch 21\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014494325965642929 Best: 0.011484603397548199\n",
      "\t\tEpoch 22\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011501457542181015 Best: 0.011484603397548199\n",
      "\t\tEpoch 23\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.025\n",
      "Validation loss: 0.013099109753966331 Best: 0.011484603397548199\n",
      "\t\tEpoch 24\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014541739597916603 Best: 0.011484603397548199\n",
      "\t\tEpoch 25\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013607427477836609 Best: 0.011484603397548199\n",
      "\t\tEpoch 26\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013302478939294815 Best: 0.011484603397548199\n",
      "\t\tEpoch 27\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01382612343877554 Best: 0.011484603397548199\n",
      "\t\tEpoch 28\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014515477232635021 Best: 0.011484603397548199\n",
      "\t\tEpoch 29\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0125\n",
      "Validation loss: 0.015177576802670956 Best: 0.011484603397548199\n",
      "\t\tEpoch 30\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014592786319553852 Best: 0.011484603397548199\n",
      "\t\tEpoch 31\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012999773025512695 Best: 0.011484603397548199\n",
      "\t\tEpoch 32\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012357036583125591 Best: 0.011484603397548199\n",
      "\t\tEpoch 33\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013570830225944519 Best: 0.011484603397548199\n",
      "\t\tEpoch 34\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01388649083673954 Best: 0.011484603397548199\n",
      "\t\tEpoch 35\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00625\n",
      "Validation loss: 0.013081197626888752 Best: 0.011484603397548199\n",
      "\t\tEpoch 36\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013742302544414997 Best: 0.011484603397548199\n",
      "\t\tEpoch 37\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014710602350533009 Best: 0.011484603397548199\n",
      "\t\tEpoch 38\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013617503456771374 Best: 0.011484603397548199\n",
      "\t\tEpoch 39\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01494972687214613 Best: 0.011484603397548199\n",
      "\t\tEpoch 40\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012365596368908882 Best: 0.011484603397548199\n",
      "\t\tEpoch 41\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.003125\n",
      "Validation loss: 0.01273615937680006 Best: 0.011484603397548199\n",
      "\t\tEpoch 42\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015629811212420464 Best: 0.011484603397548199\n",
      "\t\tEpoch 43\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012458281591534615 Best: 0.011484603397548199\n",
      "\t\tEpoch 44\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014153973199427128 Best: 0.011484603397548199\n",
      "\t\tEpoch 45\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012974139302968979 Best: 0.011484603397548199\n",
      "\t\tEpoch 46\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012917858548462391 Best: 0.011484603397548199\n",
      "\t\tEpoch 47\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0015625\n",
      "Validation loss: 0.013422194868326187 Best: 0.011484603397548199\n",
      "\t\tEpoch 48\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015136797912418842 Best: 0.011484603397548199\n",
      "\t\tEpoch 49\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01673283614218235 Best: 0.011484603397548199\n",
      "\t\tEpoch 50\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01578972488641739 Best: 0.011484603397548199\n",
      "\t\tEpoch 51\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01603022962808609 Best: 0.011484603397548199\n",
      "\t\tEpoch 52\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01668771542608738 Best: 0.011484603397548199\n",
      "\t\tEpoch 53\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00078125\n",
      "Validation loss: 0.017518403008580208 Best: 0.011484603397548199\n",
      "\t\tEpoch 54\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.017541760578751564 Best: 0.011484603397548199\n",
      "\t\tEpoch 55\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.016754932701587677 Best: 0.011484603397548199\n",
      "\t\tEpoch 56\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015153473243117332 Best: 0.011484603397548199\n",
      "\t\tEpoch 57\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01538426149636507 Best: 0.011484603397548199\n",
      "\t\tEpoch 58\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01643238589167595 Best: 0.011484603397548199\n",
      "\t\tEpoch 59\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.000390625\n",
      "Validation loss: 0.015027078799903393 Best: 0.011484603397548199\n",
      "\t\tEpoch 60\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014538053423166275 Best: 0.011484603397548199\n",
      "\t\tEpoch 61\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014595708809792995 Best: 0.011484603397548199\n",
      "\t\tEpoch 62\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014857828617095947 Best: 0.011484603397548199\n",
      "\t\tEpoch 63\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.016098259016871452 Best: 0.011484603397548199\n",
      "\t\tEpoch 64\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014519891701638699 Best: 0.011484603397548199\n",
      "\t\tEpoch 65\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0001953125\n",
      "Validation loss: 0.012727157212793827 Best: 0.011484603397548199\n",
      "\t\tEpoch 66\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013759136199951172 Best: 0.011484603397548199\n",
      "\t\tEpoch 67\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015493838116526604 Best: 0.011484603397548199\n",
      "\t\tEpoch 68\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015688037499785423 Best: 0.011484603397548199\n",
      "\t\tEpoch 69\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015377146191895008 Best: 0.011484603397548199\n",
      "\t\tEpoch 70\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01579415425658226 Best: 0.011484603397548199\n",
      "\t\tEpoch 71\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 9.765625e-05\n",
      "Validation loss: 0.01406847219914198 Best: 0.011484603397548199\n",
      "\t\tEpoch 72\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013624774292111397 Best: 0.011484603397548199\n",
      "\t\tEpoch 73\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015095891430974007 Best: 0.011484603397548199\n",
      "\t\tEpoch 74\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014476695097982883 Best: 0.011484603397548199\n",
      "\t\tEpoch 75\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014848005957901478 Best: 0.011484603397548199\n",
      "\t\tEpoch 76\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012794177047908306 Best: 0.011484603397548199\n",
      "\t\tEpoch 77\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 4.8828125e-05\n",
      "Validation loss: 0.012953517958521843 Best: 0.011484603397548199\n",
      "\t\tEpoch 78\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013230426236987114 Best: 0.011484603397548199\n",
      "\t\tEpoch 79\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014319405891001225 Best: 0.011484603397548199\n",
      "\t\tEpoch 80\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015535674057900906 Best: 0.011484603397548199\n",
      "\t\tEpoch 81\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013982986100018024 Best: 0.011484603397548199\n",
      "\t\tEpoch 82\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014441507868468761 Best: 0.011484603397548199\n",
      "\t\tEpoch 83\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 2.44140625e-05\n",
      "Validation loss: 0.014678450301289558 Best: 0.011484603397548199\n",
      "\t\tEpoch 84\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014972138218581676 Best: 0.011484603397548199\n",
      "\t\tEpoch 85\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012319369241595268 Best: 0.011484603397548199\n",
      "\t\tEpoch 86\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013020813465118408 Best: 0.011484603397548199\n",
      "\t\tEpoch 87\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015296626836061478 Best: 0.011484603397548199\n",
      "\t\tEpoch 88\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01478329487144947 Best: 0.011484603397548199\n",
      "\t\tEpoch 89\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 1.220703125e-05\n",
      "Validation loss: 0.016604773700237274 Best: 0.011484603397548199\n",
      "\t\tEpoch 90\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014458338730037212 Best: 0.011484603397548199\n",
      "\t\tEpoch 91\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015160209499299526 Best: 0.011484603397548199\n",
      "\t\tEpoch 92\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.016529176384210587 Best: 0.011484603397548199\n",
      "\t\tEpoch 93\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.016760440543293953 Best: 0.011484603397548199\n",
      "\t\tEpoch 94\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014088408090174198 Best: 0.011484603397548199\n",
      "\t\tEpoch 95\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 6.103515625e-06\n",
      "Validation loss: 0.01380907278507948 Best: 0.011484603397548199\n",
      "\t\tEpoch 96\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.016469424590468407 Best: 0.011484603397548199\n",
      "\t\tEpoch 97\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.016414333134889603 Best: 0.011484603397548199\n",
      "\t\tEpoch 98\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01617722399532795 Best: 0.011484603397548199\n",
      "\t\tEpoch 99\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015939809381961823 Best: 0.011484603397548199\n",
      "\t\tEpoch 100\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.017980078235268593 Best: 0.011484603397548199\n",
      "0.08761794100453013\n",
      "\t\t\tQuantile Network\n",
      "Use RandomForestRegressor\n",
      "ymax and min: tensor(2.1502) tensor(-3.0348)\n",
      "Using marginal loss\n",
      "\t\tEpoch 1\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 1\n",
      "Validation loss: 0.019581036642193794 Best: 0.019581036642193794\n",
      "\t\tEpoch 2\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 2\n",
      "Validation loss: 0.01830284111201763 Best: 0.01830284111201763\n",
      "\t\tEpoch 3\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 3\n",
      "Validation loss: 0.018260369077324867 Best: 0.018260369077324867\n",
      "\t\tEpoch 4\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 4\n",
      "Validation loss: 0.015854384750127792 Best: 0.015854384750127792\n",
      "\t\tEpoch 5\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01605459675192833 Best: 0.015854384750127792\n",
      "\t\tEpoch 6\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0161858219653368 Best: 0.015854384750127792\n",
      "\t\tEpoch 7\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 7\n",
      "Validation loss: 0.013911803252995014 Best: 0.013911803252995014\n",
      "\t\tEpoch 8\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014054856263101101 Best: 0.013911803252995014\n",
      "\t\tEpoch 9\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 9\n",
      "Validation loss: 0.012197451665997505 Best: 0.012197451665997505\n",
      "\t\tEpoch 10\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 10\n",
      "Validation loss: 0.011706653982400894 Best: 0.011706653982400894\n",
      "\t\tEpoch 11\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013350356370210648 Best: 0.011706653982400894\n",
      "\t\tEpoch 12\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 12\n",
      "Validation loss: 0.011115589179098606 Best: 0.011115589179098606\n",
      "\t\tEpoch 13\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01257255021482706 Best: 0.011115589179098606\n",
      "\t\tEpoch 14\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01257869228720665 Best: 0.011115589179098606\n",
      "\t\tEpoch 15\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.05\n",
      "Validation loss: 0.013863899745047092 Best: 0.011115589179098606\n",
      "\t\tEpoch 16\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01192537136375904 Best: 0.011115589179098606\n",
      "\t\tEpoch 17\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 17\n",
      "Validation loss: 0.009756612591445446 Best: 0.009756612591445446\n",
      "\t\tEpoch 18\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 18\n",
      "Validation loss: 0.009705173783004284 Best: 0.009705173783004284\n",
      "\t\tEpoch 19\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012580710463225842 Best: 0.009705173783004284\n",
      "\t\tEpoch 20\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014207798056304455 Best: 0.009705173783004284\n",
      "\t\tEpoch 21\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012789038009941578 Best: 0.009705173783004284\n",
      "\t\tEpoch 22\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013152523897588253 Best: 0.009705173783004284\n",
      "\t\tEpoch 23\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.025\n",
      "Validation loss: 0.011814083904027939 Best: 0.009705173783004284\n",
      "\t\tEpoch 24\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012764080427587032 Best: 0.009705173783004284\n",
      "\t\tEpoch 25\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014139161445200443 Best: 0.009705173783004284\n",
      "\t\tEpoch 26\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01304011419415474 Best: 0.009705173783004284\n",
      "\t\tEpoch 27\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011247720569372177 Best: 0.009705173783004284\n",
      "\t\tEpoch 28\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012926014140248299 Best: 0.009705173783004284\n",
      "\t\tEpoch 29\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0125\n",
      "Validation loss: 0.015157312154769897 Best: 0.009705173783004284\n",
      "\t\tEpoch 30\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01596115715801716 Best: 0.009705173783004284\n",
      "\t\tEpoch 31\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01263566967099905 Best: 0.009705173783004284\n",
      "\t\tEpoch 32\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014168167486786842 Best: 0.009705173783004284\n",
      "\t\tEpoch 33\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01344738993793726 Best: 0.009705173783004284\n",
      "\t\tEpoch 34\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012897396460175514 Best: 0.009705173783004284\n",
      "\t\tEpoch 35\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00625\n",
      "Validation loss: 0.012765942141413689 Best: 0.009705173783004284\n",
      "\t\tEpoch 36\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01148133259266615 Best: 0.009705173783004284\n",
      "\t\tEpoch 37\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01106504537165165 Best: 0.009705173783004284\n",
      "\t\tEpoch 38\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.010655885562300682 Best: 0.009705173783004284\n",
      "\t\tEpoch 39\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013216360472142696 Best: 0.009705173783004284\n",
      "\t\tEpoch 40\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012004516087472439 Best: 0.009705173783004284\n",
      "\t\tEpoch 41\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.003125\n",
      "Validation loss: 0.011103779077529907 Best: 0.009705173783004284\n",
      "\t\tEpoch 42\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011458909139037132 Best: 0.009705173783004284\n",
      "\t\tEpoch 43\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011571180075407028 Best: 0.009705173783004284\n",
      "\t\tEpoch 44\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012007818557322025 Best: 0.009705173783004284\n",
      "\t\tEpoch 45\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013757562264800072 Best: 0.009705173783004284\n",
      "\t\tEpoch 46\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015867648646235466 Best: 0.009705173783004284\n",
      "\t\tEpoch 47\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0015625\n",
      "Validation loss: 0.011175449937582016 Best: 0.009705173783004284\n",
      "\t\tEpoch 48\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013034430332481861 Best: 0.009705173783004284\n",
      "\t\tEpoch 49\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014427770860493183 Best: 0.009705173783004284\n",
      "\t\tEpoch 50\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013257250189781189 Best: 0.009705173783004284\n",
      "\t\tEpoch 51\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01287724357098341 Best: 0.009705173783004284\n",
      "\t\tEpoch 52\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014811947010457516 Best: 0.009705173783004284\n",
      "\t\tEpoch 53\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00078125\n",
      "Validation loss: 0.013022905215620995 Best: 0.009705173783004284\n",
      "\t\tEpoch 54\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01217514555901289 Best: 0.009705173783004284\n",
      "\t\tEpoch 55\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.010214224457740784 Best: 0.009705173783004284\n",
      "\t\tEpoch 56\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013945712707936764 Best: 0.009705173783004284\n",
      "\t\tEpoch 57\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012266192585229874 Best: 0.009705173783004284\n",
      "\t\tEpoch 58\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.010875931940972805 Best: 0.009705173783004284\n",
      "\t\tEpoch 59\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.000390625\n",
      "Validation loss: 0.011893093585968018 Best: 0.009705173783004284\n",
      "\t\tEpoch 60\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012150845490396023 Best: 0.009705173783004284\n",
      "\t\tEpoch 61\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012070389464497566 Best: 0.009705173783004284\n",
      "\t\tEpoch 62\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.010655347257852554 Best: 0.009705173783004284\n",
      "\t\tEpoch 63\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012478485703468323 Best: 0.009705173783004284\n",
      "\t\tEpoch 64\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.010508714243769646 Best: 0.009705173783004284\n",
      "\t\tEpoch 65\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0001953125\n",
      "Validation loss: 0.01377960667014122 Best: 0.009705173783004284\n",
      "\t\tEpoch 66\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012994645163416862 Best: 0.009705173783004284\n",
      "\t\tEpoch 67\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013292058371007442 Best: 0.009705173783004284\n",
      "\t\tEpoch 68\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015116815455257893 Best: 0.009705173783004284\n",
      "\t\tEpoch 69\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01305754017084837 Best: 0.009705173783004284\n",
      "\t\tEpoch 70\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011979194357991219 Best: 0.009705173783004284\n",
      "\t\tEpoch 71\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 9.765625e-05\n",
      "Validation loss: 0.012879469431936741 Best: 0.009705173783004284\n",
      "\t\tEpoch 72\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013363009318709373 Best: 0.009705173783004284\n",
      "\t\tEpoch 73\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014326391741633415 Best: 0.009705173783004284\n",
      "\t\tEpoch 74\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012912978418171406 Best: 0.009705173783004284\n",
      "\t\tEpoch 75\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01419502031058073 Best: 0.009705173783004284\n",
      "\t\tEpoch 76\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.016407616436481476 Best: 0.009705173783004284\n",
      "\t\tEpoch 77\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 4.8828125e-05\n",
      "Validation loss: 0.014166941866278648 Best: 0.009705173783004284\n",
      "\t\tEpoch 78\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013881217688322067 Best: 0.009705173783004284\n",
      "\t\tEpoch 79\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015562196262180805 Best: 0.009705173783004284\n",
      "\t\tEpoch 80\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014590741135179996 Best: 0.009705173783004284\n",
      "\t\tEpoch 81\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013738216832280159 Best: 0.009705173783004284\n",
      "\t\tEpoch 82\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01403257716447115 Best: 0.009705173783004284\n",
      "\t\tEpoch 83\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 2.44140625e-05\n",
      "Validation loss: 0.012817810289561749 Best: 0.009705173783004284\n",
      "\t\tEpoch 84\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014938137494027615 Best: 0.009705173783004284\n",
      "\t\tEpoch 85\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013526814989745617 Best: 0.009705173783004284\n",
      "\t\tEpoch 86\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01140152383595705 Best: 0.009705173783004284\n",
      "\t\tEpoch 87\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013274922035634518 Best: 0.009705173783004284\n",
      "\t\tEpoch 88\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015658864751458168 Best: 0.009705173783004284\n",
      "\t\tEpoch 89\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 1.220703125e-05\n",
      "Validation loss: 0.01390023436397314 Best: 0.009705173783004284\n",
      "\t\tEpoch 90\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015307491645216942 Best: 0.009705173783004284\n",
      "\t\tEpoch 91\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015260678716003895 Best: 0.009705173783004284\n",
      "\t\tEpoch 92\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012655426748096943 Best: 0.009705173783004284\n",
      "\t\tEpoch 93\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01339760608971119 Best: 0.009705173783004284\n",
      "\t\tEpoch 94\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014041765592992306 Best: 0.009705173783004284\n",
      "\t\tEpoch 95\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 6.103515625e-06\n",
      "Validation loss: 0.012517248280346394 Best: 0.009705173783004284\n",
      "\t\tEpoch 96\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013768489472568035 Best: 0.009705173783004284\n",
      "\t\tEpoch 97\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01227598823606968 Best: 0.009705173783004284\n",
      "\t\tEpoch 98\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012951898388564587 Best: 0.009705173783004284\n",
      "\t\tEpoch 99\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014137204736471176 Best: 0.009705173783004284\n",
      "\t\tEpoch 100\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012666644528508186 Best: 0.009705173783004284\n",
      "0.10953782830545236\n",
      "\t\t\tQuantile Network\n",
      "Use RandomForestRegressor\n",
      "ymax and min: tensor(2.4341) tensor(-2.9401)\n",
      "Using marginal loss\n",
      "\t\tEpoch 1\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 1\n",
      "Validation loss: 0.020355040207505226 Best: 0.020355040207505226\n",
      "\t\tEpoch 2\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.020832912996411324 Best: 0.020355040207505226\n",
      "\t\tEpoch 3\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 3\n",
      "Validation loss: 0.019896799698472023 Best: 0.019896799698472023\n",
      "\t\tEpoch 4\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 4\n",
      "Validation loss: 0.019546357914805412 Best: 0.019546357914805412\n",
      "\t\tEpoch 5\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 5\n",
      "Validation loss: 0.016752514988183975 Best: 0.016752514988183975\n",
      "\t\tEpoch 6\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01708609238266945 Best: 0.016752514988183975\n",
      "\t\tEpoch 7\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 7\n",
      "Validation loss: 0.014441106468439102 Best: 0.014441106468439102\n",
      "\t\tEpoch 8\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.017944904044270515 Best: 0.014441106468439102\n",
      "\t\tEpoch 9\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01763143204152584 Best: 0.014441106468439102\n",
      "\t\tEpoch 10\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01761770248413086 Best: 0.014441106468439102\n",
      "\t\tEpoch 11\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019833585247397423 Best: 0.014441106468439102\n",
      "\t\tEpoch 12\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.05\n",
      "Validation loss: 0.02309447154402733 Best: 0.014441106468439102\n",
      "\t\tEpoch 13\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.02147388458251953 Best: 0.014441106468439102\n",
      "\t\tEpoch 14\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.02022206038236618 Best: 0.014441106468439102\n",
      "\t\tEpoch 15\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.020334118977189064 Best: 0.014441106468439102\n",
      "\t\tEpoch 16\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01927466318011284 Best: 0.014441106468439102\n",
      "\t\tEpoch 17\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019339827820658684 Best: 0.014441106468439102\n",
      "\t\tEpoch 18\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.025\n",
      "Validation loss: 0.017168812453746796 Best: 0.014441106468439102\n",
      "\t\tEpoch 19\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.018912892788648605 Best: 0.014441106468439102\n",
      "\t\tEpoch 20\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019183943048119545 Best: 0.014441106468439102\n",
      "\t\tEpoch 21\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.018449675291776657 Best: 0.014441106468439102\n",
      "\t\tEpoch 22\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.021343516185879707 Best: 0.014441106468439102\n",
      "\t\tEpoch 23\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.02322218380868435 Best: 0.014441106468439102\n",
      "\t\tEpoch 24\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0125\n",
      "Validation loss: 0.019298264756798744 Best: 0.014441106468439102\n",
      "\t\tEpoch 25\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.017407719045877457 Best: 0.014441106468439102\n",
      "\t\tEpoch 26\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.020397763699293137 Best: 0.014441106468439102\n",
      "\t\tEpoch 27\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.020466148853302002 Best: 0.014441106468439102\n",
      "\t\tEpoch 28\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.020244624465703964 Best: 0.014441106468439102\n",
      "\t\tEpoch 29\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01932278461754322 Best: 0.014441106468439102\n",
      "\t\tEpoch 30\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00625\n",
      "Validation loss: 0.020801544189453125 Best: 0.014441106468439102\n",
      "\t\tEpoch 31\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.02033260650932789 Best: 0.014441106468439102\n",
      "\t\tEpoch 32\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.02067198045551777 Best: 0.014441106468439102\n",
      "\t\tEpoch 33\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.02058088220655918 Best: 0.014441106468439102\n",
      "\t\tEpoch 34\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.021144628524780273 Best: 0.014441106468439102\n",
      "\t\tEpoch 35\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01941596530377865 Best: 0.014441106468439102\n",
      "\t\tEpoch 36\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.003125\n",
      "Validation loss: 0.018661996349692345 Best: 0.014441106468439102\n",
      "\t\tEpoch 37\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019408510997891426 Best: 0.014441106468439102\n",
      "\t\tEpoch 38\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.017807818949222565 Best: 0.014441106468439102\n",
      "\t\tEpoch 39\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019649412482976913 Best: 0.014441106468439102\n",
      "\t\tEpoch 40\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.02053472213447094 Best: 0.014441106468439102\n",
      "\t\tEpoch 41\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.020246708765625954 Best: 0.014441106468439102\n",
      "\t\tEpoch 42\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0015625\n",
      "Validation loss: 0.018790654838085175 Best: 0.014441106468439102\n",
      "\t\tEpoch 43\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01910606026649475 Best: 0.014441106468439102\n",
      "\t\tEpoch 44\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.017498083412647247 Best: 0.014441106468439102\n",
      "\t\tEpoch 45\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.018963264301419258 Best: 0.014441106468439102\n",
      "\t\tEpoch 46\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.020345846191048622 Best: 0.014441106468439102\n",
      "\t\tEpoch 47\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.020974019542336464 Best: 0.014441106468439102\n",
      "\t\tEpoch 48\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00078125\n",
      "Validation loss: 0.01802428811788559 Best: 0.014441106468439102\n",
      "\t\tEpoch 49\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.018378576263785362 Best: 0.014441106468439102\n",
      "\t\tEpoch 50\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.02028420753777027 Best: 0.014441106468439102\n",
      "\t\tEpoch 51\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.02008163183927536 Best: 0.014441106468439102\n",
      "\t\tEpoch 52\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019524866715073586 Best: 0.014441106468439102\n",
      "\t\tEpoch 53\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01963319629430771 Best: 0.014441106468439102\n",
      "\t\tEpoch 54\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.000390625\n",
      "Validation loss: 0.017625117674469948 Best: 0.014441106468439102\n",
      "\t\tEpoch 55\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01927567832171917 Best: 0.014441106468439102\n",
      "\t\tEpoch 56\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019204579293727875 Best: 0.014441106468439102\n",
      "\t\tEpoch 57\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.018786661326885223 Best: 0.014441106468439102\n",
      "\t\tEpoch 58\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.018056102097034454 Best: 0.014441106468439102\n",
      "\t\tEpoch 59\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.021042775362730026 Best: 0.014441106468439102\n",
      "\t\tEpoch 60\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0001953125\n",
      "Validation loss: 0.02028338424861431 Best: 0.014441106468439102\n",
      "\t\tEpoch 61\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.02018251083791256 Best: 0.014441106468439102\n",
      "\t\tEpoch 62\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.021246688440442085 Best: 0.014441106468439102\n",
      "\t\tEpoch 63\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.022285478189587593 Best: 0.014441106468439102\n",
      "\t\tEpoch 64\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.020616965368390083 Best: 0.014441106468439102\n",
      "\t\tEpoch 65\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.021707074716687202 Best: 0.014441106468439102\n",
      "\t\tEpoch 66\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 9.765625e-05\n",
      "Validation loss: 0.022825462743639946 Best: 0.014441106468439102\n",
      "\t\tEpoch 67\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0224644523113966 Best: 0.014441106468439102\n",
      "\t\tEpoch 68\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.02339976094663143 Best: 0.014441106468439102\n",
      "\t\tEpoch 69\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.020115027204155922 Best: 0.014441106468439102\n",
      "\t\tEpoch 70\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019708072766661644 Best: 0.014441106468439102\n",
      "\t\tEpoch 71\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.020540393888950348 Best: 0.014441106468439102\n",
      "\t\tEpoch 72\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 4.8828125e-05\n",
      "Validation loss: 0.019801119342446327 Best: 0.014441106468439102\n",
      "\t\tEpoch 73\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.021718120202422142 Best: 0.014441106468439102\n",
      "\t\tEpoch 74\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.017691008746623993 Best: 0.014441106468439102\n",
      "\t\tEpoch 75\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.018631020560860634 Best: 0.014441106468439102\n",
      "\t\tEpoch 76\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.02020343951880932 Best: 0.014441106468439102\n",
      "\t\tEpoch 77\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.02024366520345211 Best: 0.014441106468439102\n",
      "\t\tEpoch 78\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 2.44140625e-05\n",
      "Validation loss: 0.020056983456015587 Best: 0.014441106468439102\n",
      "\t\tEpoch 79\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.018952829763293266 Best: 0.014441106468439102\n",
      "\t\tEpoch 80\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0194063950330019 Best: 0.014441106468439102\n",
      "\t\tEpoch 81\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.020053459331393242 Best: 0.014441106468439102\n",
      "\t\tEpoch 82\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019386526197195053 Best: 0.014441106468439102\n",
      "\t\tEpoch 83\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.020983697846531868 Best: 0.014441106468439102\n",
      "\t\tEpoch 84\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 1.220703125e-05\n",
      "Validation loss: 0.019265731796622276 Best: 0.014441106468439102\n",
      "\t\tEpoch 85\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019685031846165657 Best: 0.014441106468439102\n",
      "\t\tEpoch 86\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01976625993847847 Best: 0.014441106468439102\n",
      "\t\tEpoch 87\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01966886967420578 Best: 0.014441106468439102\n",
      "\t\tEpoch 88\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019461344927549362 Best: 0.014441106468439102\n",
      "\t\tEpoch 89\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019748708233237267 Best: 0.014441106468439102\n",
      "\t\tEpoch 90\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 6.103515625e-06\n",
      "Validation loss: 0.021593134850263596 Best: 0.014441106468439102\n",
      "\t\tEpoch 91\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.020127426832914352 Best: 0.014441106468439102\n",
      "\t\tEpoch 92\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.018755266442894936 Best: 0.014441106468439102\n",
      "\t\tEpoch 93\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.018427670001983643 Best: 0.014441106468439102\n",
      "\t\tEpoch 94\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.021494291722774506 Best: 0.014441106468439102\n",
      "\t\tEpoch 95\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.021929442882537842 Best: 0.014441106468439102\n",
      "\t\tEpoch 96\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 3.0517578125e-06\n",
      "Validation loss: 0.021537037566304207 Best: 0.014441106468439102\n",
      "\t\tEpoch 97\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.022394010797142982 Best: 0.014441106468439102\n",
      "\t\tEpoch 98\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.02019048109650612 Best: 0.014441106468439102\n",
      "\t\tEpoch 99\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.018666645511984825 Best: 0.014441106468439102\n",
      "\t\tEpoch 100\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019739361479878426 Best: 0.014441106468439102\n",
      "0.10868971558008113\n",
      "\t\t\tQuantile Network\n",
      "Use RandomForestRegressor\n",
      "ymax and min: tensor(2.4466) tensor(-2.9921)\n",
      "Using marginal loss\n",
      "\t\tEpoch 1\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 1\n",
      "Validation loss: 0.01967797242105007 Best: 0.01967797242105007\n",
      "\t\tEpoch 2\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 2\n",
      "Validation loss: 0.019208034500479698 Best: 0.019208034500479698\n",
      "\t\tEpoch 3\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 3\n",
      "Validation loss: 0.018864426761865616 Best: 0.018864426761865616\n",
      "\t\tEpoch 4\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 4\n",
      "Validation loss: 0.016916798427700996 Best: 0.016916798427700996\n",
      "\t\tEpoch 5\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 5\n",
      "Validation loss: 0.01628061942756176 Best: 0.01628061942756176\n",
      "\t\tEpoch 6\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 6\n",
      "Validation loss: 0.015328856185078621 Best: 0.015328856185078621\n",
      "\t\tEpoch 7\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 7\n",
      "Validation loss: 0.015195546671748161 Best: 0.015195546671748161\n",
      "\t\tEpoch 8\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 8\n",
      "Validation loss: 0.01487889513373375 Best: 0.01487889513373375\n",
      "\t\tEpoch 9\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015059988014400005 Best: 0.01487889513373375\n",
      "\t\tEpoch 10\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015291476622223854 Best: 0.01487889513373375\n",
      "\t\tEpoch 11\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 11\n",
      "Validation loss: 0.012093166820704937 Best: 0.012093166820704937\n",
      "\t\tEpoch 12\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 12\n",
      "Validation loss: 0.011127456091344357 Best: 0.011127456091344357\n",
      "\t\tEpoch 13\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01202491857111454 Best: 0.011127456091344357\n",
      "\t\tEpoch 14\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013203711248934269 Best: 0.011127456091344357\n",
      "\t\tEpoch 15\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011666025966405869 Best: 0.011127456091344357\n",
      "\t\tEpoch 16\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011496486142277718 Best: 0.011127456091344357\n",
      "\t\tEpoch 17\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.05\n",
      "Validation loss: 0.012640816159546375 Best: 0.011127456091344357\n",
      "\t\tEpoch 18\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011863539926707745 Best: 0.011127456091344357\n",
      "\t\tEpoch 19\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015005023218691349 Best: 0.011127456091344357\n",
      "\t\tEpoch 20\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012902836315333843 Best: 0.011127456091344357\n",
      "\t\tEpoch 21\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014199431985616684 Best: 0.011127456091344357\n",
      "\t\tEpoch 22\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012208913452923298 Best: 0.011127456091344357\n",
      "\t\tEpoch 23\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.025\n",
      "Validation loss: 0.012435462325811386 Best: 0.011127456091344357\n",
      "\t\tEpoch 24\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014038536697626114 Best: 0.011127456091344357\n",
      "\t\tEpoch 25\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012759080156683922 Best: 0.011127456091344357\n",
      "\t\tEpoch 26\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01291295699775219 Best: 0.011127456091344357\n",
      "\t\tEpoch 27\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 27\n",
      "Validation loss: 0.010777506977319717 Best: 0.010777506977319717\n",
      "\t\tEpoch 28\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011609160341322422 Best: 0.010777506977319717\n",
      "\t\tEpoch 29\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 29\n",
      "Validation loss: 0.010566949844360352 Best: 0.010566949844360352\n",
      "\t\tEpoch 30\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012551317922770977 Best: 0.010566949844360352\n",
      "\t\tEpoch 31\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0125\n",
      "Validation loss: 0.01210346631705761 Best: 0.010566949844360352\n",
      "\t\tEpoch 32\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013460393995046616 Best: 0.010566949844360352\n",
      "\t\tEpoch 33\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013399283401668072 Best: 0.010566949844360352\n",
      "\t\tEpoch 34\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013027023524045944 Best: 0.010566949844360352\n",
      "\t\tEpoch 35\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012059478089213371 Best: 0.010566949844360352\n",
      "\t\tEpoch 36\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012794075533747673 Best: 0.010566949844360352\n",
      "\t\tEpoch 37\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00625\n",
      "Validation loss: 0.01134778093546629 Best: 0.010566949844360352\n",
      "\t\tEpoch 38\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01227408368140459 Best: 0.010566949844360352\n",
      "\t\tEpoch 39\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013090578839182854 Best: 0.010566949844360352\n",
      "\t\tEpoch 40\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012835294008255005 Best: 0.010566949844360352\n",
      "\t\tEpoch 41\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013677584938704967 Best: 0.010566949844360352\n",
      "\t\tEpoch 42\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013167307712137699 Best: 0.010566949844360352\n",
      "\t\tEpoch 43\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.003125\n",
      "Validation loss: 0.012556894682347775 Best: 0.010566949844360352\n",
      "\t\tEpoch 44\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012846170924603939 Best: 0.010566949844360352\n",
      "\t\tEpoch 45\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011719312518835068 Best: 0.010566949844360352\n",
      "\t\tEpoch 46\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013179673813283443 Best: 0.010566949844360352\n",
      "\t\tEpoch 47\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013062205165624619 Best: 0.010566949844360352\n",
      "\t\tEpoch 48\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011663451790809631 Best: 0.010566949844360352\n",
      "\t\tEpoch 49\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0015625\n",
      "Validation loss: 0.013303639367222786 Best: 0.010566949844360352\n",
      "\t\tEpoch 50\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012680882588028908 Best: 0.010566949844360352\n",
      "\t\tEpoch 51\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013094931840896606 Best: 0.010566949844360352\n",
      "\t\tEpoch 52\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012769094668328762 Best: 0.010566949844360352\n",
      "\t\tEpoch 53\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013491249643266201 Best: 0.010566949844360352\n",
      "\t\tEpoch 54\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012814769521355629 Best: 0.010566949844360352\n",
      "\t\tEpoch 55\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00078125\n",
      "Validation loss: 0.01091858558356762 Best: 0.010566949844360352\n",
      "\t\tEpoch 56\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01219856645911932 Best: 0.010566949844360352\n",
      "\t\tEpoch 57\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013383939862251282 Best: 0.010566949844360352\n",
      "\t\tEpoch 58\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014203538186848164 Best: 0.010566949844360352\n",
      "\t\tEpoch 59\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014914603903889656 Best: 0.010566949844360352\n",
      "\t\tEpoch 60\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013347819447517395 Best: 0.010566949844360352\n",
      "\t\tEpoch 61\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.000390625\n",
      "Validation loss: 0.01386943832039833 Best: 0.010566949844360352\n",
      "\t\tEpoch 62\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014139728620648384 Best: 0.010566949844360352\n",
      "\t\tEpoch 63\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012839152477681637 Best: 0.010566949844360352\n",
      "\t\tEpoch 64\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011557081714272499 Best: 0.010566949844360352\n",
      "\t\tEpoch 65\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013633821159601212 Best: 0.010566949844360352\n",
      "\t\tEpoch 66\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012755321338772774 Best: 0.010566949844360352\n",
      "\t\tEpoch 67\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0001953125\n",
      "Validation loss: 0.012111809104681015 Best: 0.010566949844360352\n",
      "\t\tEpoch 68\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012976511381566525 Best: 0.010566949844360352\n",
      "\t\tEpoch 69\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013360464945435524 Best: 0.010566949844360352\n",
      "\t\tEpoch 70\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012302939780056477 Best: 0.010566949844360352\n",
      "\t\tEpoch 71\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012428807094693184 Best: 0.010566949844360352\n",
      "\t\tEpoch 72\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01192319393157959 Best: 0.010566949844360352\n",
      "\t\tEpoch 73\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 9.765625e-05\n",
      "Validation loss: 0.013100767508149147 Best: 0.010566949844360352\n",
      "\t\tEpoch 74\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011584571562707424 Best: 0.010566949844360352\n",
      "\t\tEpoch 75\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014817031100392342 Best: 0.010566949844360352\n",
      "\t\tEpoch 76\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011669264174997807 Best: 0.010566949844360352\n",
      "\t\tEpoch 77\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012974029406905174 Best: 0.010566949844360352\n",
      "\t\tEpoch 78\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012106547132134438 Best: 0.010566949844360352\n",
      "\t\tEpoch 79\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 4.8828125e-05\n",
      "Validation loss: 0.012634768150746822 Best: 0.010566949844360352\n",
      "\t\tEpoch 80\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014176789671182632 Best: 0.010566949844360352\n",
      "\t\tEpoch 81\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012024967931210995 Best: 0.010566949844360352\n",
      "\t\tEpoch 82\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012249414809048176 Best: 0.010566949844360352\n",
      "\t\tEpoch 83\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014564484357833862 Best: 0.010566949844360352\n",
      "\t\tEpoch 84\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013113700784742832 Best: 0.010566949844360352\n",
      "\t\tEpoch 85\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 2.44140625e-05\n",
      "Validation loss: 0.013474341481924057 Best: 0.010566949844360352\n",
      "\t\tEpoch 86\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013557743281126022 Best: 0.010566949844360352\n",
      "\t\tEpoch 87\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012955615296959877 Best: 0.010566949844360352\n",
      "\t\tEpoch 88\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013111650943756104 Best: 0.010566949844360352\n",
      "\t\tEpoch 89\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0134459612891078 Best: 0.010566949844360352\n",
      "\t\tEpoch 90\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0131826875731349 Best: 0.010566949844360352\n",
      "\t\tEpoch 91\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 1.220703125e-05\n",
      "Validation loss: 0.01350068673491478 Best: 0.010566949844360352\n",
      "\t\tEpoch 92\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013494784943759441 Best: 0.010566949844360352\n",
      "\t\tEpoch 93\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013246498070657253 Best: 0.010566949844360352\n",
      "\t\tEpoch 94\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013306722976267338 Best: 0.010566949844360352\n",
      "\t\tEpoch 95\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013061930425465107 Best: 0.010566949844360352\n",
      "\t\tEpoch 96\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012943129055202007 Best: 0.010566949844360352\n",
      "\t\tEpoch 97\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 6.103515625e-06\n",
      "Validation loss: 0.012754248455166817 Best: 0.010566949844360352\n",
      "\t\tEpoch 98\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013305572792887688 Best: 0.010566949844360352\n",
      "\t\tEpoch 99\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01307420339435339 Best: 0.010566949844360352\n",
      "\t\tEpoch 100\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014519196934998035 Best: 0.010566949844360352\n",
      "0.13104026108035302\n",
      "\t\t\tQuantile Network\n",
      "Use RandomForestRegressor\n",
      "ymax and min: tensor(2.4955) tensor(-3.0722)\n",
      "Using marginal loss\n",
      "\t\tEpoch 1\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 1\n",
      "Validation loss: 0.017883582040667534 Best: 0.017883582040667534\n",
      "\t\tEpoch 2\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 2\n",
      "Validation loss: 0.01708770915865898 Best: 0.01708770915865898\n",
      "\t\tEpoch 3\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 3\n",
      "Validation loss: 0.016773903742432594 Best: 0.016773903742432594\n",
      "\t\tEpoch 4\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 4\n",
      "Validation loss: 0.014698809012770653 Best: 0.014698809012770653\n",
      "\t\tEpoch 5\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01501219067722559 Best: 0.014698809012770653\n",
      "\t\tEpoch 6\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 6\n",
      "Validation loss: 0.014226539060473442 Best: 0.014226539060473442\n",
      "\t\tEpoch 7\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 7\n",
      "Validation loss: 0.014065386727452278 Best: 0.014065386727452278\n",
      "\t\tEpoch 8\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014646533876657486 Best: 0.014065386727452278\n",
      "\t\tEpoch 9\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014677249826490879 Best: 0.014065386727452278\n",
      "\t\tEpoch 10\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01488004345446825 Best: 0.014065386727452278\n",
      "\t\tEpoch 11\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 11\n",
      "Validation loss: 0.013552505522966385 Best: 0.013552505522966385\n",
      "\t\tEpoch 12\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014902383089065552 Best: 0.013552505522966385\n",
      "\t\tEpoch 13\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 13\n",
      "Validation loss: 0.013440249487757683 Best: 0.013440249487757683\n",
      "\t\tEpoch 14\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014101698063313961 Best: 0.013440249487757683\n",
      "\t\tEpoch 15\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.05\n",
      "Validation loss: 0.016536159440875053 Best: 0.013440249487757683\n",
      "\t\tEpoch 16\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 16\n",
      "Validation loss: 0.012804146856069565 Best: 0.012804146856069565\n",
      "\t\tEpoch 17\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01568826287984848 Best: 0.012804146856069565\n",
      "\t\tEpoch 18\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0160236693918705 Best: 0.012804146856069565\n",
      "\t\tEpoch 19\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01523964386433363 Best: 0.012804146856069565\n",
      "\t\tEpoch 20\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014501389116048813 Best: 0.012804146856069565\n",
      "\t\tEpoch 21\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 21\n",
      "Validation loss: 0.012389486655592918 Best: 0.012389486655592918\n",
      "\t\tEpoch 22\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014147648587822914 Best: 0.012389486655592918\n",
      "\t\tEpoch 23\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.025\n",
      "Validation loss: 0.014326249249279499 Best: 0.012389486655592918\n",
      "\t\tEpoch 24\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014716573990881443 Best: 0.012389486655592918\n",
      "\t\tEpoch 25\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01281713880598545 Best: 0.012389486655592918\n",
      "\t\tEpoch 26\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014466415159404278 Best: 0.012389486655592918\n",
      "\t\tEpoch 27\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 27\n",
      "Validation loss: 0.012179827317595482 Best: 0.012179827317595482\n",
      "\t\tEpoch 28\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012899225577712059 Best: 0.012179827317595482\n",
      "\t\tEpoch 29\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013969252817332745 Best: 0.012179827317595482\n",
      "\t\tEpoch 30\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0125\n",
      "Validation loss: 0.013378224335610867 Best: 0.012179827317595482\n",
      "\t\tEpoch 31\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015199556015431881 Best: 0.012179827317595482\n",
      "\t\tEpoch 32\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012956980615854263 Best: 0.012179827317595482\n",
      "\t\tEpoch 33\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01343031320720911 Best: 0.012179827317595482\n",
      "\t\tEpoch 34\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01364175509661436 Best: 0.012179827317595482\n",
      "\t\tEpoch 35\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014112343080341816 Best: 0.012179827317595482\n",
      "\t\tEpoch 36\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00625\n",
      "Validation loss: 0.0130379693582654 Best: 0.012179827317595482\n",
      "\t\tEpoch 37\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012883774004876614 Best: 0.012179827317595482\n",
      "\t\tEpoch 38\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014616508968174458 Best: 0.012179827317595482\n",
      "\t\tEpoch 39\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014652547426521778 Best: 0.012179827317595482\n",
      "\t\tEpoch 40\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01434363704174757 Best: 0.012179827317595482\n",
      "\t\tEpoch 41\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013449299149215221 Best: 0.012179827317595482\n",
      "\t\tEpoch 42\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.003125\n",
      "Validation loss: 0.01385266799479723 Best: 0.012179827317595482\n",
      "\t\tEpoch 43\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015690164640545845 Best: 0.012179827317595482\n",
      "\t\tEpoch 44\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013598566874861717 Best: 0.012179827317595482\n",
      "\t\tEpoch 45\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013905650936067104 Best: 0.012179827317595482\n",
      "\t\tEpoch 46\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01532407384365797 Best: 0.012179827317595482\n",
      "\t\tEpoch 47\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01320941187441349 Best: 0.012179827317595482\n",
      "\t\tEpoch 48\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0015625\n",
      "Validation loss: 0.013288459740579128 Best: 0.012179827317595482\n",
      "\t\tEpoch 49\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014562889002263546 Best: 0.012179827317595482\n",
      "\t\tEpoch 50\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013077867217361927 Best: 0.012179827317595482\n",
      "\t\tEpoch 51\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012715297751128674 Best: 0.012179827317595482\n",
      "\t\tEpoch 52\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013671203516423702 Best: 0.012179827317595482\n",
      "\t\tEpoch 53\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012554177083075047 Best: 0.012179827317595482\n",
      "\t\tEpoch 54\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00078125\n",
      "Validation loss: 0.014081066474318504 Best: 0.012179827317595482\n",
      "\t\tEpoch 55\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.016171319410204887 Best: 0.012179827317595482\n",
      "\t\tEpoch 56\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013590282760560513 Best: 0.012179827317595482\n",
      "\t\tEpoch 57\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01329833921045065 Best: 0.012179827317595482\n",
      "\t\tEpoch 58\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01485575269907713 Best: 0.012179827317595482\n",
      "\t\tEpoch 59\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013257193379104137 Best: 0.012179827317595482\n",
      "\t\tEpoch 60\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.000390625\n",
      "Validation loss: 0.014701085165143013 Best: 0.012179827317595482\n",
      "\t\tEpoch 61\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014283688738942146 Best: 0.012179827317595482\n",
      "\t\tEpoch 62\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013932919129729271 Best: 0.012179827317595482\n",
      "\t\tEpoch 63\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014493483118712902 Best: 0.012179827317595482\n",
      "\t\tEpoch 64\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013913419097661972 Best: 0.012179827317595482\n",
      "\t\tEpoch 65\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013825923204421997 Best: 0.012179827317595482\n",
      "\t\tEpoch 66\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0001953125\n",
      "Validation loss: 0.014766847714781761 Best: 0.012179827317595482\n",
      "\t\tEpoch 67\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014337214641273022 Best: 0.012179827317595482\n",
      "\t\tEpoch 68\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015330972149968147 Best: 0.012179827317595482\n",
      "\t\tEpoch 69\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014449242502450943 Best: 0.012179827317595482\n",
      "\t\tEpoch 70\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014815486036241055 Best: 0.012179827317595482\n",
      "\t\tEpoch 71\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013328502885997295 Best: 0.012179827317595482\n",
      "\t\tEpoch 72\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 9.765625e-05\n",
      "Validation loss: 0.014314953237771988 Best: 0.012179827317595482\n",
      "\t\tEpoch 73\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01493063010275364 Best: 0.012179827317595482\n",
      "\t\tEpoch 74\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01323124673217535 Best: 0.012179827317595482\n",
      "\t\tEpoch 75\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014057489112019539 Best: 0.012179827317595482\n",
      "\t\tEpoch 76\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013896907679736614 Best: 0.012179827317595482\n",
      "\t\tEpoch 77\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01549440622329712 Best: 0.012179827317595482\n",
      "\t\tEpoch 78\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 4.8828125e-05\n",
      "Validation loss: 0.016108229756355286 Best: 0.012179827317595482\n",
      "\t\tEpoch 79\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015025087632238865 Best: 0.012179827317595482\n",
      "\t\tEpoch 80\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014939953573048115 Best: 0.012179827317595482\n",
      "\t\tEpoch 81\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013921728357672691 Best: 0.012179827317595482\n",
      "\t\tEpoch 82\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013823830522596836 Best: 0.012179827317595482\n",
      "\t\tEpoch 83\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013179248198866844 Best: 0.012179827317595482\n",
      "\t\tEpoch 84\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 2.44140625e-05\n",
      "Validation loss: 0.014507305808365345 Best: 0.012179827317595482\n",
      "\t\tEpoch 85\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015415676869452 Best: 0.012179827317595482\n",
      "\t\tEpoch 86\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015280220657587051 Best: 0.012179827317595482\n",
      "\t\tEpoch 87\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014165165834128857 Best: 0.012179827317595482\n",
      "\t\tEpoch 88\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014595387503504753 Best: 0.012179827317595482\n",
      "\t\tEpoch 89\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01502986904233694 Best: 0.012179827317595482\n",
      "\t\tEpoch 90\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 1.220703125e-05\n",
      "Validation loss: 0.014685430563986301 Best: 0.012179827317595482\n",
      "\t\tEpoch 91\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013451735489070415 Best: 0.012179827317595482\n",
      "\t\tEpoch 92\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014953400939702988 Best: 0.012179827317595482\n",
      "\t\tEpoch 93\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014178219251334667 Best: 0.012179827317595482\n",
      "\t\tEpoch 94\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015814390033483505 Best: 0.012179827317595482\n",
      "\t\tEpoch 95\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014788834378123283 Best: 0.012179827317595482\n",
      "\t\tEpoch 96\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 6.103515625e-06\n",
      "Validation loss: 0.016282154247164726 Best: 0.012179827317595482\n",
      "\t\tEpoch 97\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013930056244134903 Best: 0.012179827317595482\n",
      "\t\tEpoch 98\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014419921673834324 Best: 0.012179827317595482\n",
      "\t\tEpoch 99\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014908533543348312 Best: 0.012179827317595482\n",
      "\t\tEpoch 100\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014352185651659966 Best: 0.012179827317595482\n",
      "0.08591729774994092\n",
      "\t\t\tQuantile Network\n",
      "Use RandomForestRegressor\n",
      "ymax and min: tensor(2.5393) tensor(-3.0851)\n",
      "Using marginal loss\n",
      "\t\tEpoch 1\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 1\n",
      "Validation loss: 0.018450964242219925 Best: 0.018450964242219925\n",
      "\t\tEpoch 2\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01949540711939335 Best: 0.018450964242219925\n",
      "\t\tEpoch 3\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019572271034121513 Best: 0.018450964242219925\n",
      "\t\tEpoch 4\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019459549337625504 Best: 0.018450964242219925\n",
      "\t\tEpoch 5\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01910260133445263 Best: 0.018450964242219925\n",
      "\t\tEpoch 6\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.018859481438994408 Best: 0.018450964242219925\n",
      "\t\tEpoch 7\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019181234762072563 Best: 0.018450964242219925\n",
      "\t\tEpoch 8\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.05\n",
      "Validation loss: 0.02104371413588524 Best: 0.018450964242219925\n",
      "\t\tEpoch 9\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 9\n",
      "Validation loss: 0.018320823088288307 Best: 0.018320823088288307\n",
      "\t\tEpoch 10\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01851526089012623 Best: 0.018320823088288307\n",
      "\t\tEpoch 11\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.020271336659789085 Best: 0.018320823088288307\n",
      "\t\tEpoch 12\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019877206534147263 Best: 0.018320823088288307\n",
      "\t\tEpoch 13\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019592123106122017 Best: 0.018320823088288307\n",
      "\t\tEpoch 14\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019230766221880913 Best: 0.018320823088288307\n",
      "\t\tEpoch 15\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.025\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 15\n",
      "Validation loss: 0.01715206541121006 Best: 0.01715206541121006\n",
      "\t\tEpoch 16\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.017627382650971413 Best: 0.01715206541121006\n",
      "\t\tEpoch 17\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019612399861216545 Best: 0.01715206541121006\n",
      "\t\tEpoch 18\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.018939323723316193 Best: 0.01715206541121006\n",
      "\t\tEpoch 19\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.02081366814672947 Best: 0.01715206541121006\n",
      "\t\tEpoch 20\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01808069832623005 Best: 0.01715206541121006\n",
      "\t\tEpoch 21\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019202448427677155 Best: 0.01715206541121006\n",
      "\t\tEpoch 22\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0125\n",
      "Validation loss: 0.019698044285178185 Best: 0.01715206541121006\n",
      "\t\tEpoch 23\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01756361499428749 Best: 0.01715206541121006\n",
      "\t\tEpoch 24\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019168075174093246 Best: 0.01715206541121006\n",
      "\t\tEpoch 25\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.02031930722296238 Best: 0.01715206541121006\n",
      "\t\tEpoch 26\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.020965751260519028 Best: 0.01715206541121006\n",
      "\t\tEpoch 27\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019826583564281464 Best: 0.01715206541121006\n",
      "\t\tEpoch 28\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00625\n",
      "Validation loss: 0.019098954275250435 Best: 0.01715206541121006\n",
      "\t\tEpoch 29\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.018777748569846153 Best: 0.01715206541121006\n",
      "\t\tEpoch 30\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.021206935867667198 Best: 0.01715206541121006\n",
      "\t\tEpoch 31\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.021273262798786163 Best: 0.01715206541121006\n",
      "\t\tEpoch 32\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019068947061896324 Best: 0.01715206541121006\n",
      "\t\tEpoch 33\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0193394236266613 Best: 0.01715206541121006\n",
      "\t\tEpoch 34\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.003125\n",
      "Validation loss: 0.01966562308371067 Best: 0.01715206541121006\n",
      "\t\tEpoch 35\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.022408556193113327 Best: 0.01715206541121006\n",
      "\t\tEpoch 36\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.020400388166308403 Best: 0.01715206541121006\n",
      "\t\tEpoch 37\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.02091209962964058 Best: 0.01715206541121006\n",
      "\t\tEpoch 38\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01897890493273735 Best: 0.01715206541121006\n",
      "\t\tEpoch 39\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0215867031365633 Best: 0.01715206541121006\n",
      "\t\tEpoch 40\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0015625\n",
      "Validation loss: 0.022579802200198174 Best: 0.01715206541121006\n",
      "\t\tEpoch 41\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.021542998030781746 Best: 0.01715206541121006\n",
      "\t\tEpoch 42\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.022288328036665916 Best: 0.01715206541121006\n",
      "\t\tEpoch 43\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0201968252658844 Best: 0.01715206541121006\n",
      "\t\tEpoch 44\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.017885658890008926 Best: 0.01715206541121006\n",
      "\t\tEpoch 45\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.018113715574145317 Best: 0.01715206541121006\n",
      "\t\tEpoch 46\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00078125\n",
      "Validation loss: 0.017829040065407753 Best: 0.01715206541121006\n",
      "\t\tEpoch 47\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0200673658400774 Best: 0.01715206541121006\n",
      "\t\tEpoch 48\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.020423753187060356 Best: 0.01715206541121006\n",
      "\t\tEpoch 49\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019295398145914078 Best: 0.01715206541121006\n",
      "\t\tEpoch 50\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019167575985193253 Best: 0.01715206541121006\n",
      "\t\tEpoch 51\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019116653129458427 Best: 0.01715206541121006\n",
      "\t\tEpoch 52\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.000390625\n",
      "Validation loss: 0.01791658252477646 Best: 0.01715206541121006\n",
      "\t\tEpoch 53\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.017455382272601128 Best: 0.01715206541121006\n",
      "\t\tEpoch 54\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.021015482023358345 Best: 0.01715206541121006\n",
      "\t\tEpoch 55\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.018512720242142677 Best: 0.01715206541121006\n",
      "\t\tEpoch 56\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01760045252740383 Best: 0.01715206541121006\n",
      "\t\tEpoch 57\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 57\n",
      "Validation loss: 0.01713738404214382 Best: 0.01713738404214382\n",
      "\t\tEpoch 58\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019336853176355362 Best: 0.01713738404214382\n",
      "\t\tEpoch 59\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0001953125\n",
      "Validation loss: 0.018250567838549614 Best: 0.01713738404214382\n",
      "\t\tEpoch 60\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01858900859951973 Best: 0.01713738404214382\n",
      "\t\tEpoch 61\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.020265819504857063 Best: 0.01713738404214382\n",
      "\t\tEpoch 62\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0188298299908638 Best: 0.01713738404214382\n",
      "\t\tEpoch 63\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.02043621614575386 Best: 0.01713738404214382\n",
      "\t\tEpoch 64\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.020247943699359894 Best: 0.01713738404214382\n",
      "\t\tEpoch 65\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 9.765625e-05\n",
      "Validation loss: 0.01873931847512722 Best: 0.01713738404214382\n",
      "\t\tEpoch 66\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019753260537981987 Best: 0.01713738404214382\n",
      "\t\tEpoch 67\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.020728914067149162 Best: 0.01713738404214382\n",
      "\t\tEpoch 68\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.021230196580290794 Best: 0.01713738404214382\n",
      "\t\tEpoch 69\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019831614568829536 Best: 0.01713738404214382\n",
      "\t\tEpoch 70\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.020381413400173187 Best: 0.01713738404214382\n",
      "\t\tEpoch 71\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 4.8828125e-05\n",
      "Validation loss: 0.020411888137459755 Best: 0.01713738404214382\n",
      "\t\tEpoch 72\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.018726808950304985 Best: 0.01713738404214382\n",
      "\t\tEpoch 73\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.018170680850744247 Best: 0.01713738404214382\n",
      "\t\tEpoch 74\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019734162837266922 Best: 0.01713738404214382\n",
      "\t\tEpoch 75\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.02059546485543251 Best: 0.01713738404214382\n",
      "\t\tEpoch 76\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.020799772813916206 Best: 0.01713738404214382\n",
      "\t\tEpoch 77\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 2.44140625e-05\n",
      "Validation loss: 0.021077394485473633 Best: 0.01713738404214382\n",
      "\t\tEpoch 78\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.020068982616066933 Best: 0.01713738404214382\n",
      "\t\tEpoch 79\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.020635703578591347 Best: 0.01713738404214382\n",
      "\t\tEpoch 80\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.021294744685292244 Best: 0.01713738404214382\n",
      "\t\tEpoch 81\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.020997246727347374 Best: 0.01713738404214382\n",
      "\t\tEpoch 82\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01860153302550316 Best: 0.01713738404214382\n",
      "\t\tEpoch 83\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 1.220703125e-05\n",
      "Validation loss: 0.018319297581911087 Best: 0.01713738404214382\n",
      "\t\tEpoch 84\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.017209792509675026 Best: 0.01713738404214382\n",
      "\t\tEpoch 85\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01757972501218319 Best: 0.01713738404214382\n",
      "\t\tEpoch 86\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01743045262992382 Best: 0.01713738404214382\n",
      "\t\tEpoch 87\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01797436736524105 Best: 0.01713738404214382\n",
      "\t\tEpoch 88\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.018482903018593788 Best: 0.01713738404214382\n",
      "\t\tEpoch 89\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 6.103515625e-06\n",
      "Validation loss: 0.01807432994246483 Best: 0.01713738404214382\n",
      "\t\tEpoch 90\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.0179261676967144 Best: 0.01713738404214382\n",
      "\t\tEpoch 91\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01727091148495674 Best: 0.01713738404214382\n",
      "\t\tEpoch 92\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019921310245990753 Best: 0.01713738404214382\n",
      "\t\tEpoch 93\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019787434488534927 Best: 0.01713738404214382\n",
      "\t\tEpoch 94\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019817765802145004 Best: 0.01713738404214382\n",
      "\t\tEpoch 95\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 3.0517578125e-06\n",
      "Validation loss: 0.019198739901185036 Best: 0.01713738404214382\n",
      "\t\tEpoch 96\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01847212389111519 Best: 0.01713738404214382\n",
      "\t\tEpoch 97\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01882466860115528 Best: 0.01713738404214382\n",
      "\t\tEpoch 98\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.017532208934426308 Best: 0.01713738404214382\n",
      "\t\tEpoch 99\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01727963238954544 Best: 0.01713738404214382\n",
      "\t\tEpoch 100\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.019165663048624992 Best: 0.01713738404214382\n",
      "0.054072687073179165\n",
      "\t\t\tQuantile Network\n",
      "Use RandomForestRegressor\n",
      "ymax and min: tensor(2.5019) tensor(-3.0294)\n",
      "Using marginal loss\n",
      "\t\tEpoch 1\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 1\n",
      "Validation loss: 0.019666075706481934 Best: 0.019666075706481934\n",
      "\t\tEpoch 2\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01993880234658718 Best: 0.019666075706481934\n",
      "\t\tEpoch 3\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 3\n",
      "Validation loss: 0.01954522170126438 Best: 0.01954522170126438\n",
      "\t\tEpoch 4\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 4\n",
      "Validation loss: 0.01916155032813549 Best: 0.01916155032813549\n",
      "\t\tEpoch 5\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 5\n",
      "Validation loss: 0.015530751086771488 Best: 0.015530751086771488\n",
      "\t\tEpoch 6\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 6\n",
      "Validation loss: 0.015482920221984386 Best: 0.015482920221984386\n",
      "\t\tEpoch 7\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 7\n",
      "Validation loss: 0.014881160110235214 Best: 0.014881160110235214\n",
      "\t\tEpoch 8\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 8\n",
      "Validation loss: 0.014370781369507313 Best: 0.014370781369507313\n",
      "\t\tEpoch 9\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 9\n",
      "Validation loss: 0.012606203556060791 Best: 0.012606203556060791\n",
      "\t\tEpoch 10\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 10\n",
      "Validation loss: 0.011156451888382435 Best: 0.011156451888382435\n",
      "\t\tEpoch 11\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014245548285543919 Best: 0.011156451888382435\n",
      "\t\tEpoch 12\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011754018254578114 Best: 0.011156451888382435\n",
      "\t\tEpoch 13\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011747042648494244 Best: 0.011156451888382435\n",
      "\t\tEpoch 14\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011757207103073597 Best: 0.011156451888382435\n",
      "\t\tEpoch 15\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012832105159759521 Best: 0.011156451888382435\n",
      "\t\tEpoch 16\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.05\n",
      "Validation loss: 0.01262336503714323 Best: 0.011156451888382435\n",
      "\t\tEpoch 17\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014402313157916069 Best: 0.011156451888382435\n",
      "\t\tEpoch 18\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 18\n",
      "Validation loss: 0.010688511654734612 Best: 0.010688511654734612\n",
      "\t\tEpoch 19\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014998666010797024 Best: 0.010688511654734612\n",
      "\t\tEpoch 20\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011388000100851059 Best: 0.010688511654734612\n",
      "\t\tEpoch 21\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01105431281030178 Best: 0.010688511654734612\n",
      "\t\tEpoch 22\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013501708395779133 Best: 0.010688511654734612\n",
      "\t\tEpoch 23\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.025\n",
      "\t\t\tSaving test set results.      <----- New high water mark on epoch 23\n",
      "Validation loss: 0.009615936316549778 Best: 0.009615936316549778\n",
      "\t\tEpoch 24\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013119778595864773 Best: 0.009615936316549778\n",
      "\t\tEpoch 25\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011833077296614647 Best: 0.009615936316549778\n",
      "\t\tEpoch 26\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011964734643697739 Best: 0.009615936316549778\n",
      "\t\tEpoch 27\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011549749411642551 Best: 0.009615936316549778\n",
      "\t\tEpoch 28\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012961095198988914 Best: 0.009615936316549778\n",
      "\t\tEpoch 29\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012553759850561619 Best: 0.009615936316549778\n",
      "\t\tEpoch 30\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0125\n",
      "Validation loss: 0.015921633690595627 Best: 0.009615936316549778\n",
      "\t\tEpoch 31\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012775206938385963 Best: 0.009615936316549778\n",
      "\t\tEpoch 32\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012321756221354008 Best: 0.009615936316549778\n",
      "\t\tEpoch 33\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012431919574737549 Best: 0.009615936316549778\n",
      "\t\tEpoch 34\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012291750870645046 Best: 0.009615936316549778\n",
      "\t\tEpoch 35\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01311776228249073 Best: 0.009615936316549778\n",
      "\t\tEpoch 36\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00625\n",
      "Validation loss: 0.011532495729625225 Best: 0.009615936316549778\n",
      "\t\tEpoch 37\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011940349824726582 Best: 0.009615936316549778\n",
      "\t\tEpoch 38\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01323459018021822 Best: 0.009615936316549778\n",
      "\t\tEpoch 39\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013419422321021557 Best: 0.009615936316549778\n",
      "\t\tEpoch 40\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014493186958134174 Best: 0.009615936316549778\n",
      "\t\tEpoch 41\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013697798363864422 Best: 0.009615936316549778\n",
      "\t\tEpoch 42\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.003125\n",
      "Validation loss: 0.016267189756035805 Best: 0.009615936316549778\n",
      "\t\tEpoch 43\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011494304053485394 Best: 0.009615936316549778\n",
      "\t\tEpoch 44\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01135329995304346 Best: 0.009615936316549778\n",
      "\t\tEpoch 45\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01322124246507883 Best: 0.009615936316549778\n",
      "\t\tEpoch 46\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01489368174225092 Best: 0.009615936316549778\n",
      "\t\tEpoch 47\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014151370152831078 Best: 0.009615936316549778\n",
      "\t\tEpoch 48\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0015625\n",
      "Validation loss: 0.010144012980163097 Best: 0.009615936316549778\n",
      "\t\tEpoch 49\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014744733460247517 Best: 0.009615936316549778\n",
      "\t\tEpoch 50\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011708904057741165 Best: 0.009615936316549778\n",
      "\t\tEpoch 51\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012355778366327286 Best: 0.009615936316549778\n",
      "\t\tEpoch 52\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01644604280591011 Best: 0.009615936316549778\n",
      "\t\tEpoch 53\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012773054651916027 Best: 0.009615936316549778\n",
      "\t\tEpoch 54\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.00078125\n",
      "Validation loss: 0.010785194113850594 Best: 0.009615936316549778\n",
      "\t\tEpoch 55\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012840895913541317 Best: 0.009615936316549778\n",
      "\t\tEpoch 56\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011488902382552624 Best: 0.009615936316549778\n",
      "\t\tEpoch 57\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015089056454598904 Best: 0.009615936316549778\n",
      "\t\tEpoch 58\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013046322390437126 Best: 0.009615936316549778\n",
      "\t\tEpoch 59\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01291872002184391 Best: 0.009615936316549778\n",
      "\t\tEpoch 60\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.000390625\n",
      "Validation loss: 0.01599295623600483 Best: 0.009615936316549778\n",
      "\t\tEpoch 61\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011830752715468407 Best: 0.009615936316549778\n",
      "\t\tEpoch 62\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012325907126069069 Best: 0.009615936316549778\n",
      "\t\tEpoch 63\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01221725344657898 Best: 0.009615936316549778\n",
      "\t\tEpoch 64\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011064001359045506 Best: 0.009615936316549778\n",
      "\t\tEpoch 65\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013942034915089607 Best: 0.009615936316549778\n",
      "\t\tEpoch 66\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 0.0001953125\n",
      "Validation loss: 0.013513931073248386 Best: 0.009615936316549778\n",
      "\t\tEpoch 67\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015744375064969063 Best: 0.009615936316549778\n",
      "\t\tEpoch 68\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013159285299479961 Best: 0.009615936316549778\n",
      "\t\tEpoch 69\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012149355374276638 Best: 0.009615936316549778\n",
      "\t\tEpoch 70\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.015004920773208141 Best: 0.009615936316549778\n",
      "\t\tEpoch 71\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014424245804548264 Best: 0.009615936316549778\n",
      "\t\tEpoch 72\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 9.765625e-05\n",
      "Validation loss: 0.013539118692278862 Best: 0.009615936316549778\n",
      "\t\tEpoch 73\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014438043348491192 Best: 0.009615936316549778\n",
      "\t\tEpoch 74\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014771233312785625 Best: 0.009615936316549778\n",
      "\t\tEpoch 75\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012896722182631493 Best: 0.009615936316549778\n",
      "\t\tEpoch 76\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014636514708399773 Best: 0.009615936316549778\n",
      "\t\tEpoch 77\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012408680282533169 Best: 0.009615936316549778\n",
      "\t\tEpoch 78\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 4.8828125e-05\n",
      "Validation loss: 0.014826327562332153 Best: 0.009615936316549778\n",
      "\t\tEpoch 79\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013395283371210098 Best: 0.009615936316549778\n",
      "\t\tEpoch 80\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.016824450343847275 Best: 0.009615936316549778\n",
      "\t\tEpoch 81\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01403124164789915 Best: 0.009615936316549778\n",
      "\t\tEpoch 82\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014962627552449703 Best: 0.009615936316549778\n",
      "\t\tEpoch 83\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011677172966301441 Best: 0.009615936316549778\n",
      "\t\tEpoch 84\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 2.44140625e-05\n",
      "Validation loss: 0.012604680843651295 Best: 0.009615936316549778\n",
      "\t\tEpoch 85\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014830848202109337 Best: 0.009615936316549778\n",
      "\t\tEpoch 86\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014615828171372414 Best: 0.009615936316549778\n",
      "\t\tEpoch 87\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01443230826407671 Best: 0.009615936316549778\n",
      "\t\tEpoch 88\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014975780621170998 Best: 0.009615936316549778\n",
      "\t\tEpoch 89\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.01218031533062458 Best: 0.009615936316549778\n",
      "\t\tEpoch 90\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 1.220703125e-05\n",
      "Validation loss: 0.01799301616847515 Best: 0.009615936316549778\n",
      "\t\tEpoch 91\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.014583845622837543 Best: 0.009615936316549778\n",
      "\t\tEpoch 92\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.011280273087322712 Best: 0.009615936316549778\n",
      "\t\tEpoch 93\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013025782071053982 Best: 0.009615936316549778\n",
      "\t\tEpoch 94\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013943882659077644 Best: 0.009615936316549778\n",
      "\t\tEpoch 95\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013126268982887268 Best: 0.009615936316549778\n",
      "\t\tEpoch 96\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Decreasing learning rate to 6.103515625e-06\n",
      "Validation loss: 0.015545662492513657 Best: 0.009615936316549778\n",
      "\t\tEpoch 97\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.012778743170201778 Best: 0.009615936316549778\n",
      "\t\tEpoch 98\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013091588392853737 Best: 0.009615936316549778\n",
      "\t\tEpoch 99\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013129590079188347 Best: 0.009615936316549778\n",
      "\t\tEpoch 100\n",
      "\t\t\tBatch 0\n",
      "\t\t\tValidation Batch 0\n",
      "Validation loss: 0.013465444557368755 Best: 0.009615936316549778\n",
      "0.0636076549125131\n",
      "Mean mse 0.08991319570092558\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    #run_benchmarks(False)\n",
    "\n",
    "    run_multivariate_benchmarks(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ffeb2f-927c-487b-8043-bbd2015fd856",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
